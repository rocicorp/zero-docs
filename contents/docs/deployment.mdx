---
title: Deploying Zero
---

So you've built your app with Zero - congratulations! Now you need to run it on a server somewhere.

You will need to deploy zero-cache, a Postgres database, your frontend, and your API server.

Zero-cache is made up of two main components:

1. One or more _view-syncers_: serving clients by running IVM-driven queries using a SQLite replica and proxying mutations to your API server.
2. One _replication-manager_: backup replication service that bridges Postgres and your view-syncers by consuming the replication stream and distributing it.

By default, these are run on the same node, but you can split the services to run on different nodes.

You will also need to deploy a Postgres database, your frontend, and your API server for the [query](/docs/queries#server-setup) and [mutate](/docs/mutators#server-setup) endpoints.

## Strategies

There are two main ways to run zero-cache in production.

### Minimum Viable Strategy

The simplest way to deploy Zero is to run everything on a single node. This is the least expensive way to run Zero, and it can take you surprisingly far.

```json
┌──────────────────────────────────────────────────────────────────────────┐
│                              Client Layer                                 │
│                                                                           │
│  ┌───────────┐  ┌───────────┐  ┌───────────┐  ┌───────────┐            │
│  │ Browser 1 │  │ Browser 2 │  │ Browser 3 │  │ Browser N │            │
│  │ (React)   │  │ (React)   │  │ (Svelte)  │  │ (Mobile)  │            │
│  └───────────┘  └───────────┘  └───────────┘  └───────────┘            │
│        │              │              │              │                    │
│        │ WebSocket    │ WebSocket    │ WebSocket    │ WebSocket         │
│        │ (queries)    │ (queries)    │ (queries)    │ (queries)         │
│        └──────────────┴──────────────┴──────────────┘                    │
└───────────────────────────────────────┬──────────────────────────────────┘
                                        │
                                        ↓
┌──────────────────────────────────────────────────────────────────────────┐
│                         Zero Cache (Single Node)                          │
│                          ZERO_NUM_SYNC_WORKERS=default                    │
│                                Port 4848                                  │
│                                                                           │
│  ┌─────────────────────────────────────────────────────────────────────┐ │
│  │ Syncer Workers (M = cores - 1)                                      │ │
│  │  - Handle client WebSocket connections                              │ │
│  │  - Compute query diffs (IVM)                                        │ │
│  │  - Send mutations to API server  ─────────────┐                    │ │
│  │  - Send custom queries to Query API ────────┐ │                    │ │
│  └─────────────────────────────────────────────────────────────────────┘ │
│                         ↑                       │ │                      │
│                         │ notifies              │ │                      │
│  ┌─────────────────────────────────────────────────────────────────────┐ │
│  │ Replicator Worker                                                   │ │
│  │  - Owns Postgres replication slot                                   │ │
│  │  - Writes to SQLite replica                                         │ │
│  │  - Does initial-sync from Postgres                                  │ │
│  └─────────────────────────────────────────────────────────────────────┘ │
│         ↑                                       │ │                      │
│         │ replication stream                    │ │                      │
│  ┌─────────────────────────────────────────────────────────────────────┐ │
│  │ SQLite Replica (zero.db)                                            │ │
│  │  - Stores replicated tables from Postgres                           │ │
│  │  - Read by all syncer workers                                       │ │
│  │  - Written only by replicator                                       │ │
│  └─────────────────────────────────────────────────────────────────────┘ │
│         ↑                                       │ │                      │
└─────────┼───────────────────────────────────────┼─┼──────────────────────┘
          │                                       │ │
          │                     HTTP POST         │ │ HTTP POST
          │                     (custom queries)  │ │ (mutations)
          │                                       ↓ ↓
┌─────────┴──────────────────────────────────────────────────────────────┐
│                         Your Infrastructure                              │
│                                                                          │
│  ┌─────────────────────────────────────┐  ┌─────────────────────────┐  │
│  │  Postgres (Upstream DB)             │  │  Your API Server        │  │
│  │                                     │  │                         │  │
│  │  ┌──────────────────────────────┐  │  │  POST /api/mutate       │  │
│  │  │ Your Tables                  │  │  │  - Validates mutations  │  │
│  │  │  - users                     │  │  │  - Writes to Postgres   │  │
│  │  │  - posts                     │  │  │  - Auth/business logic  │  │
│  │  │  - comments                  │◄─┼──┼─                        │  │
│  │  └──────────────────────────────┘  │  │                         │  │
│  │                                     │  │  POST /api/query        │  │
│  │  ┌──────────────────────────────┐  │  │  - Transform queries    │  │
│  │  │ Zero Metadata                │  │  │  - Add custom data      │  │
│  │  │  - zero_0/cvr (CVR data)     │  │  │  - Auth checks          │  │
│  │  │  - zero.clients              │  │  │  - Call external APIs   │  │
│  │  │  - zero.instances            │  │  └─────────────────────────┘  │
│  │  └──────────────────────────────┘  │                                │
│  │                                     │                                │
│  │  Replication Slot: zero_0           │                                │
│  └─────────────────────────────────────┘                                │
└──────────────────────────────────────────────────────────────────────────┘
```

Here is an example `docker-compose.yml` file for a single-node deployment ([try it out!](https://github.com/rocicorp/onboarding/blob/maximal/docker-compose.single-node.yml)):

```yaml
services:
  upstream_db:
    image: postgres:18
    environment:
      POSTGRES_DB: zero
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    ports:
      - 5432:5432
    command: postgres -c wal_level=logical
    healthcheck:
      test: pg_isready -U user --dbname=postgres
      interval: 10s

  api:
    build: ./api
    ports:
      - 3000:3000
    environment:
      # your API handles mutations and writes to the PG db
      ZERO_UPSTREAM_DB: postgres://user:password@upstream_db:5432/zero
    depends_on:
      upstream_db:
        condition: service_healthy

  zero-cache:
    image: rocicorp/zero:{version}
    ports:
      - 4848:4848
    environment:
      # used for replication from postgres
      ZERO_UPSTREAM_DB: postgres://user:password@upstream_db:5432/zero
      # path to the SQLite replica
      ZERO_REPLICA_FILE: /data/zero.db
      # password used to access the inspector and /statz
      ZERO_ADMIN_PASSWORD: pickanewpassword
      # URLs for your API's query and mutate endpoints
      ZERO_QUERY_URL: http://api:3000/api/zero/query
      ZERO_MUTATE_URL: http://api:3000/api/zero/mutate
    volumes:
      # disk for the SQLite replica should be high IOPS
      - zero_cache_data:/data
    depends_on:
      upstream_db:
        condition: service_healthy
      api:
        condition: service_started
    healthcheck:
      test: curl -f http://localhost:4848/keepalive
      interval: 5s
```

### Maximal Strategy

Once you reach the limits of the single-node deployment, you can split zero-cache into a multi-node topology. This is more expensive to run, but it gives you more flexibility and scalability.

```json
┌──────────────────────────────────────────────────────────────────────────┐
│                              Client Layer                                 │
│  ┌───────────┐  ┌───────────┐  ┌───────────┐  ┌───────────┐            │
│  │ Browser 1 │  │ Browser 2 │  │ Browser 3 │  │ Browser N │            │
│  └───────────┘  └───────────┘  └───────────┘  └───────────┘            │
│        │              │              │              │                    │
│        │ WebSocket    │ WebSocket    │ WebSocket    │ WebSocket         │
│        └──────────────┴──────────────┴──────────────┘                    │
└───────────────────────────────────────┬──────────────────────────────────┘
                                        │
                                        ↓
                            ┌───────────────────────┐
                            │   Load Balancer       │
                            └───────────────────────┘
                                        │
                    ┌───────────────────┼───────────────────┐
                    ↓                   ↓                   ↓
┌──────────────────────────┐ ┌──────────────────────┐ ┌──────────────────────┐
│ View-Syncer Node 1       │ │ View-Syncer Node 2   │ │ View-Syncer Node N   │
│ Port 4848                │ │ Port 4848            │ │ Port 4848            │
│                          │ │                      │ │                      │
│ ┌──────────────────────┐ │ │ ┌──────────────────┐│ │ ┌──────────────────┐ │
│ │ Syncer Workers       │ │ │ │ Syncer Workers   ││ │ │ Syncer Workers   │ │
│ │ (M = cores - 1)      │ │ │ │ (M = cores - 1)  ││ │ │ (M = cores - 1)  │ │
│ │ - Serve client conns │ │ │ │ - Serve clients  ││ │ │ - Serve clients  │ │
│ │ - Compute diffs ─────┼─┼─┼─┼──────────────────┼┼─┼─┼──────────────────┼─┼─┐
│ │ - Forward mutations ─┼─┼─┼─┼──────────────────┼┼─┼─┼──────────────────┼─┼─┼─┐
│ └──────────────────────┘ │ │ └──────────────────┘│ │ └──────────────────┘ │ │ │
│          ↑               │ │          ↑          │ │          ↑           │ │ │
│ ┌──────────────────────┐ │ │ ┌──────────────────┐│ │ ┌──────────────────┐ │ │ │
│ │ Replicator           │ │ │ │ Replicator       ││ │ │ Replicator       │ │ │ │
│ │ - Subscribe to       │ │ │ │ - Subscribe to   ││ │ │ - Subscribe to   │ │ │ │
│ │   repl-manager WS    │ │ │ │   repl-mgr WS    ││ │ │   repl-mgr WS    │ │ │ │
│ │ - Write to SQLite    │ │ │ │ - Write to SQLite││ │ │ - Write to SQLite│ │ │ │
│ └──────────────────────┘ │ │ └──────────────────┘│ │ └──────────────────┘ │ │ │
│          ↑               │ │          ↑          │ │          ↑           │ │ │
│ ┌──────────────────────┐ │ │ ┌──────────────────┐│ │ ┌──────────────────┐ │ │ │
│ │ SQLite Replica       │ │ │ │ SQLite Replica   ││ │ │ SQLite Replica   │ │ │ │
│ │ (ephemeral)          │ │ │ │ (ephemeral)      ││ │ │ (ephemeral)      │ │ │ │
│ │ Restored from S3     │ │ │ │ Restored from S3 ││ │ │ Restored from S3 │ │ │ │
│ └──────────────────────┘ │ │ └──────────────────┘│ │ └──────────────────┘ │ │ │
│                          │ │                      │ │                      │ │ │
│ ┌──────────────────────┐ │ │ ┌──────────────────┐│ │ ┌──────────────────┐ │ │ │
│ │ Reaper (CVR GC)      │ │ │ │ Reaper (CVR GC)  ││ │ │ Reaper (CVR GC)  │ │ │ │
│ └──────────────────────┘ │ │ └──────────────────┘│ │ └──────────────────┘ │ │ │
└──────────────────────────┘ └──────────────────────┘ └──────────────────────┘ │ │
            │                           │                           │           │ │
            │ WebSocket                 │ WebSocket                 │ WebSocket │ │
            └───────────────────────────┴───────────────────────────┘           │ │
                                        ↓                                        │ │
┌──────────────────────────────────────────────────────────────────────────────┐│ │
│                    Replication-Manager (Single Instance)                      ││ │
│                                                                               ││ │
│  ┌─────────────────────────────────────────────────────────────────────────┐││ │
│  │ change-streamer                                         Port 4849        │││ │
│  │  - Consumes Postgres replication slot                                   │││ │
│  │  - Writes to Change DB                                                  │││ │
│  │  - Serves N view-syncers via WebSocket                                  │││ │
│  │  - Coordinates change-log cleanup                                       │││ │
│  └─────────────────────────────────────────────────────────────────────────┘││ │
│                                                                               ││ │
│  ┌─────────────────────────────────────────────────────────────────────────┐││ │
│  │ backup replicator                                                        │││ │
│  │  - Keeps SQLite replica current for litestream                          │││ │
│  └─────────────────────────────────────────────────────────────────────────┘││ │
│                                   ↓                                          ││ │
│  ┌─────────────────────────────────────────────────────────────────────────┐││ │
│  │ SQLite Backup Replica (zero.db)                                          │││ │
│  └─────────────────────────────────────────────────────────────────────────┘││ │
│                                   ↓                                          ││ │
│  ┌─────────────────────────────────────────────────────────────────────────┐││ │
│  │ litestream                                              Port 4850        │││ │
│  │  - Backs up SQLite to object storage                                    │││ │
│  │  - Creates incremental snapshots                                        │││ │
│  │  - Exports Prometheus metrics                                           │││ │
│  └─────────────────────────────────────────────────────────────────────────┘││ │
└───────────────────────────────────────────────────────────────────────────────┘│ │
                                  ↓                                             │ │
┌──────────────────────────────────────────────────────────────────────────────┐│ │
│                     Object Storage (S3-compatible)                            ││ │
│                                                                               ││ │
│  s3://my-bucket/zero/                                                         ││ │
│    ├── snapshots/                                                             ││ │
│    │   ├── 00000001/zero.db                                                   ││ │
│    │   └── 00000050/zero.db                                                   ││ │
│    └── wal/                                                                   ││ │
│        ├── 00000001-00000010.wal                                              ││ │
│        └── 00000041-00000050.wal                                              ││ │
│                                                                               ││ │
│  View-syncers restore from here on startup                                   ││ │
└───────────────────────────────────────────────────────────────────────────────┘│ │
                                                                                │ │
                                                          HTTP POST              │ │
                                                          (mutations)            │ │
                                                                    HTTP POST   │ │
                                                                    (queries)   │ │
                                                                          ↓ ↓   │ │
┌──────────────────────────────────────────────────────────────────────────────┐│ │
│                              Your Infrastructure                              ││ │
│                                                                               ││ │
│  ┌─────────────────────────────────────┐  ┌─────────────────────────────┐   ││ │
│  │  Postgres Cluster                   │  │  Your API Servers           │   ││ │
│  │                                     │  │  (horizontally scaled)       │   ││ │
│  │  ┌──────────────────────────────┐  │  │                              │   ││ │
│  │  │ Primary (read-write)         │  │  │  POST /api/mutate            │◄──┼┼─┘
│  │  │                              │  │  │  - Auth & validation         │   ││
│  │  │  Your Schemas:               │  │  │  - Business logic            │   ││
│  │  │  - public.users              │  │  │  - Writes to Postgres        │   ││
│  │  │  - public.posts              │◄─┼──┼─                             │   ││
│  │  │  - public.comments           │  │  │  POST /api/query             │◄──┼┘
│  │  │  - ...                       │  │  │  - Transform queries         │   │
│  │  │                              │  │  │  - Add custom data           │   │
│  │  │  Zero Metadata Schemas:      │  │  │  - Auth checks               │   │
│  │  │  - zero_0                    │  │  └─────────────────────────────┘   │
│  │  │  - zero_0/cvr (CVR DB)       │  │                                     │
│  │  │  - zero_0/cdc (Change DB)    │  │                                     │
│  │  │                              │  │                                     │
│  │  │  Replication Slot: zero_0    │  │                                     │
│  │  └──────────────────────────────┘  │                                     │
│  │            ↓                        │                                     │
│  │  ┌──────────────────────────────┐  │                                     │
│  │  │ Replicas (read-only)         │  │                                     │
│  │  └──────────────────────────────┘  │                                     │
│  └─────────────────────────────────────┘                                     │
└───────────────────────────────────────────────────────────────────────────────┘
```

Here is an example `docker-compose.yml` file for a multi-node deployment:

```yaml
services:
  upstream_db:
    image: postgres:18
    environment:
      POSTGRES_DB: zero
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    ports:
      - 5432:5432
    command: postgres -c wal_level=logical
    healthcheck:
      test: pg_isready -U user --dbname=postgres
      interval: 10s

  your_api:
    build: ./your_api
    ports:
      - 3000:3000
    environment:
      # your API writes mutations to the database
      ZERO_UPSTREAM_DB: postgres://user:password@upstream_db:5432/zero
    depends_on:
      upstream_db:
        condition: service_healthy

  # "Mini S3" (MinIO) provides a working s3://... `ZERO_LITESTREAM_BACKUP_URL`
  # This should be an S3-compatible object storage service in production.
  mini-s3:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    ports:
      - 9000:9000 # S3 API
      - 9001:9001 # Console UI
    volumes:
      - s3_data:/data
    healthcheck:
      test: curl -f http://localhost:9000/minio/health/live
      interval: 5s

  # Creates the bucket used by `ZERO_LITESTREAM_BACKUP_URL`
  # This is only needed for local development.
  mini-s3-create-bucket:
    image: minio/mc:latest
    depends_on:
      mini-s3:
        condition: service_healthy
    entrypoint: ['/bin/sh', '-lc']
    command: |
      mc alias set local http://mini-s3:9000 "minioadmin" "minioadmin"
      mc mb -p local/zero-backups || true

  replication-manager:
    # use the replication-manager tag
    image: rocicorp/zero:{version}-replication-manager
    ports:
      - 4849:4849
    depends_on:
      upstream_db:
        condition: service_healthy
      your_api:
        condition: service_started
      mini-s3-create-bucket:
        condition: service_started
    environment:
      # used for replication from postgres
      ZERO_UPSTREAM_DB: postgres://user:password@upstream_db:5432/zero
      # used for storing client view records
      ZERO_CVR_DB: postgres://user:password@upstream_db:5432/zero
      # used for storing recent replication log entries
      ZERO_CHANGE_DB: postgres://user:password@upstream_db:5432/zero

      # path to the SQLite replica
      ZERO_REPLICA_FILE: /data/replica.db
      # password used to access the inspector and /statz
      ZERO_ADMIN_PASSWORD: pickanewpassword

      # URL for backing up the SQLite replica (include a simple version number for future cleanup)
      ZERO_LITESTREAM_BACKUP_URL: s3://zero-backups/litestream-v1

      # S3 creds + Mini S3 endpoint (view-syncers restore from backup on startup)
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      LITESTREAM_S3_ENDPOINT: http://mini-s3:9000
      # LITESTREAM_S3_FORCE_PATH_STYLE: true
      # LITESTREAM_S3_REGION: us-east-1
    volumes:
      # storage for the SQLite replica should be high IOPS
      - replication_data:/data
    healthcheck:
      test: curl -f http://localhost:4849/keepalive
      interval: 5s

  # only one view-syncer in this example, but there can be N
  view-syncer:
    image: rocicorp/zero:{version}-view-syncer
    ports:
      - 4848:4848
    depends_on:
      replication-manager:
        condition: service_healthy
    environment:
      # path to the SQLite replica
      ZERO_REPLICA_FILE: /data/replica.db
      # password used to access the inspector and /statz
      ZERO_ADMIN_PASSWORD: pickanewpassword
      # URLs for your API's query and mutate endpoints
      ZERO_QUERY_URL: http://your_api:3000/api/zero/query
      ZERO_MUTATE_URL: http://your_api:3000/api/zero/mutate

      # URL for connecting to the replication-manager
      ZERO_CHANGE_STREAMER_URI: http://replication-manager:4849

      # S3 creds + Mini S3 endpoint (view-syncers restore from backup on startup)
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      LITESTREAM_S3_ENDPOINT: http://mini-s3:9000
      # LITESTREAM_S3_FORCE_PATH_STYLE: true
      # LITESTREAM_S3_REGION: us-east-1
    volumes:
      # storage for the SQLite replica should be high IOPS
      - view_syncer_data:/data
    healthcheck:
      test: curl -f http://localhost:4848/keepalive
      interval: 5s
```

#### Scaling

The view-syncers in the multi-node topology can be horizontally scaled as needed.

TODO PG connection budget

You can also override the number of sync workers _per view-syncer_ with `ZERO_NUM_SYNC_WORKERS` (see [#components](Components) for more information on sync workers).

#### Performance expectations

TODO??

## Replica Lifecycle

Zero-cache is backed by a SQLite replica of your database. Well, technically, it can be multiple SQLite replicas.
The replication-manager always has its own _source_ replica file, and the view-syncers can either share the same file or have their own _serving_ replica file:

1. On single-node deployments without litestream backups, the view-syncers share the same replica file as the replication-manager, and source and serving are the same file.
2. On single-node deployments with litestream backups, the view-syncers use an ephemeral serving fork of the source replica file.
3. On multi-node deployments, the view-syncers are on entirely different nodes and similarly must have their own serving replica file.

The replication-manager's source replica can be replicated to S3 using [litestream](https://github.com/benbjohnson/litestream).
This is optional, but allows for fast recovery from restarts and failures, instead of doing a full initial sync from upstream.

The important thing to note is that in any case, there is always upstream Postgres as the source of truth. If the SQLite replica is somehow corrupted or the litestream backup disappears, the replication-manager can resync its source replica from it.

If the litestream backup URL is changed (by, for example, appending a number to the [`ZERO_LITESTREAM_BACKUP_URL`](/docs/zero-cache-config#litestream-backup-url) value), it will force a resync from upstream.

### IVM Performance

You want to optimize disk IOPS for the serving replica, since this is the file that is read by the view-syncers to run IVM-based queries, and one of the main bottlenecks for query hydration performance.
View syncer's IVM is "hydrate once, then incrementally push diffs" against the ZQL pipeline, so performance is mostly about:

1. How fast the server can materialize a subscription the first time (hydration).
2. How fast it can keep it up to date (IVM advancement).

Different bottlenecks dominate each phase.

#### Hydration

- **SQLite read cost**: hydration is essentially "run the query against the replica and stream all matching rows into the pipeline", so it's bounded by [SQLite scan/index performance](/docs/debug/inspector#analyzing-queries) + result size.
- **Churn / TTL eviction**: if queries get [evicted](/docs/reading-data#query-caching) (inactive long enough) and then get re-requested, you pay hydration again.
- **Custom query transform latency**: the HTTP request from zero-cache to your API at [`ZERO_QUERY_URL`](/docs/zero-cache-config#query-url) does transform/authorization for queries, adding network + CPU before hydration starts.

#### IVM advancement

- **Replication throughput**: the view-syncer can only advance when the replicator commits and emits version-ready. If upstream replication is behind, query advancement is capped by how fast the replica advances.
- **Change volume per transaction**: advancement cost scales with number of changed _rows_, not number of queries.
- **Circuit breaker behavior**: if advancement looks like it'll take longer than rehydrating, zero-cache intentionally aborts and resets pipelines (which trades "slow incremental" for "rehydrate").

#### System-level

- **Number of client groups per sync worker**: each client group has its own pipelines; CPU and memory per group limits how many can be "fast" at once. Since Node is single-threaded, one client group can technically starve other groups. This is handled with time slicing and can be configured with the yield parameters, e.g. [`ZERO_YIELD_THRESHOLD_MS`](/docs/zero-cache-config#yield-threshold-ms).
- **SQLite concurrency limits**: it's designed here for one writer (replicator) + many concurrent readers (view-syncer snapshots). It scales, but very heavy read workloads can still contend on cache/IO.
- **Network to clients**: even if IVM is fast, it can take time to send large patches over websocket and the client must apply them.

## Networking and Routing

### Credentials

#### Cookie Based

If your app relies on [cookie auth](/docs/auth#cookies), you need to make sure that the view-syncers are on the same root domain as your web app. Since they are typically available on a subdomain, auth cookies must be set with the `Domain` attribute set to your root domain (e.g., `.example.com`).

#### Token

Networking for [token auth](/docs/auth#tokens) is simpler. You can run the view-syncer on any domain, and the client will pass the token to zero-cache in the `Sec-WebSocket-Protocol` header.

### Load Balancing

View syncers must be publicly reachable by port 4848 by clients, and the replication-manager can have internal networking with the view-syncers on port 4849.

The external load balancer must support websockets, and can use the health check at `/keepalive` to verify view-syncers and replication-managers are healthy.

#### Sticky Sessions

View syncers are designed to be disposable, but since they keep hydrated query pipelines in memory, it's important to try to keep clients connected to the same instance.
If a reconnect/refresh lands on a different instance, that instance usually has to rehydrate instead of reusing warm state.

If you are seeing a lot of Rehome errors, you may need to enable sticky sessions. Two instances can end up doing redundant hydration/advancement work for the same `clientGroupID`, and the "loser" will eventually force clients to reconnect.

### Single Region

For a single-region deployment, all services can be colocated. Configure `zero-cache` to point to your primary Postgres database for all three database connections (`ZERO_UPSTREAM_DB`, `ZERO_CVR_DB`, and `ZERO_CHANGE_DB`).

### Geo-Replicated

For multi-region deployments, we recommend a "1 replication-manager + N view-syncer clusters" topology:

- One replication-manager in your primary region owns the Postgres replication slots.
- View syncer clusters in every region restore the SQLite replica from the replication-manager's litestream backup and subscribe to its change-streamer.

The database configuration should follow these guidelines:

- All clusters connect to the same `ZERO_UPSTREAM_DB` (your primary Postgres database). The view-syncers use this only for garbage collection of `.mutations` rows.
- Each region/cluster can use a local Postgres database for `ZERO_CVR_DB`. It can start empty; it's region-local state.
- `ZERO_CHANGE_DB` is only required for clusters that run a replication-manager. If a cluster is view-syncer-only and you configure it with an explicit `ZERO_CHANGE_STREAMER_URI`, the change DB is not used and can be the same as the `ZERO_CVR_DB`.
- Use the same `ZERO_APP_ID` for a single logical app across all regions. Don't make it "per region" unless you intentionally want isolated, separate deployments.
- Size `max_replication_slots` based on how many replication-managers you run - budget 2 slots per replication-manager per shard (steady state uses 1; non-disruptive resyncs can temporarily consume 2).

The API endpoints should be distributed differently:

- The [mutate endpoint](/docs/mutators#server-setup) should be colocated with your primary Postgres database to minimize write latency.
- The [query endpoint](/docs/queries#server-setup) should be geographically distributed close to your view-syncer clusters, because view-syncers call it during query transform/hydration and it does read/write to the primary Postgres database.

The routing must be configured to route the frontend WebSocket traffic to the closest view-syncer cluster (geo DNS/CDN), and prefer sticky routing for a given clientGroupID to avoid cross-region "rehome" or cold-start behavior (a different region won't have the same CVR state if `ZERO_CVR_DB` is region-local).

## Upgrades and Rollouts

The replication-manager and view-syncers have different upgrade patterns, since they have different runtime characteristics. The replication-manager requires a full handoff, since it is the single owner of the changelog DB state. The view-syncers can simply be drained and reconnected, since they are designed to be disposable.

### Replication Manager

- The replication-manager's change-streamer is the single owner of the changelog DB state. Ownership is tracked in the change DB and is set when the new instance starts.
- A new replication-manager does not immediately take over the stream. The HTTP server delays by [`ZERO_CHANGE_STREAMER_STARTUP_DELAY_MS`](/docs/zero-cache-config#change-streamer-startup-delay-ms) so a load balancer can mark it healthy first.

### View Syncer

- View-syncers are designed to be disposable: clients reconnect to another instance if they get kicked off.
- On `SIGTERM`, the sync worker does a paced drain: it shuts down view-syncers one at a time, waiting roughly "a hydration time" between them to avoid a thundering herd of rehydrations on the available servers.

Implication for upgrades: if the replication protocol changes (or the change-streamer only exists at an older version), a new view-syncer/replicator can’t talk to an old replication-manager. The system handles this by retrying: restore/snapshot reservation
logic explicitly retries until a “compatible replication-manager has been rolled out”, allowing parallel rollout (packages/zero-cache/src/services/litestream/commands.ts:259).

### Client and View Syncer Compatibility

Clients and view-syncers have a compatibility contract that is versioned by the `PROTOCOL_VERSION`. It checks the client's requested version and rejects anything outside its supported range (defined by `MIN_SERVER_SUPPORTED_SYNC_PROTOCOL`) with a `VersionNotSupported` error.

In practice this means: new view-syncer versions can keep accepting older clients for some window, but an older view-syncer will not understand a newer client.

Client storage is also versioned by the `PROTOCOL_VERSION`. When the protocol/schema changes, the client will effectively create a new local store and re-sync.

### View Syncer and Replication Manager Compatibility

The view-syncer and replication-manager have their own internal versioning between each other. The replication-manager HTTP server accepts a range `[MIN_SUPPORTED_PROTOCOL_VERSION ... PROTOCOL_VERSION]`. The view-syncer is hard-coded to use it's own `PROTOCOL_VERSION`.

### Rollout Strategy

1. Upgrade the API servers (your mutate and query endpoints) first.
2. Upgrade replication-manager next, so it can serve the newer replication protocol and snapshot semantics.
3. Upgrade view-syncers. If they come up before the replication-manager, they'll sit in retry loops until the manager is updated.
4. Upgrade clients last.

## Zero-cache Components

| Component             | Replication Manager | View Syncer          |
| --------------------- | ------------------- | -------------------- |
| Owns replication slot | ✅                  | ❌                   |
| Serves client queries | ❌                  | ✅                   |
| Backs up replica      | ✅ (if URL set)     | ❌                   |
| Restores from backup  | Optional            | Required             |
| Subscribes to changes | N/A (produces)      | ✅                   |
| CVR management        | ❌                  | ✅                   |
| Number deployed       | 1                   | N (horizontal scale) |

### Replication Manager

The replication-manager is made up of the following components:

1. **Change Streamer**: owns the Postgres replication slot, reads changes from Postgres, stores changes in change DB, serves changes to `N` view-syncers via WS, manages change-log cleanup. PG replication → change streamer → changelog table → purging after litestream backup.
2. **Backup Replicator**: subscribes to change-streamer and maintains a read-only copy of SQLite replica for litestream.
3. **Litestream**: backs up the SQLite replica to S3.

```json
  ┌────────────────────────────────────────────────────────────────────────────┐
  │                       Replication-Manager (1 instance)                      │
  │                                                                             │
  │  ┌──────────────────────────┐ ┌─────────────────┐ ┌──────────────────┐   │
  │  │   change-streamer        │ │ backup          │ │ litestream       │   │
  │  │   Worker Process         │ │ replicator      │ │ Subprocess       │   │
  │  │ ┌──────────────────────┐ │ │ Worker Process  │ │                  │   │
  │  │ │ ChangeSource         │ │ │                 │ │ (only if         │   │
  │  │ │ - Postgres repl slot │ │ │ ┌─────────────┐ │ │ backupURL set)   │   │
  │  │ │ - OR custom source   │ │ │ │ Reads SQLite│ │ │                  │   │
  │  │ └──────────────────────┘ │ │ │ replica     │ │ │ - Backs up to    │   │
  │  │          ↓               │ │ │             │ │ │   S3/storage     │   │
  │  │ ┌──────────────────────┐ │ │ │ Subscribes  │ │ │ - Exports        │   │
  │  │ │ ChangeStreamerImpl   │ │ │ │ to change-  │ │ │   Prometheus     │   │
  │  │ │ ┌──────────────────┐ │ │ │ │ streamer WS │ │ │   metrics        │   │
  │  │ │ │ Storer           │ │ │ │ │             │ │ │   (port 4850)    │   │
  │  │ │ │ - Writes to      │ │ │ │ │ Applies     │ │ │                  │   │
  │  │ │ │   change DB      │ │ │ │ │ changes to  │ │ └──────────────────┘   │
  │  │ │ │ - Catchup subs   │ │ │ │ │ backup copy │ │       ↑                │
  │  │ │ │ - Purge old logs │ │ │ │ └─────────────┘ │       │ polls metrics  │
  │  │ │ └──────────────────┘ │ │ └─────────────────┘       │                │
  │  │ │ ┌──────────────────┐ │ │        ↑                  │                │
  │  │ │ │ Forwarder        │ │ │        │ subscribes       │                │
  │  │ │ │ - Live forward   │ │ │        │                  │                │
  │  │ │ └──────────────────┘ │ │        │                  │                │
  │  │ └──────────────────────┘ │ │        │                  │                │
  │  │          ↓               │ │        │                  │                │
  │  │ ┌──────────────────────┐ │ │        │                  │                │
  │  │ │ BackupMonitor        │─┼─┼────────┘                  │                │
  │  │ │ - Polls litestream   │─┼─┼─────────────────────────┘                │
  │  │ │   metrics            │ │ │                                            │
  │  │ │ - Schedules cleanup  │ │ │                                            │
  │  │ │ - Handles /snapshot  │ │ │                                            │
  │  │ └──────────────────────┘ │ │                                            │
  │  │          ↓               │ │                                            │
  │  │ ┌──────────────────────┐ │ │                                            │
  │  │ │ChangeStreamerHttp    │ │ │                                            │
  │  │ │ Server (port 4849)   │ │ │                                            │
  │  │ │                      │ │ │                                            │
  │  │ │ GET /snapshot   [WS] │ │ │                                            │
  │  │ │ GET /changes    [WS] │ │ │                                            │
  │  │ └──────────────────────┘ │ │                                            │
  │  └──────────────────────────┘ └────────────────────────────────────────────┘
  │            ↓ ↓ ↓                                                            │
  └────────────┼─┼─┼──────────────────────────────────────────────────────────┘
              │ │ │
              │ │ └──── WebSocket to backup replicator (internal)
              │ │
              │ └────── WebSocket N to view-syncer N
              └──────── WebSocket 1 to view-syncer 1
```

### View Syncer(s)

The (`N`) view-syncers are made up of the following components:

1. **Sync Workers**: runs IVM for client groups, handles websocket connections, and pushes mutations to the upstream API. The default number of sync workers per view-syncer is `M` = number of cores - 1.
2. **Serving Replicator**: maintains a serving SQLite replica, subscribes to the remote change streamer via WS, and notifies the sync workers with new changes.
3. **Reaper**: garbage collects inactive CVRs.

```json
┌──────────────────────────────────────────────────────────────────────────────┐
│                      Replication-Manager (1 instance)                        │
│  ┌────────────────────────────────────────────────────────────────────────┐  │
│  │              ChangeStreamerHttpServer (port 4849)                       │  │
│  └────────────────────────────────────────────────────────────────────────┘  │
└──────────────────────────────────────────────────────────────────────────────┘
          ↑                         ↑                         ↑
          │ WebSocket 1             │ WebSocket 2             │ WebSocket N
          │                         │                         │
┌───────────────────────┐  ┌───────────────────────┐  ┌───────────────────────┐
│   View-Syncer Node 1   │  │   View-Syncer Node 2   │  │   View-Syncer Node N   │
│                       │  │                       │  │                       │
│  ┌─────────────────┐  │  │  ┌─────────────────┐  │  │  ┌─────────────────┐  │
│  │   Dispatcher    │  │  │  │   Dispatcher    │  │  │  │   Dispatcher    │  │
│  └─────────────────┘  │  │  └─────────────────┘  │  │  └─────────────────┘  │
│                       │  │                       │  │                       │
│  ┌─────────────────┐  │  │  ┌─────────────────┐  │  │  ┌─────────────────┐  │
│  │     Reaper      │  │  │  │     Reaper      │  │  │  │     Reaper      │  │
│  │    (CVR GC)     │  │  │  │    (CVR GC)     │  │  │  │    (CVR GC)     │  │
│  └─────────────────┘  │  │  └─────────────────┘  │  │  └─────────────────┘  │
│                       │  │                       │  │                       │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │                                 Replicator                             │  │
│  │  - Owns SQLite                                                         │  │
│  └──────────────────────────────────────────────────────────────────────-─┘  │
│          │ notifies                                                          │
│          ↓                                                                   │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │                              Syncer Workers                            │  │
│  │                           (serve clients)                               │  │
│  │  ┌──────────────────┐  ┌──────────────────┐  ┌──────────────────┐      │  │
│  │  │  Syncer Worker 1  │  │  Syncer Worker 2  │  │  Syncer Worker 3  │      │  │
│  │  ├──────────────────┤  ├──────────────────┤  ├──────────────────┤      │  │
│  │  │       ...        │  │       ...        │  │       ...        │      │  │
│  │  ├──────────────────┤  ├──────────────────┤  ├──────────────────┤      │  │
│  │  │  Syncer Worker M  │  │  Syncer Worker M  │  │  Syncer Worker M  │      │  │
│  │  └──────────────────┘  └──────────────────┘  └──────────────────┘      │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
└───────────────────────┘  └───────────────────────┘  └───────────────────────┘
```
