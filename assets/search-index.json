[
  {
    "id": "0-auth",
    "title": "Authentication",
    "searchTitle": "Authentication",
    "url": "/docs/auth",
    "content": "Setting up auth in Zero apps has a few steps: Setting the userID on the client Sending credentials to the mutate and queries endpoints Setting the Context type to implement permissions Logging out if desired Setting userID Because multiple users can share the same browser, Zero requires that you provide a userID parameter on construction: import {ZeroProvider} from '@rocicorp/zero/react' import type {ZeroOptions} from '@rocicorp/zero' const opts: ZeroOptions = { // ... userID: 'user-123' } return ( <ZeroProvider {...opts}> <App /> </ZeroProvider> )import {ZeroProvider} from '@rocicorp/zero/solid' import type {ZeroOptions} from '@rocicorp/zero' const opts: ZeroOptions = { // ... userID: 'user-123' } return ( <ZeroProvider {...opts}> <App /> </ZeroProvider> )import {Zero} from '@rocicorp/zero' import type {ZeroOptions} from '@rocicorp/zero' const opts: ZeroOptions = { // ... userID: 'user-123' } const zero = new Zero(opts) If the user is not logged in, just pass empty string or some other constant value: const opts: ZeroOptions = { // ... userID: 'anon' } Zero segregates the client-side storage for each user. This allows users to quickly switch between multiple users and accounts without resyncing. All users that have access to a browser profile have access to the same IndexedDB instances. There is nothing that Zero can do about this ‚Äì users can just open the folder where the data is stored and look inside it. If you have more than one set of Zero data per-user (i.e., for different apps in the same domain), you can additionally use the storageKey parameter: const opts: ZeroOptions = { // ... userID: 'user-123', storageKey: 'my-app' } If specified, storageKey is concatenated along with userID and other internal Zero information to form a unique IndexedDB database name. Sending Credentials You can send credentials using either cookies or tokens. Cookies The most common way to authenticate Zero is with cookies. To enable it, set the ZERO_QUERY_FORWARD_COOKIES and ZERO_MUTATE_FORWARD_COOKIES options to true: export ZERO_QUERY_FORWARD_COOKIES=\"true\" export ZERO_MUTATE_FORWARD_COOKIES=\"true\" # run zero-cache, e.g. `npx zero-cache-dev` Zero-cache will then forward all cookies sent to cacheURL to your mutate and queries endpoints: const opts: ZeroOptions = { schema, // Cookies sent to zero.example.com will be forwarded to // api.example.com/mutate and api.example.com/queries. cacheURL: 'https://zero.example.com', mutateURL: 'https://api.example.com/mutate', queryURL: 'https://api.example.com/queries' } Cookies will show up in the normal HTTP Cookie header and you can authenticate these endpoints just like you would any API request. Deployment In order for cookie auth to work, the browser must send your frontend's cookies to zero-cache, so that zero-cache can forward them to your API. During development, this works automatically as long as your frontend and zero-cache are both running on localhost with different ports. Browsers send cookies based on domain name, not port number, so cookies set by localhost:3000 are also sent to localhost:4848. For production you'll need to do two things: Run zero-cache on a subdomain of your main site (e.g., zero.example.com if your main site is example.com). Consult your hosting provider's docs, or your favorite LLM for how to configure this. Set cookies from your main site with the Domain attribute set to your root domain (e.g., .example.com). If you use a third-party auth provider, consult their docs on how to do this. For example, for Better Auth, this is done with the crossSubDomainCookies feature. Do not set SameSite=None on cookies used for authentication with Zero. Because Zero uses WebSockets, setting SameSite=None can expose your application to Cross-Site WebSocket Hijacking (CSWSH) attacks. Use SameSite=Lax (the browser default) or SameSite=Strict instead. Tokens Zero also supports token-based authentication. If you have an opaque auth token, such as a JWT or a token from your auth provider, you can pass it to Zero's auth parameter: const opts: ZeroOptions = { // ... auth: token } Zero will forward this token to your mutate and queries endpoints in an Authorization: Bearer <token> header, which you can use to authenticate the request as normal: export async function handleMutate(request: Request) { const session = await authenticate( request.headers.get('Authorization') ) // handle mutate request ... } Auth Failure and Refresh To mark a request as unauthorized, return a 401 or 403 status code from your queries or mutate endpoint. export async function handleMutate(request: Request) { const session = await authenticate( request.headers.get('Authorization') ) if (!session) { // can be 401 or 403 return json({error: 'Unauthorized'}, {status: 401}) } // handle mutate request ... } This will cause Zero to disconnect from zero-cache and the connection status will change to needs-auth. You can then re-authenticate the user and call zero.connection.connect() to reconnect to zero-cache: function NeedsAuthDialog() { const connectionState = useConnectionState() const refreshCookie = async () => { await login() // no token needed since we use cookie auth zero.connection.connect() } if (connectionState.name === 'needs-auth') { return ( <div> <h1>Authentication Required</h1> <button onClick={refreshCookie}>Login</button> </div> ) } return null } Or, if you aren't using cookie auth: function NeedsAuthDialog() { const connectionState = useConnectionState() const refreshAuthToken = async () => { const token = await fetchNewToken() // pass a new token to reconnect to zero-cache zero.connection.connect({auth: token}) } if (connectionState.name === 'needs-auth') { return ( <div> <h1>Authentication Required</h1> <button onClick={refreshAuthToken}>Login</button> </div> ) } return null } Context When a user is authenticated, you will want to know who they are in your queries and mutators to enforce permissions. To do this, define a Context type that includes the user's ID and any other relevant information, then register that type with Zero: export type ZeroContext = { userID: string role: 'admin' | 'user' } declare module '@rocicorp/zero' { interface DefaultTypes { context: ZeroContext } } Then pass an instance of this context when instantiating Zero: const opts: ZeroOptions = { // ... context: { userID: 'user-123', role: 'admin' } } On the server-side, you will also pass an instance of this context when invoking your queries and mutators: const query = mustGetQuery(queries, name) query.fn({args, ctx}) // or const mutator = mustGetMutator(mutators, name) mutator.fn({tx, args, ctx}) You can then access the context within your queries and mutators to implement permissions. Permission Patterns Zero does not have (or need) a first-class permission system like RLS. Instead, you implement permissions by authenticating the user in your queries and mutators endpoints, and creating a Context object that contains the user's ID and other information. This context is passed to your queries and mutators and used to control what data the user can access. Here are a collection of common permissions patterns and how to implement them in Zero. Read Permissions Only Owned Rows // Use the context's `userID` to filter the rows to only the // ones owned by the user. const myPosts = defineQuery(({ctx: {userID}}) => { return zql.post.where('authorID', userID) }) Owned or Shared Rows // Use the context's `userID` to filter the rows to only the // ones owned by the user or shared with the user. const allowedPosts = defineQuery(({ctx: {userID}}) => { return zql.post.where(({cmp, exists, or}) => or( cmp('authorID', userID), exists('sharedWith', q => q.where('userID', userID)) ) ) }) Owned Rows or All if Admin const allowedPosts = defineQuery( ({ctx: {userID, role}}) => { if (role === 'admin') { return zql.post } return zql.post.where('authorID', userID) } ) Write Permissions Enforce Ownership // All created items are owned by the user who created them. const createPost = defineMutator( z.object({ id: z.string(), title: z.string(), content: z.string() }), (tx, {ctx: {userID}, args: {id, title, content}}) => { return zql.post.insert({ id, title, content, authorID: userID }) } ) Edit Owned Rows const updatePost = defineMutator( z.object({ id: z.string(), content: z.string().optional() }), (tx, {ctx: {userID}, args: {id, content}}) => { const prev = await tx.run( zql.post.where('id', id).one() ) if (!prev) { return } if (prev.authorID !== userID) { throw new Error('Access denied') } return zql.post.update({ id, content }) } ) Edit Owned or Shared Rows const updatePost = defineMutator( z.object({ id: z.string(), content: z.string().optional() }), (tx, {ctx: {userID}, args: {id, content}}) => { const prev = await tx.run( zql.post .where('id', id) .related('sharedWith', q => q.where('userID', userID) ) .one() ) if (!prev) { return } if ( prev.authorID !== userID && prev.sharedWith.length === 0 ) { throw new Error('Access denied') } return zql.post.update({ id, content }) } ) Edit Owned or All if Admin const updatePost = defineMutator( z.object({ id: z.string(), content: z.string().optional() }), (tx, {ctx: {role, userID}, args: {id, content}}) => { const prev = await tx.run( zql.post.where('id', id).one() ) if (!prev) { return } if (role !== 'admin' && prev.authorID !== userID) { throw new Error('Access denied') } return zql.post.update({ id, content }) } ) Logging Out When a user logs out, you should consider what should happen to the synced data. If you do nothing, the synced data will be left on the device. The next login will be a little faster because Zero doesn't have to resync that data from scratch. But also, the data will be left on the device indefinitely which could be undesirable for privacy and security. If you instead want to clear data on logout, Zero provides the dropAllDatabases function: import {dropAllDatabases} from '@rocicorp/zero' // Returns an object with: // - The names of the successfully dropped databases // - Any errors encountered while dropping const {dropped, errors} = await dropAllDatabases()",
    "headings": [
      {
        "text": "Setting userID",
        "id": "setting-userid"
      },
      {
        "text": "Sending Credentials",
        "id": "sending-credentials"
      },
      {
        "text": "Cookies",
        "id": "cookies"
      },
      {
        "text": "Deployment",
        "id": "deployment"
      },
      {
        "text": "Tokens",
        "id": "tokens"
      },
      {
        "text": "Auth Failure and Refresh",
        "id": "auth-failure-and-refresh"
      },
      {
        "text": "Context",
        "id": "context"
      },
      {
        "text": "Permission Patterns",
        "id": "permission-patterns"
      },
      {
        "text": "Read Permissions",
        "id": "read-permissions"
      },
      {
        "text": "Only Owned Rows",
        "id": "only-owned-rows"
      },
      {
        "text": "Owned or Shared Rows",
        "id": "owned-or-shared-rows"
      },
      {
        "text": "Owned Rows or All if Admin",
        "id": "owned-rows-or-all-if-admin"
      },
      {
        "text": "Write Permissions",
        "id": "write-permissions"
      },
      {
        "text": "Enforce Ownership",
        "id": "enforce-ownership"
      },
      {
        "text": "Edit Owned Rows",
        "id": "edit-owned-rows"
      },
      {
        "text": "Edit Owned or Shared Rows",
        "id": "edit-owned-or-shared-rows"
      },
      {
        "text": "Edit Owned or All if Admin",
        "id": "edit-owned-or-all-if-admin"
      },
      {
        "text": "Logging Out",
        "id": "logging-out"
      }
    ],
    "kind": "page"
  },
  {
    "id": "62-auth#setting-userid",
    "title": "Authentication",
    "searchTitle": "Setting userID",
    "sectionTitle": "Setting userID",
    "sectionId": "setting-userid",
    "url": "/docs/auth",
    "content": "Because multiple users can share the same browser, Zero requires that you provide a userID parameter on construction: import {ZeroProvider} from '@rocicorp/zero/react' import type {ZeroOptions} from '@rocicorp/zero' const opts: ZeroOptions = { // ... userID: 'user-123' } return ( <ZeroProvider {...opts}> <App /> </ZeroProvider> )import {ZeroProvider} from '@rocicorp/zero/solid' import type {ZeroOptions} from '@rocicorp/zero' const opts: ZeroOptions = { // ... userID: 'user-123' } return ( <ZeroProvider {...opts}> <App /> </ZeroProvider> )import {Zero} from '@rocicorp/zero' import type {ZeroOptions} from '@rocicorp/zero' const opts: ZeroOptions = { // ... userID: 'user-123' } const zero = new Zero(opts) If the user is not logged in, just pass empty string or some other constant value: const opts: ZeroOptions = { // ... userID: 'anon' } Zero segregates the client-side storage for each user. This allows users to quickly switch between multiple users and accounts without resyncing. All users that have access to a browser profile have access to the same IndexedDB instances. There is nothing that Zero can do about this ‚Äì users can just open the folder where the data is stored and look inside it. If you have more than one set of Zero data per-user (i.e., for different apps in the same domain), you can additionally use the storageKey parameter: const opts: ZeroOptions = { // ... userID: 'user-123', storageKey: 'my-app' } If specified, storageKey is concatenated along with userID and other internal Zero information to form a unique IndexedDB database name.",
    "kind": "section"
  },
  {
    "id": "63-auth#sending-credentials",
    "title": "Authentication",
    "searchTitle": "Sending Credentials",
    "sectionTitle": "Sending Credentials",
    "sectionId": "sending-credentials",
    "url": "/docs/auth",
    "content": "You can send credentials using either cookies or tokens. Cookies The most common way to authenticate Zero is with cookies. To enable it, set the ZERO_QUERY_FORWARD_COOKIES and ZERO_MUTATE_FORWARD_COOKIES options to true: export ZERO_QUERY_FORWARD_COOKIES=\"true\" export ZERO_MUTATE_FORWARD_COOKIES=\"true\" # run zero-cache, e.g. `npx zero-cache-dev` Zero-cache will then forward all cookies sent to cacheURL to your mutate and queries endpoints: const opts: ZeroOptions = { schema, // Cookies sent to zero.example.com will be forwarded to // api.example.com/mutate and api.example.com/queries. cacheURL: 'https://zero.example.com', mutateURL: 'https://api.example.com/mutate', queryURL: 'https://api.example.com/queries' } Cookies will show up in the normal HTTP Cookie header and you can authenticate these endpoints just like you would any API request. Deployment In order for cookie auth to work, the browser must send your frontend's cookies to zero-cache, so that zero-cache can forward them to your API. During development, this works automatically as long as your frontend and zero-cache are both running on localhost with different ports. Browsers send cookies based on domain name, not port number, so cookies set by localhost:3000 are also sent to localhost:4848. For production you'll need to do two things: Run zero-cache on a subdomain of your main site (e.g., zero.example.com if your main site is example.com). Consult your hosting provider's docs, or your favorite LLM for how to configure this. Set cookies from your main site with the Domain attribute set to your root domain (e.g., .example.com). If you use a third-party auth provider, consult their docs on how to do this. For example, for Better Auth, this is done with the crossSubDomainCookies feature. Do not set SameSite=None on cookies used for authentication with Zero. Because Zero uses WebSockets, setting SameSite=None can expose your application to Cross-Site WebSocket Hijacking (CSWSH) attacks. Use SameSite=Lax (the browser default) or SameSite=Strict instead. Tokens Zero also supports token-based authentication. If you have an opaque auth token, such as a JWT or a token from your auth provider, you can pass it to Zero's auth parameter: const opts: ZeroOptions = { // ... auth: token } Zero will forward this token to your mutate and queries endpoints in an Authorization: Bearer <token> header, which you can use to authenticate the request as normal: export async function handleMutate(request: Request) { const session = await authenticate( request.headers.get('Authorization') ) // handle mutate request ... }",
    "kind": "section"
  },
  {
    "id": "64-auth#cookies",
    "title": "Authentication",
    "searchTitle": "Cookies",
    "sectionTitle": "Cookies",
    "sectionId": "cookies",
    "url": "/docs/auth",
    "content": "The most common way to authenticate Zero is with cookies. To enable it, set the ZERO_QUERY_FORWARD_COOKIES and ZERO_MUTATE_FORWARD_COOKIES options to true: export ZERO_QUERY_FORWARD_COOKIES=\"true\" export ZERO_MUTATE_FORWARD_COOKIES=\"true\" # run zero-cache, e.g. `npx zero-cache-dev` Zero-cache will then forward all cookies sent to cacheURL to your mutate and queries endpoints: const opts: ZeroOptions = { schema, // Cookies sent to zero.example.com will be forwarded to // api.example.com/mutate and api.example.com/queries. cacheURL: 'https://zero.example.com', mutateURL: 'https://api.example.com/mutate', queryURL: 'https://api.example.com/queries' } Cookies will show up in the normal HTTP Cookie header and you can authenticate these endpoints just like you would any API request. Deployment In order for cookie auth to work, the browser must send your frontend's cookies to zero-cache, so that zero-cache can forward them to your API. During development, this works automatically as long as your frontend and zero-cache are both running on localhost with different ports. Browsers send cookies based on domain name, not port number, so cookies set by localhost:3000 are also sent to localhost:4848. For production you'll need to do two things: Run zero-cache on a subdomain of your main site (e.g., zero.example.com if your main site is example.com). Consult your hosting provider's docs, or your favorite LLM for how to configure this. Set cookies from your main site with the Domain attribute set to your root domain (e.g., .example.com). If you use a third-party auth provider, consult their docs on how to do this. For example, for Better Auth, this is done with the crossSubDomainCookies feature. Do not set SameSite=None on cookies used for authentication with Zero. Because Zero uses WebSockets, setting SameSite=None can expose your application to Cross-Site WebSocket Hijacking (CSWSH) attacks. Use SameSite=Lax (the browser default) or SameSite=Strict instead.",
    "kind": "section"
  },
  {
    "id": "65-auth#deployment",
    "title": "Authentication",
    "searchTitle": "Deployment",
    "sectionTitle": "Deployment",
    "sectionId": "deployment",
    "url": "/docs/auth",
    "content": "In order for cookie auth to work, the browser must send your frontend's cookies to zero-cache, so that zero-cache can forward them to your API. During development, this works automatically as long as your frontend and zero-cache are both running on localhost with different ports. Browsers send cookies based on domain name, not port number, so cookies set by localhost:3000 are also sent to localhost:4848. For production you'll need to do two things: Run zero-cache on a subdomain of your main site (e.g., zero.example.com if your main site is example.com). Consult your hosting provider's docs, or your favorite LLM for how to configure this. Set cookies from your main site with the Domain attribute set to your root domain (e.g., .example.com). If you use a third-party auth provider, consult their docs on how to do this. For example, for Better Auth, this is done with the crossSubDomainCookies feature. Do not set SameSite=None on cookies used for authentication with Zero. Because Zero uses WebSockets, setting SameSite=None can expose your application to Cross-Site WebSocket Hijacking (CSWSH) attacks. Use SameSite=Lax (the browser default) or SameSite=Strict instead.",
    "kind": "section"
  },
  {
    "id": "66-auth#tokens",
    "title": "Authentication",
    "searchTitle": "Tokens",
    "sectionTitle": "Tokens",
    "sectionId": "tokens",
    "url": "/docs/auth",
    "content": "Zero also supports token-based authentication. If you have an opaque auth token, such as a JWT or a token from your auth provider, you can pass it to Zero's auth parameter: const opts: ZeroOptions = { // ... auth: token } Zero will forward this token to your mutate and queries endpoints in an Authorization: Bearer <token> header, which you can use to authenticate the request as normal: export async function handleMutate(request: Request) { const session = await authenticate( request.headers.get('Authorization') ) // handle mutate request ... }",
    "kind": "section"
  },
  {
    "id": "67-auth#auth-failure-and-refresh",
    "title": "Authentication",
    "searchTitle": "Auth Failure and Refresh",
    "sectionTitle": "Auth Failure and Refresh",
    "sectionId": "auth-failure-and-refresh",
    "url": "/docs/auth",
    "content": "To mark a request as unauthorized, return a 401 or 403 status code from your queries or mutate endpoint. export async function handleMutate(request: Request) { const session = await authenticate( request.headers.get('Authorization') ) if (!session) { // can be 401 or 403 return json({error: 'Unauthorized'}, {status: 401}) } // handle mutate request ... } This will cause Zero to disconnect from zero-cache and the connection status will change to needs-auth. You can then re-authenticate the user and call zero.connection.connect() to reconnect to zero-cache: function NeedsAuthDialog() { const connectionState = useConnectionState() const refreshCookie = async () => { await login() // no token needed since we use cookie auth zero.connection.connect() } if (connectionState.name === 'needs-auth') { return ( <div> <h1>Authentication Required</h1> <button onClick={refreshCookie}>Login</button> </div> ) } return null } Or, if you aren't using cookie auth: function NeedsAuthDialog() { const connectionState = useConnectionState() const refreshAuthToken = async () => { const token = await fetchNewToken() // pass a new token to reconnect to zero-cache zero.connection.connect({auth: token}) } if (connectionState.name === 'needs-auth') { return ( <div> <h1>Authentication Required</h1> <button onClick={refreshAuthToken}>Login</button> </div> ) } return null }",
    "kind": "section"
  },
  {
    "id": "68-auth#context",
    "title": "Authentication",
    "searchTitle": "Context",
    "sectionTitle": "Context",
    "sectionId": "context",
    "url": "/docs/auth",
    "content": "When a user is authenticated, you will want to know who they are in your queries and mutators to enforce permissions. To do this, define a Context type that includes the user's ID and any other relevant information, then register that type with Zero: export type ZeroContext = { userID: string role: 'admin' | 'user' } declare module '@rocicorp/zero' { interface DefaultTypes { context: ZeroContext } } Then pass an instance of this context when instantiating Zero: const opts: ZeroOptions = { // ... context: { userID: 'user-123', role: 'admin' } } On the server-side, you will also pass an instance of this context when invoking your queries and mutators: const query = mustGetQuery(queries, name) query.fn({args, ctx}) // or const mutator = mustGetMutator(mutators, name) mutator.fn({tx, args, ctx}) You can then access the context within your queries and mutators to implement permissions.",
    "kind": "section"
  },
  {
    "id": "69-auth#permission-patterns",
    "title": "Authentication",
    "searchTitle": "Permission Patterns",
    "sectionTitle": "Permission Patterns",
    "sectionId": "permission-patterns",
    "url": "/docs/auth",
    "content": "Zero does not have (or need) a first-class permission system like RLS. Instead, you implement permissions by authenticating the user in your queries and mutators endpoints, and creating a Context object that contains the user's ID and other information. This context is passed to your queries and mutators and used to control what data the user can access. Here are a collection of common permissions patterns and how to implement them in Zero. Read Permissions Only Owned Rows // Use the context's `userID` to filter the rows to only the // ones owned by the user. const myPosts = defineQuery(({ctx: {userID}}) => { return zql.post.where('authorID', userID) }) Owned or Shared Rows // Use the context's `userID` to filter the rows to only the // ones owned by the user or shared with the user. const allowedPosts = defineQuery(({ctx: {userID}}) => { return zql.post.where(({cmp, exists, or}) => or( cmp('authorID', userID), exists('sharedWith', q => q.where('userID', userID)) ) ) }) Owned Rows or All if Admin const allowedPosts = defineQuery( ({ctx: {userID, role}}) => { if (role === 'admin') { return zql.post } return zql.post.where('authorID', userID) } ) Write Permissions Enforce Ownership // All created items are owned by the user who created them. const createPost = defineMutator( z.object({ id: z.string(), title: z.string(), content: z.string() }), (tx, {ctx: {userID}, args: {id, title, content}}) => { return zql.post.insert({ id, title, content, authorID: userID }) } ) Edit Owned Rows const updatePost = defineMutator( z.object({ id: z.string(), content: z.string().optional() }), (tx, {ctx: {userID}, args: {id, content}}) => { const prev = await tx.run( zql.post.where('id', id).one() ) if (!prev) { return } if (prev.authorID !== userID) { throw new Error('Access denied') } return zql.post.update({ id, content }) } ) Edit Owned or Shared Rows const updatePost = defineMutator( z.object({ id: z.string(), content: z.string().optional() }), (tx, {ctx: {userID}, args: {id, content}}) => { const prev = await tx.run( zql.post .where('id', id) .related('sharedWith', q => q.where('userID', userID) ) .one() ) if (!prev) { return } if ( prev.authorID !== userID && prev.sharedWith.length === 0 ) { throw new Error('Access denied') } return zql.post.update({ id, content }) } ) Edit Owned or All if Admin const updatePost = defineMutator( z.object({ id: z.string(), content: z.string().optional() }), (tx, {ctx: {role, userID}, args: {id, content}}) => { const prev = await tx.run( zql.post.where('id', id).one() ) if (!prev) { return } if (role !== 'admin' && prev.authorID !== userID) { throw new Error('Access denied') } return zql.post.update({ id, content }) } )",
    "kind": "section"
  },
  {
    "id": "70-auth#read-permissions",
    "title": "Authentication",
    "searchTitle": "Read Permissions",
    "sectionTitle": "Read Permissions",
    "sectionId": "read-permissions",
    "url": "/docs/auth",
    "content": "Only Owned Rows // Use the context's `userID` to filter the rows to only the // ones owned by the user. const myPosts = defineQuery(({ctx: {userID}}) => { return zql.post.where('authorID', userID) }) Owned or Shared Rows // Use the context's `userID` to filter the rows to only the // ones owned by the user or shared with the user. const allowedPosts = defineQuery(({ctx: {userID}}) => { return zql.post.where(({cmp, exists, or}) => or( cmp('authorID', userID), exists('sharedWith', q => q.where('userID', userID)) ) ) }) Owned Rows or All if Admin const allowedPosts = defineQuery( ({ctx: {userID, role}}) => { if (role === 'admin') { return zql.post } return zql.post.where('authorID', userID) } )",
    "kind": "section"
  },
  {
    "id": "71-auth#only-owned-rows",
    "title": "Authentication",
    "searchTitle": "Only Owned Rows",
    "sectionTitle": "Only Owned Rows",
    "sectionId": "only-owned-rows",
    "url": "/docs/auth",
    "content": "// Use the context's `userID` to filter the rows to only the // ones owned by the user. const myPosts = defineQuery(({ctx: {userID}}) => { return zql.post.where('authorID', userID) })",
    "kind": "section"
  },
  {
    "id": "72-auth#owned-or-shared-rows",
    "title": "Authentication",
    "searchTitle": "Owned or Shared Rows",
    "sectionTitle": "Owned or Shared Rows",
    "sectionId": "owned-or-shared-rows",
    "url": "/docs/auth",
    "content": "// Use the context's `userID` to filter the rows to only the // ones owned by the user or shared with the user. const allowedPosts = defineQuery(({ctx: {userID}}) => { return zql.post.where(({cmp, exists, or}) => or( cmp('authorID', userID), exists('sharedWith', q => q.where('userID', userID)) ) ) })",
    "kind": "section"
  },
  {
    "id": "73-auth#owned-rows-or-all-if-admin",
    "title": "Authentication",
    "searchTitle": "Owned Rows or All if Admin",
    "sectionTitle": "Owned Rows or All if Admin",
    "sectionId": "owned-rows-or-all-if-admin",
    "url": "/docs/auth",
    "content": "const allowedPosts = defineQuery( ({ctx: {userID, role}}) => { if (role === 'admin') { return zql.post } return zql.post.where('authorID', userID) } )",
    "kind": "section"
  },
  {
    "id": "74-auth#write-permissions",
    "title": "Authentication",
    "searchTitle": "Write Permissions",
    "sectionTitle": "Write Permissions",
    "sectionId": "write-permissions",
    "url": "/docs/auth",
    "content": "Enforce Ownership // All created items are owned by the user who created them. const createPost = defineMutator( z.object({ id: z.string(), title: z.string(), content: z.string() }), (tx, {ctx: {userID}, args: {id, title, content}}) => { return zql.post.insert({ id, title, content, authorID: userID }) } ) Edit Owned Rows const updatePost = defineMutator( z.object({ id: z.string(), content: z.string().optional() }), (tx, {ctx: {userID}, args: {id, content}}) => { const prev = await tx.run( zql.post.where('id', id).one() ) if (!prev) { return } if (prev.authorID !== userID) { throw new Error('Access denied') } return zql.post.update({ id, content }) } ) Edit Owned or Shared Rows const updatePost = defineMutator( z.object({ id: z.string(), content: z.string().optional() }), (tx, {ctx: {userID}, args: {id, content}}) => { const prev = await tx.run( zql.post .where('id', id) .related('sharedWith', q => q.where('userID', userID) ) .one() ) if (!prev) { return } if ( prev.authorID !== userID && prev.sharedWith.length === 0 ) { throw new Error('Access denied') } return zql.post.update({ id, content }) } ) Edit Owned or All if Admin const updatePost = defineMutator( z.object({ id: z.string(), content: z.string().optional() }), (tx, {ctx: {role, userID}, args: {id, content}}) => { const prev = await tx.run( zql.post.where('id', id).one() ) if (!prev) { return } if (role !== 'admin' && prev.authorID !== userID) { throw new Error('Access denied') } return zql.post.update({ id, content }) } )",
    "kind": "section"
  },
  {
    "id": "75-auth#enforce-ownership",
    "title": "Authentication",
    "searchTitle": "Enforce Ownership",
    "sectionTitle": "Enforce Ownership",
    "sectionId": "enforce-ownership",
    "url": "/docs/auth",
    "content": "// All created items are owned by the user who created them. const createPost = defineMutator( z.object({ id: z.string(), title: z.string(), content: z.string() }), (tx, {ctx: {userID}, args: {id, title, content}}) => { return zql.post.insert({ id, title, content, authorID: userID }) } )",
    "kind": "section"
  },
  {
    "id": "76-auth#edit-owned-rows",
    "title": "Authentication",
    "searchTitle": "Edit Owned Rows",
    "sectionTitle": "Edit Owned Rows",
    "sectionId": "edit-owned-rows",
    "url": "/docs/auth",
    "content": "const updatePost = defineMutator( z.object({ id: z.string(), content: z.string().optional() }), (tx, {ctx: {userID}, args: {id, content}}) => { const prev = await tx.run( zql.post.where('id', id).one() ) if (!prev) { return } if (prev.authorID !== userID) { throw new Error('Access denied') } return zql.post.update({ id, content }) } )",
    "kind": "section"
  },
  {
    "id": "77-auth#edit-owned-or-shared-rows",
    "title": "Authentication",
    "searchTitle": "Edit Owned or Shared Rows",
    "sectionTitle": "Edit Owned or Shared Rows",
    "sectionId": "edit-owned-or-shared-rows",
    "url": "/docs/auth",
    "content": "const updatePost = defineMutator( z.object({ id: z.string(), content: z.string().optional() }), (tx, {ctx: {userID}, args: {id, content}}) => { const prev = await tx.run( zql.post .where('id', id) .related('sharedWith', q => q.where('userID', userID) ) .one() ) if (!prev) { return } if ( prev.authorID !== userID && prev.sharedWith.length === 0 ) { throw new Error('Access denied') } return zql.post.update({ id, content }) } )",
    "kind": "section"
  },
  {
    "id": "78-auth#edit-owned-or-all-if-admin",
    "title": "Authentication",
    "searchTitle": "Edit Owned or All if Admin",
    "sectionTitle": "Edit Owned or All if Admin",
    "sectionId": "edit-owned-or-all-if-admin",
    "url": "/docs/auth",
    "content": "const updatePost = defineMutator( z.object({ id: z.string(), content: z.string().optional() }), (tx, {ctx: {role, userID}, args: {id, content}}) => { const prev = await tx.run( zql.post.where('id', id).one() ) if (!prev) { return } if (role !== 'admin' && prev.authorID !== userID) { throw new Error('Access denied') } return zql.post.update({ id, content }) } )",
    "kind": "section"
  },
  {
    "id": "79-auth#logging-out",
    "title": "Authentication",
    "searchTitle": "Logging Out",
    "sectionTitle": "Logging Out",
    "sectionId": "logging-out",
    "url": "/docs/auth",
    "content": "When a user logs out, you should consider what should happen to the synced data. If you do nothing, the synced data will be left on the device. The next login will be a little faster because Zero doesn't have to resync that data from scratch. But also, the data will be left on the device indefinitely which could be undesirable for privacy and security. If you instead want to clear data on logout, Zero provides the dropAllDatabases function: import {dropAllDatabases} from '@rocicorp/zero' // Returns an object with: // - The names of the successfully dropped databases // - Any errors encountered while dropping const {dropped, errors} = await dropAllDatabases()",
    "kind": "section"
  },
  {
    "id": "1-community",
    "title": "From the Community",
    "searchTitle": "From the Community",
    "url": "/docs/community",
    "content": "Integrations with various tools, built by the Zero dev community. If you have made something that should be here, send us a pull request. UI Frameworks One is a full-stack React (and React Native!) framework with built-in Zero support. zero-svelte and zero-svelte-query are two different approaches to Zero bindings for Svelte. zero-vue adds Zero bindings to Vue. zero-astro adds Zero bindings to Astro. Miscellaneous undo is a simple undo/redo library that was originally built for Replicache, but works just as well with Zero.",
    "headings": [
      {
        "text": "UI Frameworks",
        "id": "ui-frameworks"
      },
      {
        "text": "Miscellaneous",
        "id": "miscellaneous"
      }
    ],
    "kind": "page"
  },
  {
    "id": "80-community#ui-frameworks",
    "title": "From the Community",
    "searchTitle": "UI Frameworks",
    "sectionTitle": "UI Frameworks",
    "sectionId": "ui-frameworks",
    "url": "/docs/community",
    "content": "One is a full-stack React (and React Native!) framework with built-in Zero support. zero-svelte and zero-svelte-query are two different approaches to Zero bindings for Svelte. zero-vue adds Zero bindings to Vue. zero-astro adds Zero bindings to Astro.",
    "kind": "section"
  },
  {
    "id": "81-community#miscellaneous",
    "title": "From the Community",
    "searchTitle": "Miscellaneous",
    "sectionTitle": "Miscellaneous",
    "sectionId": "miscellaneous",
    "url": "/docs/community",
    "content": "undo is a simple undo/redo library that was originally built for Replicache, but works just as well with Zero.",
    "kind": "section"
  },
  {
    "id": "2-connecting-to-postgres",
    "title": "Connecting to Postgres",
    "searchTitle": "Connecting to Postgres",
    "url": "/docs/connecting-to-postgres",
    "content": "In the future, Zero will work with many different backend databases. Today only Postgres is supported. Specifically, Zero requires Postgres v15.0 or higher, and support for logical replication. Here are some common Postgres options and what we know about their support level: Event Triggers Zero uses Postgres ‚ÄúEvent Triggers‚Äù when possible to implement high-quality, efficient schema migration. Some hosted Postgres providers don‚Äôt provide access to Event Triggers. Zero still works out of the box with these providers, but for correctness, any schema change triggers a full reset of all server-side and client-side state. For small databases (< 10GB) this can be OK, but for bigger databases we recommend choosing a provider that grants access to Event Triggers. Configuration WAL Level The Postgres wal_level config parameter has to be set to logical. You can check what level your pg has with this command: psql -c 'SHOW wal_level' If it doesn‚Äôt output logical then you need to change the wal level. To do this, run: psql -c \"ALTER SYSTEM SET wal_level = 'logical';\" Then restart Postgres. On most pg systems you can do this like so: data_dir=$(psql -t -A -c 'SHOW data_directory') pg_ctl -D \"$data_dir\" restart After your server restarts, show the wal_level again to ensure it has changed: psql -c 'SHOW wal_level' Bounding WAL Size For development databases, you can set a max_slot_wal_keep_size value in Postgres. This will help limit the amount of WAL kept around. This is a configuration parameter that bounds the amount of WAL kept around for replication slots, and invalidates the slots that are too far behind. zero-cache will automatically detect if the replication slot has been invalidated and re-sync replicas from scratch. This configuration can cause problems like slot has been invalidated because it exceeded the maximum reserved size and is not recommended for production databases. Provider-Specific Notes Google Cloud SQL Zero works with Google Cloud SQL out of the box. In many configurations, when you connect with a user that has sufficient privileges, zero-cache will create its default publication automatically. If your Cloud SQL user does not have permission to create publications, you can still use Zero by creating a publication manually and then specifying that publication name in App Publications when running zero-cache. On Google Cloud SQL for PostgreSQL, enable logical decoding by turning on the instance flag cloudsql.logical_decoding. You do not set wal_level directly on Cloud SQL. See Google's documentation for details: Configure logical replication. Fly.io Fly does not support TLS on their internal networks. If you run both zero-cache and Postgres on Fly, you need to stop zero-cache from trying to use TLS to talk to Postgres. You can do this by adding the sslmode=disable query parameter to your connection strings from zero-cache. Supabase Postgres Version Supabase requires at least 15.8.1.083 for event trigger support. If you have a lower 15.x, Zero will still work but schema updates will be slower. See Supabase's docs for upgrading your Postgres version. Connection Type In order to connect to Supabase you must use the \"Direct Connection\" style connection string, not the pooler: This is because Zero sets up a logical replication slot, which is only supported with a direct connection. IPv4 You may also need to assign an IPv4 address to your Supabase instance: This will be required if you cannot use IPv6 from wherever zero-cache is running. Most cloud providers support IPv6, but some do not. For example, if you are running zero-cache in AWS, it is possible to use IPv6 but difficult. Hetzner offers cheap hosted VPS that supports IPv6. IPv4 addresses are only supported on the Pro plan and are an extra $4/month. PlanetScale for Postgres PlanetScale doesn't support creating publications with the FOR ALL TABLES clause. Zero typically uses this to create an initial default publication during setup. You can workaround this by creating a publication explicitly listing the tables you want to replicate. Neon Neon fully supports Zero, but you should be aware of how Neon's pricing model and Zero interact. Because Zero keeps an open connection to Postgres to replicate changes, as long as zero-cache is running, Postgres will be running and you will be charged by Neon. For production databases that have enough usage to always be running anyway, this is fine. But for smaller applications that would otherwise not always be running, this can create a surprisingly high bill. You may want to choose a provider that charge a flat monthly rate instead. Also some users choose Neon because they hope to use branching for previews. Note that Zero doesn't support this usage model well yet, and if not done with care, Zero can end up keeping each Neon preview branch running too üò≥. We are actively working on better preview support.",
    "headings": [
      {
        "text": "Event Triggers",
        "id": "event-triggers"
      },
      {
        "text": "Configuration",
        "id": "configuration"
      },
      {
        "text": "WAL Level",
        "id": "wal-level"
      },
      {
        "text": "Bounding WAL Size",
        "id": "bounding-wal-size"
      },
      {
        "text": "Provider-Specific Notes",
        "id": "provider-specific-notes"
      },
      {
        "text": "Google Cloud SQL",
        "id": "google-cloud-sql"
      },
      {
        "text": "Fly.io",
        "id": "flyio"
      },
      {
        "text": "Supabase",
        "id": "supabase"
      },
      {
        "text": "Postgres Version",
        "id": "postgres-version"
      },
      {
        "text": "Connection Type",
        "id": "connection-type"
      },
      {
        "text": "IPv4",
        "id": "ipv4"
      },
      {
        "text": "PlanetScale for Postgres",
        "id": "planetscale-for-postgres"
      },
      {
        "text": "Neon",
        "id": "neon"
      }
    ],
    "kind": "page"
  },
  {
    "id": "82-connecting-to-postgres#event-triggers",
    "title": "Connecting to Postgres",
    "searchTitle": "Event Triggers",
    "sectionTitle": "Event Triggers",
    "sectionId": "event-triggers",
    "url": "/docs/connecting-to-postgres",
    "content": "Zero uses Postgres ‚ÄúEvent Triggers‚Äù when possible to implement high-quality, efficient schema migration. Some hosted Postgres providers don‚Äôt provide access to Event Triggers. Zero still works out of the box with these providers, but for correctness, any schema change triggers a full reset of all server-side and client-side state. For small databases (< 10GB) this can be OK, but for bigger databases we recommend choosing a provider that grants access to Event Triggers.",
    "kind": "section"
  },
  {
    "id": "83-connecting-to-postgres#configuration",
    "title": "Connecting to Postgres",
    "searchTitle": "Configuration",
    "sectionTitle": "Configuration",
    "sectionId": "configuration",
    "url": "/docs/connecting-to-postgres",
    "content": "WAL Level The Postgres wal_level config parameter has to be set to logical. You can check what level your pg has with this command: psql -c 'SHOW wal_level' If it doesn‚Äôt output logical then you need to change the wal level. To do this, run: psql -c \"ALTER SYSTEM SET wal_level = 'logical';\" Then restart Postgres. On most pg systems you can do this like so: data_dir=$(psql -t -A -c 'SHOW data_directory') pg_ctl -D \"$data_dir\" restart After your server restarts, show the wal_level again to ensure it has changed: psql -c 'SHOW wal_level' Bounding WAL Size For development databases, you can set a max_slot_wal_keep_size value in Postgres. This will help limit the amount of WAL kept around. This is a configuration parameter that bounds the amount of WAL kept around for replication slots, and invalidates the slots that are too far behind. zero-cache will automatically detect if the replication slot has been invalidated and re-sync replicas from scratch. This configuration can cause problems like slot has been invalidated because it exceeded the maximum reserved size and is not recommended for production databases.",
    "kind": "section"
  },
  {
    "id": "84-connecting-to-postgres#wal-level",
    "title": "Connecting to Postgres",
    "searchTitle": "WAL Level",
    "sectionTitle": "WAL Level",
    "sectionId": "wal-level",
    "url": "/docs/connecting-to-postgres",
    "content": "The Postgres wal_level config parameter has to be set to logical. You can check what level your pg has with this command: psql -c 'SHOW wal_level' If it doesn‚Äôt output logical then you need to change the wal level. To do this, run: psql -c \"ALTER SYSTEM SET wal_level = 'logical';\" Then restart Postgres. On most pg systems you can do this like so: data_dir=$(psql -t -A -c 'SHOW data_directory') pg_ctl -D \"$data_dir\" restart After your server restarts, show the wal_level again to ensure it has changed: psql -c 'SHOW wal_level'",
    "kind": "section"
  },
  {
    "id": "85-connecting-to-postgres#bounding-wal-size",
    "title": "Connecting to Postgres",
    "searchTitle": "Bounding WAL Size",
    "sectionTitle": "Bounding WAL Size",
    "sectionId": "bounding-wal-size",
    "url": "/docs/connecting-to-postgres",
    "content": "For development databases, you can set a max_slot_wal_keep_size value in Postgres. This will help limit the amount of WAL kept around. This is a configuration parameter that bounds the amount of WAL kept around for replication slots, and invalidates the slots that are too far behind. zero-cache will automatically detect if the replication slot has been invalidated and re-sync replicas from scratch. This configuration can cause problems like slot has been invalidated because it exceeded the maximum reserved size and is not recommended for production databases.",
    "kind": "section"
  },
  {
    "id": "86-connecting-to-postgres#provider-specific-notes",
    "title": "Connecting to Postgres",
    "searchTitle": "Provider-Specific Notes",
    "sectionTitle": "Provider-Specific Notes",
    "sectionId": "provider-specific-notes",
    "url": "/docs/connecting-to-postgres",
    "content": "Google Cloud SQL Zero works with Google Cloud SQL out of the box. In many configurations, when you connect with a user that has sufficient privileges, zero-cache will create its default publication automatically. If your Cloud SQL user does not have permission to create publications, you can still use Zero by creating a publication manually and then specifying that publication name in App Publications when running zero-cache. On Google Cloud SQL for PostgreSQL, enable logical decoding by turning on the instance flag cloudsql.logical_decoding. You do not set wal_level directly on Cloud SQL. See Google's documentation for details: Configure logical replication. Fly.io Fly does not support TLS on their internal networks. If you run both zero-cache and Postgres on Fly, you need to stop zero-cache from trying to use TLS to talk to Postgres. You can do this by adding the sslmode=disable query parameter to your connection strings from zero-cache. Supabase Postgres Version Supabase requires at least 15.8.1.083 for event trigger support. If you have a lower 15.x, Zero will still work but schema updates will be slower. See Supabase's docs for upgrading your Postgres version. Connection Type In order to connect to Supabase you must use the \"Direct Connection\" style connection string, not the pooler: This is because Zero sets up a logical replication slot, which is only supported with a direct connection. IPv4 You may also need to assign an IPv4 address to your Supabase instance: This will be required if you cannot use IPv6 from wherever zero-cache is running. Most cloud providers support IPv6, but some do not. For example, if you are running zero-cache in AWS, it is possible to use IPv6 but difficult. Hetzner offers cheap hosted VPS that supports IPv6. IPv4 addresses are only supported on the Pro plan and are an extra $4/month. PlanetScale for Postgres PlanetScale doesn't support creating publications with the FOR ALL TABLES clause. Zero typically uses this to create an initial default publication during setup. You can workaround this by creating a publication explicitly listing the tables you want to replicate. Neon Neon fully supports Zero, but you should be aware of how Neon's pricing model and Zero interact. Because Zero keeps an open connection to Postgres to replicate changes, as long as zero-cache is running, Postgres will be running and you will be charged by Neon. For production databases that have enough usage to always be running anyway, this is fine. But for smaller applications that would otherwise not always be running, this can create a surprisingly high bill. You may want to choose a provider that charge a flat monthly rate instead. Also some users choose Neon because they hope to use branching for previews. Note that Zero doesn't support this usage model well yet, and if not done with care, Zero can end up keeping each Neon preview branch running too üò≥. We are actively working on better preview support.",
    "kind": "section"
  },
  {
    "id": "87-connecting-to-postgres#google-cloud-sql",
    "title": "Connecting to Postgres",
    "searchTitle": "Google Cloud SQL",
    "sectionTitle": "Google Cloud SQL",
    "sectionId": "google-cloud-sql",
    "url": "/docs/connecting-to-postgres",
    "content": "Zero works with Google Cloud SQL out of the box. In many configurations, when you connect with a user that has sufficient privileges, zero-cache will create its default publication automatically. If your Cloud SQL user does not have permission to create publications, you can still use Zero by creating a publication manually and then specifying that publication name in App Publications when running zero-cache. On Google Cloud SQL for PostgreSQL, enable logical decoding by turning on the instance flag cloudsql.logical_decoding. You do not set wal_level directly on Cloud SQL. See Google's documentation for details: Configure logical replication.",
    "kind": "section"
  },
  {
    "id": "88-connecting-to-postgres#flyio",
    "title": "Connecting to Postgres",
    "searchTitle": "Fly.io",
    "sectionTitle": "Fly.io",
    "sectionId": "flyio",
    "url": "/docs/connecting-to-postgres",
    "content": "Fly does not support TLS on their internal networks. If you run both zero-cache and Postgres on Fly, you need to stop zero-cache from trying to use TLS to talk to Postgres. You can do this by adding the sslmode=disable query parameter to your connection strings from zero-cache.",
    "kind": "section"
  },
  {
    "id": "89-connecting-to-postgres#supabase",
    "title": "Connecting to Postgres",
    "searchTitle": "Supabase",
    "sectionTitle": "Supabase",
    "sectionId": "supabase",
    "url": "/docs/connecting-to-postgres",
    "content": "Postgres Version Supabase requires at least 15.8.1.083 for event trigger support. If you have a lower 15.x, Zero will still work but schema updates will be slower. See Supabase's docs for upgrading your Postgres version. Connection Type In order to connect to Supabase you must use the \"Direct Connection\" style connection string, not the pooler: This is because Zero sets up a logical replication slot, which is only supported with a direct connection. IPv4 You may also need to assign an IPv4 address to your Supabase instance: This will be required if you cannot use IPv6 from wherever zero-cache is running. Most cloud providers support IPv6, but some do not. For example, if you are running zero-cache in AWS, it is possible to use IPv6 but difficult. Hetzner offers cheap hosted VPS that supports IPv6. IPv4 addresses are only supported on the Pro plan and are an extra $4/month.",
    "kind": "section"
  },
  {
    "id": "90-connecting-to-postgres#postgres-version",
    "title": "Connecting to Postgres",
    "searchTitle": "Postgres Version",
    "sectionTitle": "Postgres Version",
    "sectionId": "postgres-version",
    "url": "/docs/connecting-to-postgres",
    "content": "Supabase requires at least 15.8.1.083 for event trigger support. If you have a lower 15.x, Zero will still work but schema updates will be slower. See Supabase's docs for upgrading your Postgres version.",
    "kind": "section"
  },
  {
    "id": "91-connecting-to-postgres#connection-type",
    "title": "Connecting to Postgres",
    "searchTitle": "Connection Type",
    "sectionTitle": "Connection Type",
    "sectionId": "connection-type",
    "url": "/docs/connecting-to-postgres",
    "content": "In order to connect to Supabase you must use the \"Direct Connection\" style connection string, not the pooler: This is because Zero sets up a logical replication slot, which is only supported with a direct connection.",
    "kind": "section"
  },
  {
    "id": "92-connecting-to-postgres#ipv4",
    "title": "Connecting to Postgres",
    "searchTitle": "IPv4",
    "sectionTitle": "IPv4",
    "sectionId": "ipv4",
    "url": "/docs/connecting-to-postgres",
    "content": "You may also need to assign an IPv4 address to your Supabase instance: This will be required if you cannot use IPv6 from wherever zero-cache is running. Most cloud providers support IPv6, but some do not. For example, if you are running zero-cache in AWS, it is possible to use IPv6 but difficult. Hetzner offers cheap hosted VPS that supports IPv6. IPv4 addresses are only supported on the Pro plan and are an extra $4/month.",
    "kind": "section"
  },
  {
    "id": "93-connecting-to-postgres#planetscale-for-postgres",
    "title": "Connecting to Postgres",
    "searchTitle": "PlanetScale for Postgres",
    "sectionTitle": "PlanetScale for Postgres",
    "sectionId": "planetscale-for-postgres",
    "url": "/docs/connecting-to-postgres",
    "content": "PlanetScale doesn't support creating publications with the FOR ALL TABLES clause. Zero typically uses this to create an initial default publication during setup. You can workaround this by creating a publication explicitly listing the tables you want to replicate.",
    "kind": "section"
  },
  {
    "id": "94-connecting-to-postgres#neon",
    "title": "Connecting to Postgres",
    "searchTitle": "Neon",
    "sectionTitle": "Neon",
    "sectionId": "neon",
    "url": "/docs/connecting-to-postgres",
    "content": "Neon fully supports Zero, but you should be aware of how Neon's pricing model and Zero interact. Because Zero keeps an open connection to Postgres to replicate changes, as long as zero-cache is running, Postgres will be running and you will be charged by Neon. For production databases that have enough usage to always be running anyway, this is fine. But for smaller applications that would otherwise not always be running, this can create a surprisingly high bill. You may want to choose a provider that charge a flat monthly rate instead. Also some users choose Neon because they hope to use branching for previews. Note that Zero doesn't support this usage model well yet, and if not done with care, Zero can end up keeping each Neon preview branch running too üò≥. We are actively working on better preview support.",
    "kind": "section"
  },
  {
    "id": "3-connection",
    "title": "Connection Status",
    "searchTitle": "Connection Status",
    "url": "/docs/connection",
    "content": "Overview Zero manages a persistent connection to zero-cache with the following lifecycle: Usage The current connection state is available in the zero.connection.state property. This is subscribable and also has reactive hooks for React and SolidJS: import {useConnectionState} from '@rocicorp/zero/react' function ConnectionStatus() { const state = useConnectionState() switch (state.name) { case 'connecting': return <div title={state.reason}>Connecting...</div> case 'connected': return <div>Connected</div> case 'disconnected': return <div title={state.reason}>Offline</div> case 'error': return <div title={state.reason}>Error</div> case 'needs-auth': return <div>Session expired</div> default: return null } }import {useConnectionState} from '@rocicorp/zero/solid' function ConnectionStatus() { const state = useConnectionState() return ( <Switch> <Match when={state().name === 'connecting'}> <div title={state().reason}>Connecting...</div> </Match> <Match when={state().name === 'connected'}> <div>Connected</div> </Match> <Match when={state().name === 'disconnected'}> <div title={state().reason}>Offline</div> </Match> <Match when={state().name === 'error'}> <div title={state().reason}>Error</div> </Match> <Match when={state().name === 'needs-auth'}> <div>Session expired</div> </Match> </Switch> ) }zero.connection.state.subscribe(state => { switch (state.name) { case 'connecting': console.log(`Connecting... ${state.reason}`) break case 'connected': console.log('Connected') break case 'disconnected': console.log(`Disconnected ${state.reason}`) break case 'error': console.log(`Error ${state.reason}`) break case 'needs-auth': console.log('Session expired') break default: return null } }) Details Connecting Zero starts in the connecting state. Once the connection is established, it transitions to connected. While connecting, Zero repeatedly tries to connect to zero-cache for 1 minute by default. This timeout can be configured with the disconnectTimeoutMs constructor parameter: const opts: ZeroOptions = { // ... disconnectTimeoutMs: 1000 * 60 * 10 // 10 minutes } Reads and writes are allowed to Zero mutators while connecting. The writes are queued and will be sent when the connection succeeds. This is intended to paper over short connectivity glitches, such as server restarts, walking into an elevator, etc. While you can increase the disconnectTimeoutMs to allow for longer periods of offline operation, this has caveats that you should understand carefully. Please see offline for more information. Disconnecting After the disconnectTimeoutMs elapses, Zero transitions to disconnected. Zero also transitions to disconnected when the tab is hidden for hiddenTabDisconnectDelay (default 5 minutes). While disconnected, Zero continues to try to reconnect to zero-cache every 5 seconds. Reads are allowed while disconnected, but writes are rejected and return an offline error. Errors If zero-cache itself crashes, or if the mutate or query endpoints return a network or HTTP error, Zero transitions to the error state. This type of error is unlikely to resolve just by retrying, so Zero doesn't try. The app can retry the connection manually by calling zero.connection.connect(). Writes are rejected while Zero is in the error state. You can forward connection errors to Sentry (or any error-monitoring tool) by subscribing to zero.connection.state. You can wrap reason in an Error and report it: import * as Sentry from '@sentry/browser' zero.connection.state.subscribe(state => { if (state.name !== 'error') return Sentry.withScope(scope => { scope.setTag('zero.connection.state', state.name) scope.setExtra('zero.connection.reason', state.reason) Sentry.captureException( new Error(`Zero connection error: ${state.reason}`) ) }) }) Auth Required If the mutate or query endpoints return a 401 or 403 status code, Zero transitions to the needs-auth state. The app should refresh the cookie or auth token and retry the connection manually by calling zero.connection.connect(). Writes are rejected while Zero is in the needs-auth state. See Authentication for more information. Offline Support While in the connecting state, Zero queues writes and replays them when the connection succeeds. This works well for short periods offline, but not long periods for several reasons. Conflicts Imagine two users are editing an article about cats. One goes offline and does a bunch of work on the article, while the other decides that the article should actually be about dogs and rewrites it. When the offline user reconnects, there is no way that any software algorithm can automatically resolve their conflict. One or the other of them is going to be upset. While this example may sound extreme, you can construct similar situations with the majority of common applications. Just take your own application and ask yourself what should really happen if one user takes their device offline for a week and makes arbitrarily complex changes while other users are working online. If you want to build an application that supports offline writes, you have three options: Make the logical datamodel append-only (i.e., users can create and mark tasks done, but cannot edit or delete them). Support custom UX to allow users to fork and merge conflicts when they occur. Only support editing from a single device. Even a single-user application can experience conflicts as soon as there are multiple devices. Writes Can be Lost Across Schema Changes Typically when a write happens while offline, it is written to local storage and replayed on next connection ‚Äì even across app restarts. However, if that app restart gets a new version of Zero, or a new schema version, Zero creates a new local storage area and the offline write is effectively lost. In Replicache, our prior sync engine on which Zero is based, this was solved with a system called \"mutation recovery\" that rescued these mutations from prior schema/Zero versions. This system has not yet been ported to Zero. Startup Can be Slow When Replaying Writes When Zero starts, it sends all queued mutations to the server and simultaneously pulls updates from the server. When the server updates are applied, any queued mutations are \"rebased\" (re-run locally) to resolve conflicts. If there are many locally queued changes, this process can be slow. To effectively support long periods offline, Zero should probably have a mode where it first sends all queued mutations, then replays them after the server updates are applied. But this would conflict with the goal to startup and get server updates fast when there aren't many queued mutations, so some work would be needed to balance these two goals. Offline Mutations Must Never Be Rejected When a mutator throws an error on the server, the handleMutateRequest function in @rocicorp/zero/server returns the error to the client but marks the mutation processed. This is a good UX and DX when online because it mirrors the behavior of normal web apps. When offline, mutations should never be rejected. Otherwise the user could lose arbitrarily large amounts of work. Mutations should be replayed until they can be accepted. This would require a different implementation of the handleMutateRequest. Also the developer would have to be very careful to respond to any erroring mutations quickly, as those clients will be unable to save work until the error is resolved.",
    "headings": [
      {
        "text": "Overview",
        "id": "overview"
      },
      {
        "text": "Usage",
        "id": "usage"
      },
      {
        "text": "Details",
        "id": "details"
      },
      {
        "text": "Connecting",
        "id": "connecting"
      },
      {
        "text": "Disconnecting",
        "id": "disconnecting"
      },
      {
        "text": "Errors",
        "id": "errors"
      },
      {
        "text": "Auth Required",
        "id": "auth-required"
      },
      {
        "text": "Offline Support",
        "id": "offline-support"
      },
      {
        "text": "Conflicts",
        "id": "conflicts"
      },
      {
        "text": "Writes Can be Lost Across Schema Changes",
        "id": "writes-can-be-lost-across-schema-changes"
      },
      {
        "text": "Startup Can be Slow When Replaying Writes",
        "id": "startup-can-be-slow-when-replaying-writes"
      },
      {
        "text": "Offline Mutations Must Never Be Rejected",
        "id": "offline-mutations-must-never-be-rejected"
      }
    ],
    "kind": "page"
  },
  {
    "id": "95-connection#overview",
    "title": "Connection Status",
    "searchTitle": "Overview",
    "sectionTitle": "Overview",
    "sectionId": "overview",
    "url": "/docs/connection",
    "content": "Zero manages a persistent connection to zero-cache with the following lifecycle:",
    "kind": "section"
  },
  {
    "id": "96-connection#usage",
    "title": "Connection Status",
    "searchTitle": "Usage",
    "sectionTitle": "Usage",
    "sectionId": "usage",
    "url": "/docs/connection",
    "content": "The current connection state is available in the zero.connection.state property. This is subscribable and also has reactive hooks for React and SolidJS: import {useConnectionState} from '@rocicorp/zero/react' function ConnectionStatus() { const state = useConnectionState() switch (state.name) { case 'connecting': return <div title={state.reason}>Connecting...</div> case 'connected': return <div>Connected</div> case 'disconnected': return <div title={state.reason}>Offline</div> case 'error': return <div title={state.reason}>Error</div> case 'needs-auth': return <div>Session expired</div> default: return null } }import {useConnectionState} from '@rocicorp/zero/solid' function ConnectionStatus() { const state = useConnectionState() return ( <Switch> <Match when={state().name === 'connecting'}> <div title={state().reason}>Connecting...</div> </Match> <Match when={state().name === 'connected'}> <div>Connected</div> </Match> <Match when={state().name === 'disconnected'}> <div title={state().reason}>Offline</div> </Match> <Match when={state().name === 'error'}> <div title={state().reason}>Error</div> </Match> <Match when={state().name === 'needs-auth'}> <div>Session expired</div> </Match> </Switch> ) }zero.connection.state.subscribe(state => { switch (state.name) { case 'connecting': console.log(`Connecting... ${state.reason}`) break case 'connected': console.log('Connected') break case 'disconnected': console.log(`Disconnected ${state.reason}`) break case 'error': console.log(`Error ${state.reason}`) break case 'needs-auth': console.log('Session expired') break default: return null } })",
    "kind": "section"
  },
  {
    "id": "97-connection#details",
    "title": "Connection Status",
    "searchTitle": "Details",
    "sectionTitle": "Details",
    "sectionId": "details",
    "url": "/docs/connection",
    "content": "Connecting Zero starts in the connecting state. Once the connection is established, it transitions to connected. While connecting, Zero repeatedly tries to connect to zero-cache for 1 minute by default. This timeout can be configured with the disconnectTimeoutMs constructor parameter: const opts: ZeroOptions = { // ... disconnectTimeoutMs: 1000 * 60 * 10 // 10 minutes } Reads and writes are allowed to Zero mutators while connecting. The writes are queued and will be sent when the connection succeeds. This is intended to paper over short connectivity glitches, such as server restarts, walking into an elevator, etc. While you can increase the disconnectTimeoutMs to allow for longer periods of offline operation, this has caveats that you should understand carefully. Please see offline for more information. Disconnecting After the disconnectTimeoutMs elapses, Zero transitions to disconnected. Zero also transitions to disconnected when the tab is hidden for hiddenTabDisconnectDelay (default 5 minutes). While disconnected, Zero continues to try to reconnect to zero-cache every 5 seconds. Reads are allowed while disconnected, but writes are rejected and return an offline error. Errors If zero-cache itself crashes, or if the mutate or query endpoints return a network or HTTP error, Zero transitions to the error state. This type of error is unlikely to resolve just by retrying, so Zero doesn't try. The app can retry the connection manually by calling zero.connection.connect(). Writes are rejected while Zero is in the error state. You can forward connection errors to Sentry (or any error-monitoring tool) by subscribing to zero.connection.state. You can wrap reason in an Error and report it: import * as Sentry from '@sentry/browser' zero.connection.state.subscribe(state => { if (state.name !== 'error') return Sentry.withScope(scope => { scope.setTag('zero.connection.state', state.name) scope.setExtra('zero.connection.reason', state.reason) Sentry.captureException( new Error(`Zero connection error: ${state.reason}`) ) }) }) Auth Required If the mutate or query endpoints return a 401 or 403 status code, Zero transitions to the needs-auth state. The app should refresh the cookie or auth token and retry the connection manually by calling zero.connection.connect(). Writes are rejected while Zero is in the needs-auth state. See Authentication for more information.",
    "kind": "section"
  },
  {
    "id": "98-connection#connecting",
    "title": "Connection Status",
    "searchTitle": "Connecting",
    "sectionTitle": "Connecting",
    "sectionId": "connecting",
    "url": "/docs/connection",
    "content": "Zero starts in the connecting state. Once the connection is established, it transitions to connected. While connecting, Zero repeatedly tries to connect to zero-cache for 1 minute by default. This timeout can be configured with the disconnectTimeoutMs constructor parameter: const opts: ZeroOptions = { // ... disconnectTimeoutMs: 1000 * 60 * 10 // 10 minutes } Reads and writes are allowed to Zero mutators while connecting. The writes are queued and will be sent when the connection succeeds. This is intended to paper over short connectivity glitches, such as server restarts, walking into an elevator, etc. While you can increase the disconnectTimeoutMs to allow for longer periods of offline operation, this has caveats that you should understand carefully. Please see offline for more information.",
    "kind": "section"
  },
  {
    "id": "99-connection#disconnecting",
    "title": "Connection Status",
    "searchTitle": "Disconnecting",
    "sectionTitle": "Disconnecting",
    "sectionId": "disconnecting",
    "url": "/docs/connection",
    "content": "After the disconnectTimeoutMs elapses, Zero transitions to disconnected. Zero also transitions to disconnected when the tab is hidden for hiddenTabDisconnectDelay (default 5 minutes). While disconnected, Zero continues to try to reconnect to zero-cache every 5 seconds. Reads are allowed while disconnected, but writes are rejected and return an offline error.",
    "kind": "section"
  },
  {
    "id": "100-connection#errors",
    "title": "Connection Status",
    "searchTitle": "Errors",
    "sectionTitle": "Errors",
    "sectionId": "errors",
    "url": "/docs/connection",
    "content": "If zero-cache itself crashes, or if the mutate or query endpoints return a network or HTTP error, Zero transitions to the error state. This type of error is unlikely to resolve just by retrying, so Zero doesn't try. The app can retry the connection manually by calling zero.connection.connect(). Writes are rejected while Zero is in the error state. You can forward connection errors to Sentry (or any error-monitoring tool) by subscribing to zero.connection.state. You can wrap reason in an Error and report it: import * as Sentry from '@sentry/browser' zero.connection.state.subscribe(state => { if (state.name !== 'error') return Sentry.withScope(scope => { scope.setTag('zero.connection.state', state.name) scope.setExtra('zero.connection.reason', state.reason) Sentry.captureException( new Error(`Zero connection error: ${state.reason}`) ) }) })",
    "kind": "section"
  },
  {
    "id": "101-connection#auth-required",
    "title": "Connection Status",
    "searchTitle": "Auth Required",
    "sectionTitle": "Auth Required",
    "sectionId": "auth-required",
    "url": "/docs/connection",
    "content": "If the mutate or query endpoints return a 401 or 403 status code, Zero transitions to the needs-auth state. The app should refresh the cookie or auth token and retry the connection manually by calling zero.connection.connect(). Writes are rejected while Zero is in the needs-auth state. See Authentication for more information.",
    "kind": "section"
  },
  {
    "id": "102-connection#offline-support",
    "title": "Connection Status",
    "searchTitle": "Offline Support",
    "sectionTitle": "Offline Support",
    "sectionId": "offline-support",
    "url": "/docs/connection",
    "content": "While in the connecting state, Zero queues writes and replays them when the connection succeeds. This works well for short periods offline, but not long periods for several reasons. Conflicts Imagine two users are editing an article about cats. One goes offline and does a bunch of work on the article, while the other decides that the article should actually be about dogs and rewrites it. When the offline user reconnects, there is no way that any software algorithm can automatically resolve their conflict. One or the other of them is going to be upset. While this example may sound extreme, you can construct similar situations with the majority of common applications. Just take your own application and ask yourself what should really happen if one user takes their device offline for a week and makes arbitrarily complex changes while other users are working online. If you want to build an application that supports offline writes, you have three options: Make the logical datamodel append-only (i.e., users can create and mark tasks done, but cannot edit or delete them). Support custom UX to allow users to fork and merge conflicts when they occur. Only support editing from a single device. Even a single-user application can experience conflicts as soon as there are multiple devices. Writes Can be Lost Across Schema Changes Typically when a write happens while offline, it is written to local storage and replayed on next connection ‚Äì even across app restarts. However, if that app restart gets a new version of Zero, or a new schema version, Zero creates a new local storage area and the offline write is effectively lost. In Replicache, our prior sync engine on which Zero is based, this was solved with a system called \"mutation recovery\" that rescued these mutations from prior schema/Zero versions. This system has not yet been ported to Zero. Startup Can be Slow When Replaying Writes When Zero starts, it sends all queued mutations to the server and simultaneously pulls updates from the server. When the server updates are applied, any queued mutations are \"rebased\" (re-run locally) to resolve conflicts. If there are many locally queued changes, this process can be slow. To effectively support long periods offline, Zero should probably have a mode where it first sends all queued mutations, then replays them after the server updates are applied. But this would conflict with the goal to startup and get server updates fast when there aren't many queued mutations, so some work would be needed to balance these two goals. Offline Mutations Must Never Be Rejected When a mutator throws an error on the server, the handleMutateRequest function in @rocicorp/zero/server returns the error to the client but marks the mutation processed. This is a good UX and DX when online because it mirrors the behavior of normal web apps. When offline, mutations should never be rejected. Otherwise the user could lose arbitrarily large amounts of work. Mutations should be replayed until they can be accepted. This would require a different implementation of the handleMutateRequest. Also the developer would have to be very careful to respond to any erroring mutations quickly, as those clients will be unable to save work until the error is resolved.",
    "kind": "section"
  },
  {
    "id": "103-connection#conflicts",
    "title": "Connection Status",
    "searchTitle": "Conflicts",
    "sectionTitle": "Conflicts",
    "sectionId": "conflicts",
    "url": "/docs/connection",
    "content": "Imagine two users are editing an article about cats. One goes offline and does a bunch of work on the article, while the other decides that the article should actually be about dogs and rewrites it. When the offline user reconnects, there is no way that any software algorithm can automatically resolve their conflict. One or the other of them is going to be upset. While this example may sound extreme, you can construct similar situations with the majority of common applications. Just take your own application and ask yourself what should really happen if one user takes their device offline for a week and makes arbitrarily complex changes while other users are working online. If you want to build an application that supports offline writes, you have three options: Make the logical datamodel append-only (i.e., users can create and mark tasks done, but cannot edit or delete them). Support custom UX to allow users to fork and merge conflicts when they occur. Only support editing from a single device. Even a single-user application can experience conflicts as soon as there are multiple devices.",
    "kind": "section"
  },
  {
    "id": "104-connection#writes-can-be-lost-across-schema-changes",
    "title": "Connection Status",
    "searchTitle": "Writes Can be Lost Across Schema Changes",
    "sectionTitle": "Writes Can be Lost Across Schema Changes",
    "sectionId": "writes-can-be-lost-across-schema-changes",
    "url": "/docs/connection",
    "content": "Typically when a write happens while offline, it is written to local storage and replayed on next connection ‚Äì even across app restarts. However, if that app restart gets a new version of Zero, or a new schema version, Zero creates a new local storage area and the offline write is effectively lost. In Replicache, our prior sync engine on which Zero is based, this was solved with a system called \"mutation recovery\" that rescued these mutations from prior schema/Zero versions. This system has not yet been ported to Zero.",
    "kind": "section"
  },
  {
    "id": "105-connection#startup-can-be-slow-when-replaying-writes",
    "title": "Connection Status",
    "searchTitle": "Startup Can be Slow When Replaying Writes",
    "sectionTitle": "Startup Can be Slow When Replaying Writes",
    "sectionId": "startup-can-be-slow-when-replaying-writes",
    "url": "/docs/connection",
    "content": "When Zero starts, it sends all queued mutations to the server and simultaneously pulls updates from the server. When the server updates are applied, any queued mutations are \"rebased\" (re-run locally) to resolve conflicts. If there are many locally queued changes, this process can be slow. To effectively support long periods offline, Zero should probably have a mode where it first sends all queued mutations, then replays them after the server updates are applied. But this would conflict with the goal to startup and get server updates fast when there aren't many queued mutations, so some work would be needed to balance these two goals.",
    "kind": "section"
  },
  {
    "id": "106-connection#offline-mutations-must-never-be-rejected",
    "title": "Connection Status",
    "searchTitle": "Offline Mutations Must Never Be Rejected",
    "sectionTitle": "Offline Mutations Must Never Be Rejected",
    "sectionId": "offline-mutations-must-never-be-rejected",
    "url": "/docs/connection",
    "content": "When a mutator throws an error on the server, the handleMutateRequest function in @rocicorp/zero/server returns the error to the client but marks the mutation processed. This is a good UX and DX when online because it mirrors the behavior of normal web apps. When offline, mutations should never be rejected. Otherwise the user could lose arbitrarily large amounts of work. Mutations should be replayed until they can be accepted. This would require a different implementation of the handleMutateRequest. Also the developer would have to be very careful to respond to any erroring mutations quickly, as those clients will be unable to save work until the error is resolved.",
    "kind": "section"
  },
  {
    "id": "4-debug/inspector",
    "title": "Inspector",
    "searchTitle": "Inspector",
    "url": "/docs/debug/inspector",
    "content": "Zero includes a rich inspector API that can help you understand performance or behavior issues you are seeing in your apps. Accessing the Inspector You access the inspector right from the standard developer console in your browser: For convenience, Zero automatically injects itself as __zero on the global scope of every Zero app. Access to the inspector is gated behind the ZERO_ADMIN_PASSWORD config variable in production (when NODE_ENV is set to \"production\"). We require this variable to be set to a non-empty value in production because we want the inspector enabled in all Zero apps without requiring a restart. Clients and Groups Once you have an inspector, you can inspect the current client and client group. For example to see active queries for the current client: let qs = await inspector.client.queries() console.table(qs) To see active queries for the entire group: let qs = await inspector.client.queries() console.table(qs) In Zero, each instance of the Zero class is a client. Each client belongs to a group, which is a set of clients that share the same clientGroupID (typically all clients within a browser profile). Zero syncs all clients in a group together, so they all see the same data. So if you are debugging performance, you often want to look at the queries for the group, since that is what Zero is actually syncing. But if you are trying to understand when particular queries get added, it's convenient to look at the queries for just the current client so that queries from other clients aren't mixed in. Queries The inspector exposes a bunch of useful information about queries. For example, to see the first query for the current client: let qs = await inspector.client.queries() console.log(qs[0]) This outputs something like: Here are some of the more useful fields: Analyzing Queries Use the analyze method to get information about how a query hydrates: await qs[0].analyze() Here are some of the most useful fields in the output: Analyzing Arbitrary ZQL You can also analyze arbitrary ZQL, not just queries that are currently active: await __zero.inspector.analyzeQuery( __builder.issues.whereExists('labels', q => q.id.equals('sync') ) ) This is useful for exploring alternative query constructions to optimize performance. To use this, you will first have to expose your builder as a property of the global object, so that you can access it from the console. For example: // schema.ts // ... const g = globalThis as any g.__builder = builder Analyzing Query Plans A Zero query is composed of one or more single-table queries connected by joins (related, whereExists). Zero delegates the single-table queries to SQLite, which has a sophisticated query planner that chooses the best indexes to use. For the joins, Zero implements its own cost-based planner to choose the best join order and algorithm. To view the plans selected by SQLite, see the sqlitePlans field returned by analyze() or analyzeQuery(). This contains the output of SQLite's EXPLAIN QUERY PLAN command for each SQLite query used: To view the join plan selected by Zero, call analyze() or analyzeQuery() with the joinPlans option set to true and see the joinPlans field in the output: Table Data In addition to information about queries, you can get direct access to the contents of the client side database. const client = __zero.inspector.client // All raw k/v data currently synced to client console.log('client map:') console.log(await client.map()) // kv table extracted into tables // This is same info that is in z.query[tableName].run() for (const tableName of Object.keys(__zero.schema.tables)) { console.log(`table ${tableName}:`) console.table(await client.rows(tableName)) } Server Version Ask the server to confirm what version it is: console.log( 'server version: ', await inspector.serverVersion() )",
    "headings": [
      {
        "text": "Accessing the Inspector",
        "id": "accessing-the-inspector"
      },
      {
        "text": "Clients and Groups",
        "id": "clients-and-groups"
      },
      {
        "text": "Queries",
        "id": "queries"
      },
      {
        "text": "Analyzing Queries",
        "id": "analyzing-queries"
      },
      {
        "text": "Analyzing Arbitrary ZQL",
        "id": "analyzing-arbitrary-zql"
      },
      {
        "text": "Analyzing Query Plans",
        "id": "analyzing-query-plans"
      },
      {
        "text": "Table Data",
        "id": "table-data"
      },
      {
        "text": "Server Version",
        "id": "server-version"
      }
    ],
    "kind": "page"
  },
  {
    "id": "107-debug/inspector#accessing-the-inspector",
    "title": "Inspector",
    "searchTitle": "Accessing the Inspector",
    "sectionTitle": "Accessing the Inspector",
    "sectionId": "accessing-the-inspector",
    "url": "/docs/debug/inspector",
    "content": "You access the inspector right from the standard developer console in your browser: For convenience, Zero automatically injects itself as __zero on the global scope of every Zero app. Access to the inspector is gated behind the ZERO_ADMIN_PASSWORD config variable in production (when NODE_ENV is set to \"production\"). We require this variable to be set to a non-empty value in production because we want the inspector enabled in all Zero apps without requiring a restart.",
    "kind": "section"
  },
  {
    "id": "108-debug/inspector#clients-and-groups",
    "title": "Inspector",
    "searchTitle": "Clients and Groups",
    "sectionTitle": "Clients and Groups",
    "sectionId": "clients-and-groups",
    "url": "/docs/debug/inspector",
    "content": "Once you have an inspector, you can inspect the current client and client group. For example to see active queries for the current client: let qs = await inspector.client.queries() console.table(qs) To see active queries for the entire group: let qs = await inspector.client.queries() console.table(qs) In Zero, each instance of the Zero class is a client. Each client belongs to a group, which is a set of clients that share the same clientGroupID (typically all clients within a browser profile). Zero syncs all clients in a group together, so they all see the same data. So if you are debugging performance, you often want to look at the queries for the group, since that is what Zero is actually syncing. But if you are trying to understand when particular queries get added, it's convenient to look at the queries for just the current client so that queries from other clients aren't mixed in.",
    "kind": "section"
  },
  {
    "id": "109-debug/inspector#queries",
    "title": "Inspector",
    "searchTitle": "Queries",
    "sectionTitle": "Queries",
    "sectionId": "queries",
    "url": "/docs/debug/inspector",
    "content": "The inspector exposes a bunch of useful information about queries. For example, to see the first query for the current client: let qs = await inspector.client.queries() console.log(qs[0]) This outputs something like: Here are some of the more useful fields:",
    "kind": "section"
  },
  {
    "id": "110-debug/inspector#analyzing-queries",
    "title": "Inspector",
    "searchTitle": "Analyzing Queries",
    "sectionTitle": "Analyzing Queries",
    "sectionId": "analyzing-queries",
    "url": "/docs/debug/inspector",
    "content": "Use the analyze method to get information about how a query hydrates: await qs[0].analyze() Here are some of the most useful fields in the output:",
    "kind": "section"
  },
  {
    "id": "111-debug/inspector#analyzing-arbitrary-zql",
    "title": "Inspector",
    "searchTitle": "Analyzing Arbitrary ZQL",
    "sectionTitle": "Analyzing Arbitrary ZQL",
    "sectionId": "analyzing-arbitrary-zql",
    "url": "/docs/debug/inspector",
    "content": "You can also analyze arbitrary ZQL, not just queries that are currently active: await __zero.inspector.analyzeQuery( __builder.issues.whereExists('labels', q => q.id.equals('sync') ) ) This is useful for exploring alternative query constructions to optimize performance. To use this, you will first have to expose your builder as a property of the global object, so that you can access it from the console. For example: // schema.ts // ... const g = globalThis as any g.__builder = builder",
    "kind": "section"
  },
  {
    "id": "112-debug/inspector#analyzing-query-plans",
    "title": "Inspector",
    "searchTitle": "Analyzing Query Plans",
    "sectionTitle": "Analyzing Query Plans",
    "sectionId": "analyzing-query-plans",
    "url": "/docs/debug/inspector",
    "content": "A Zero query is composed of one or more single-table queries connected by joins (related, whereExists). Zero delegates the single-table queries to SQLite, which has a sophisticated query planner that chooses the best indexes to use. For the joins, Zero implements its own cost-based planner to choose the best join order and algorithm. To view the plans selected by SQLite, see the sqlitePlans field returned by analyze() or analyzeQuery(). This contains the output of SQLite's EXPLAIN QUERY PLAN command for each SQLite query used: To view the join plan selected by Zero, call analyze() or analyzeQuery() with the joinPlans option set to true and see the joinPlans field in the output:",
    "kind": "section"
  },
  {
    "id": "113-debug/inspector#table-data",
    "title": "Inspector",
    "searchTitle": "Table Data",
    "sectionTitle": "Table Data",
    "sectionId": "table-data",
    "url": "/docs/debug/inspector",
    "content": "In addition to information about queries, you can get direct access to the contents of the client side database. const client = __zero.inspector.client // All raw k/v data currently synced to client console.log('client map:') console.log(await client.map()) // kv table extracted into tables // This is same info that is in z.query[tableName].run() for (const tableName of Object.keys(__zero.schema.tables)) { console.log(`table ${tableName}:`) console.table(await client.rows(tableName)) }",
    "kind": "section"
  },
  {
    "id": "114-debug/inspector#server-version",
    "title": "Inspector",
    "searchTitle": "Server Version",
    "sectionTitle": "Server Version",
    "sectionId": "server-version",
    "url": "/docs/debug/inspector",
    "content": "Ask the server to confirm what version it is: console.log( 'server version: ', await inspector.serverVersion() )",
    "kind": "section"
  },
  {
    "id": "5-debug/otel",
    "title": "OpenTelemetry",
    "searchTitle": "OpenTelemetry",
    "url": "/docs/debug/otel",
    "content": "The zero-cache service embeds the JavaScript OTLP Exporter and can send logs, traces, and metrics to any standard otel collector. To enable otel, set the following environment variables then run zero-cache as normal: OTEL_EXPORTER_OTLP_ENDPOINT=\"<your otel endpoint>\" OTEL_EXPORTER_OTLP_HEADERS=\"<auth headers from your otel collector>\" OTEL_RESOURCE_ATTRIBUTES=\"<resource attributes from your otel collector>\" OTEL_NODE_RESOURCE_DETECTORS=\"env,host,os\" Grafana Cloud Walkthrough Here are instructions to setup Grafana Cloud, but the setup for other otel collectors should be similar. Sign up for Grafana Cloud (Free Tier) Click Connections > Add Connection in the left sidebar add-connection Search for \"OpenTelemetry\" and select it Click \"Quickstart\" quickstart Select \"JavaScript\" javascript Create a new token Copy the environment variables into your .env file or similar copy-env Start zero-cache Look for logs under \"Drilldown\" > \"Logs\" in left sidebar",
    "headings": [
      {
        "text": "Grafana Cloud Walkthrough",
        "id": "grafana-cloud-walkthrough"
      }
    ],
    "kind": "page"
  },
  {
    "id": "115-debug/otel#grafana-cloud-walkthrough",
    "title": "OpenTelemetry",
    "searchTitle": "Grafana Cloud Walkthrough",
    "sectionTitle": "Grafana Cloud Walkthrough",
    "sectionId": "grafana-cloud-walkthrough",
    "url": "/docs/debug/otel",
    "content": "Here are instructions to setup Grafana Cloud, but the setup for other otel collectors should be similar. Sign up for Grafana Cloud (Free Tier) Click Connections > Add Connection in the left sidebar add-connection Search for \"OpenTelemetry\" and select it Click \"Quickstart\" quickstart Select \"JavaScript\" javascript Create a new token Copy the environment variables into your .env file or similar copy-env Start zero-cache Look for logs under \"Drilldown\" > \"Logs\" in left sidebar",
    "kind": "section"
  },
  {
    "id": "6-debug/query-asts",
    "title": "Query ASTs",
    "searchTitle": "Query ASTs",
    "url": "/docs/debug/query-asts",
    "content": "An AST (Abstract Syntax Tree) is a representation of a query that is used internally by Zero. It is not meant to be human readable, but it sometimes shows up in logs and other places. If you need to read one of these, save the AST to a json file. Then run the following command: cat ast.json | npx ast-to-zql The returned ZQL query will be using server names, rather than client names, to identify columns and tables. If you provide the schema file as an option you will get mapped back to client names: cat ast.json | npx ast-to-zql --schema schema.ts This comes into play if, in your schema.ts, you use the from feature to have different names on the client than your backend DB. The ast-to-zql process is a de-compilation of sorts. Given that, the ZQL string you get back will not be identical to the one you wrote in your application. Regardless, the queries will be semantically equivalent.",
    "headings": [],
    "kind": "page"
  },
  {
    "id": "7-debug/replication",
    "title": "Replication",
    "searchTitle": "Replication",
    "url": "/docs/debug/replication",
    "content": "Resetting During development we all do strange things (unsafely changing schemas, removing files, etc.). If the replica ever gets wedged (stops replicating, acts strange) you can wipe it and start over. If you copied your setup from hello-zero or hello-zero-solid, you can also run npm run dev:clean Otherwise you can run rm /tmp/my-zero-replica.db* (see your .env file for the replica file location) to clear the contents of the replica. It is always safe to wipe the replica. Wiping will have no impact on your upstream database. Downstream zero-clients will get re-synced when they connect. Inspecting For data to be synced to the client it must first be replicated to zero-cache. You can check the contents of zero-cache via: $ npx @rocicorp/zero-sqlite3 /tmp/my-zero-replica.db Zero uses a different version of SQLite that runs in WAL2 mode, which means the database files cannot be opened with standard SQLite tools. To inspect your Zero database, you have two options: Use our pre-compiled SQLite build @rocicorp/zero-sqlite3 as described above Build SQLite from the SQLite bedrock branch yourself This will drop you into a sqlite3 shell with which you can use to explore the contents of the replica. sqlite> .tables _zero.changeLog emoji viewState _zero.replicationConfig issue zero.permissions _zero.replicationState issueLabel zero.schemaVersions _zero.runtimeEvents label zero_0.clients _zero.versionHistory user comment userPref sqlite> .mode qbox sqlite> SELECT * FROM label; ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ id ‚îÇ name ‚îÇ _0_version ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ 'ic_g-DZTYDApZR_v7Cdcy' ‚îÇ 'bug' ‚îÇ '4ehreg' ‚îÇ ... Miscellaneous If you see FATAL: sorry, too many clients already in logs, it‚Äôs because you have two zero-cache instances running against dev. One is probably in a background tab somewhere. In production, zero-cache can run horizontally scaled but on dev it doesn‚Äôt run in the config that allows that.",
    "headings": [
      {
        "text": "Resetting",
        "id": "resetting"
      },
      {
        "text": "Inspecting",
        "id": "inspecting"
      },
      {
        "text": "Miscellaneous",
        "id": "miscellaneous"
      }
    ],
    "kind": "page"
  },
  {
    "id": "116-debug/replication#resetting",
    "title": "Replication",
    "searchTitle": "Resetting",
    "sectionTitle": "Resetting",
    "sectionId": "resetting",
    "url": "/docs/debug/replication",
    "content": "During development we all do strange things (unsafely changing schemas, removing files, etc.). If the replica ever gets wedged (stops replicating, acts strange) you can wipe it and start over. If you copied your setup from hello-zero or hello-zero-solid, you can also run npm run dev:clean Otherwise you can run rm /tmp/my-zero-replica.db* (see your .env file for the replica file location) to clear the contents of the replica. It is always safe to wipe the replica. Wiping will have no impact on your upstream database. Downstream zero-clients will get re-synced when they connect.",
    "kind": "section"
  },
  {
    "id": "117-debug/replication#inspecting",
    "title": "Replication",
    "searchTitle": "Inspecting",
    "sectionTitle": "Inspecting",
    "sectionId": "inspecting",
    "url": "/docs/debug/replication",
    "content": "For data to be synced to the client it must first be replicated to zero-cache. You can check the contents of zero-cache via: $ npx @rocicorp/zero-sqlite3 /tmp/my-zero-replica.db Zero uses a different version of SQLite that runs in WAL2 mode, which means the database files cannot be opened with standard SQLite tools. To inspect your Zero database, you have two options: Use our pre-compiled SQLite build @rocicorp/zero-sqlite3 as described above Build SQLite from the SQLite bedrock branch yourself This will drop you into a sqlite3 shell with which you can use to explore the contents of the replica. sqlite> .tables _zero.changeLog emoji viewState _zero.replicationConfig issue zero.permissions _zero.replicationState issueLabel zero.schemaVersions _zero.runtimeEvents label zero_0.clients _zero.versionHistory user comment userPref sqlite> .mode qbox sqlite> SELECT * FROM label; ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ id ‚îÇ name ‚îÇ _0_version ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ 'ic_g-DZTYDApZR_v7Cdcy' ‚îÇ 'bug' ‚îÇ '4ehreg' ‚îÇ ...",
    "kind": "section"
  },
  {
    "id": "118-debug/replication#miscellaneous",
    "title": "Replication",
    "searchTitle": "Miscellaneous",
    "sectionTitle": "Miscellaneous",
    "sectionId": "miscellaneous",
    "url": "/docs/debug/replication",
    "content": "If you see FATAL: sorry, too many clients already in logs, it‚Äôs because you have two zero-cache instances running against dev. One is probably in a background tab somewhere. In production, zero-cache can run horizontally scaled but on dev it doesn‚Äôt run in the config that allows that.",
    "kind": "section"
  },
  {
    "id": "8-debug/slow-queries",
    "title": "Slow Queries",
    "searchTitle": "Slow Queries",
    "url": "/docs/debug/slow-queries",
    "content": "In the zero-cache logs, you may see statements indicating a query is slow: hash=3rhuw19xt9vry transformationHash=1nv7ot74gxfl7 Slow query materialization 325.46865100000286 Or, you may just notice queries taking longer than expected in the UI. Here are some tips to help debug such slow queries. Query Plan The @rocicorp/zero package ships with a CLI to help debug query plans. You can run it with: # see all parameters npx analyze-query --help # analyze a specific query npx analyze-query \\ --schema-path=\"./schema.ts\" \\ --replica-file=\"./zero.db\" \\ --query='albums.where(\"artistId\", \"artist_1\").orderBy(\"createdAt\", \"asc\").limit(10)' This command will output the query plan and time to execute each phase of that plan: $ npx analyze-query \\ --schema-path=\"./schema.ts\" \\ --replica-file=\"./zero.db\" \\ --query='albums.where(\"artistId\", \"artist_1\").orderBy(\"createdAt\", \"asc\").limit(10)' Loading schema from ./schema.ts === Query Stats: === total synced rows: 10 albums vended: { 'SELECT \"id\",\"title\",\"artist_id\",\"release_year\",\"cover_art_url\",\"created_at\",\"_0_version\" FROM \"albums\" WHERE \"artist_id\" = ? ORDER BY \"created_at\" asc, \"id\" asc': 10 } Rows Read (into JS): 10 time: 3.12ms ms === Rows Scanned (by SQLite): === albums: { 'SELECT \"id\",\"title\",\"artist_id\",\"release_year\",\"cover_art_url\",\"created_at\",\"_0_version\" FROM \"albums\" WHERE \"artist_id\" = ? ORDER BY \"created_at\" asc, \"id\" asc': 25 } total rows scanned: 25 === Query Plans: === query SELECT \"id\",\"title\",\"artist_id\",\"release_year\",\"cover_art_url\",\"created_at\",\"_0_version\" FROM \"albums\" WHERE \"artist_id\" = ? ORDER BY \"created_at\" asc, \"id\" asc SCAN albums USE TEMP B-TREE FOR ORDER BY Ideally, run this command on the server where your zero.db replica file is located, so it uses the same disk as zero-cache. Adjust the --schema-path to point to your schema file (you may need to copy this onto the server). The --query arg is the ZQL query you want to analyze. Running locally, the analyzer will use any local .env file to find your environment configuration (so you don't need to manually provide the replica file). Optimizing the Plan You should look for any TEMP B-TREE entries in the query plan. These indicate that the query is not properly indexed in SQLite, and that zero-cache had to create a temporary index to satisfy the query. You should add appropriate indexes upstream to fix this. ZQL adds all primary key columns to the orderBy clause for a predictable total order, but only appends those PK columns which are not already present in the order of the query. This means that upstream indexes must also include the PK columns. Feel free to share your query plans with us in Discord if you need help optimizing them. Check ttl If you are seeing unexpected UI flicker when moving between views, it is possible that the queries backing these views have a ttl of never. Set the ttl to something like 5m to keep data cached across navigations. You may alternately want to preload some data at app startup. Conversely, if you are setting ttl to long values, then you may have many backgrounded queries running that the app is not using. You can see which queries are running using the inspector. Ensure that only expected queries are running. Locality If you see log lines like: flushed cvr ... (124ms) this indicates that zero-cache is likely deployed too far away from your CVR database. If you did not configure a CVR database URL then this will be your product's Postgres DB. A slow CVR flush can slow down Zero, since it must complete the flush before sending query result(s) to clients. Try moving zero-cache to be deployed as close as possible to the CVR database. Check Storage zero-cache is effectively a database. It requires fast (low latency and high bandwidth) disk access to perform well. If you're running on network attached storage with high latency, or on AWS with low IOPS, then this is the most likely culprit. Some hosting providers scale IOPS with vCPU. Increasing the vCPU will increase storage throughput and likely resolve the issue. Fly.io provides physically attached SSDs, even for their smallest VMs. Deploying zero-cache there (or any other provider that offers physically attached SSDs) is another option. /statz zero-cache makes some internal health statistics available via the /statz endpoint of zero-cache. In order to access this, you must configure an admin password.",
    "headings": [
      {
        "text": "Query Plan",
        "id": "query-plan"
      },
      {
        "text": "Optimizing the Plan",
        "id": "optimizing-the-plan"
      },
      {
        "text": "Check ttl",
        "id": "check-ttl"
      },
      {
        "text": "Locality",
        "id": "locality"
      },
      {
        "text": "Check Storage",
        "id": "check-storage"
      },
      {
        "text": "/statz",
        "id": "statz"
      }
    ],
    "kind": "page"
  },
  {
    "id": "119-debug/slow-queries#query-plan",
    "title": "Slow Queries",
    "searchTitle": "Query Plan",
    "sectionTitle": "Query Plan",
    "sectionId": "query-plan",
    "url": "/docs/debug/slow-queries",
    "content": "The @rocicorp/zero package ships with a CLI to help debug query plans. You can run it with: # see all parameters npx analyze-query --help # analyze a specific query npx analyze-query \\ --schema-path=\"./schema.ts\" \\ --replica-file=\"./zero.db\" \\ --query='albums.where(\"artistId\", \"artist_1\").orderBy(\"createdAt\", \"asc\").limit(10)' This command will output the query plan and time to execute each phase of that plan: $ npx analyze-query \\ --schema-path=\"./schema.ts\" \\ --replica-file=\"./zero.db\" \\ --query='albums.where(\"artistId\", \"artist_1\").orderBy(\"createdAt\", \"asc\").limit(10)' Loading schema from ./schema.ts === Query Stats: === total synced rows: 10 albums vended: { 'SELECT \"id\",\"title\",\"artist_id\",\"release_year\",\"cover_art_url\",\"created_at\",\"_0_version\" FROM \"albums\" WHERE \"artist_id\" = ? ORDER BY \"created_at\" asc, \"id\" asc': 10 } Rows Read (into JS): 10 time: 3.12ms ms === Rows Scanned (by SQLite): === albums: { 'SELECT \"id\",\"title\",\"artist_id\",\"release_year\",\"cover_art_url\",\"created_at\",\"_0_version\" FROM \"albums\" WHERE \"artist_id\" = ? ORDER BY \"created_at\" asc, \"id\" asc': 25 } total rows scanned: 25 === Query Plans: === query SELECT \"id\",\"title\",\"artist_id\",\"release_year\",\"cover_art_url\",\"created_at\",\"_0_version\" FROM \"albums\" WHERE \"artist_id\" = ? ORDER BY \"created_at\" asc, \"id\" asc SCAN albums USE TEMP B-TREE FOR ORDER BY Ideally, run this command on the server where your zero.db replica file is located, so it uses the same disk as zero-cache. Adjust the --schema-path to point to your schema file (you may need to copy this onto the server). The --query arg is the ZQL query you want to analyze. Running locally, the analyzer will use any local .env file to find your environment configuration (so you don't need to manually provide the replica file). Optimizing the Plan You should look for any TEMP B-TREE entries in the query plan. These indicate that the query is not properly indexed in SQLite, and that zero-cache had to create a temporary index to satisfy the query. You should add appropriate indexes upstream to fix this. ZQL adds all primary key columns to the orderBy clause for a predictable total order, but only appends those PK columns which are not already present in the order of the query. This means that upstream indexes must also include the PK columns. Feel free to share your query plans with us in Discord if you need help optimizing them.",
    "kind": "section"
  },
  {
    "id": "120-debug/slow-queries#optimizing-the-plan",
    "title": "Slow Queries",
    "searchTitle": "Optimizing the Plan",
    "sectionTitle": "Optimizing the Plan",
    "sectionId": "optimizing-the-plan",
    "url": "/docs/debug/slow-queries",
    "content": "You should look for any TEMP B-TREE entries in the query plan. These indicate that the query is not properly indexed in SQLite, and that zero-cache had to create a temporary index to satisfy the query. You should add appropriate indexes upstream to fix this. ZQL adds all primary key columns to the orderBy clause for a predictable total order, but only appends those PK columns which are not already present in the order of the query. This means that upstream indexes must also include the PK columns. Feel free to share your query plans with us in Discord if you need help optimizing them.",
    "kind": "section"
  },
  {
    "id": "121-debug/slow-queries#check-ttl",
    "title": "Slow Queries",
    "searchTitle": "Check ttl",
    "sectionTitle": "Check ttl",
    "sectionId": "check-ttl",
    "url": "/docs/debug/slow-queries",
    "content": "If you are seeing unexpected UI flicker when moving between views, it is possible that the queries backing these views have a ttl of never. Set the ttl to something like 5m to keep data cached across navigations. You may alternately want to preload some data at app startup. Conversely, if you are setting ttl to long values, then you may have many backgrounded queries running that the app is not using. You can see which queries are running using the inspector. Ensure that only expected queries are running.",
    "kind": "section"
  },
  {
    "id": "122-debug/slow-queries#locality",
    "title": "Slow Queries",
    "searchTitle": "Locality",
    "sectionTitle": "Locality",
    "sectionId": "locality",
    "url": "/docs/debug/slow-queries",
    "content": "If you see log lines like: flushed cvr ... (124ms) this indicates that zero-cache is likely deployed too far away from your CVR database. If you did not configure a CVR database URL then this will be your product's Postgres DB. A slow CVR flush can slow down Zero, since it must complete the flush before sending query result(s) to clients. Try moving zero-cache to be deployed as close as possible to the CVR database.",
    "kind": "section"
  },
  {
    "id": "123-debug/slow-queries#check-storage",
    "title": "Slow Queries",
    "searchTitle": "Check Storage",
    "sectionTitle": "Check Storage",
    "sectionId": "check-storage",
    "url": "/docs/debug/slow-queries",
    "content": "zero-cache is effectively a database. It requires fast (low latency and high bandwidth) disk access to perform well. If you're running on network attached storage with high latency, or on AWS with low IOPS, then this is the most likely culprit. Some hosting providers scale IOPS with vCPU. Increasing the vCPU will increase storage throughput and likely resolve the issue. Fly.io provides physically attached SSDs, even for their smallest VMs. Deploying zero-cache there (or any other provider that offers physically attached SSDs) is another option.",
    "kind": "section"
  },
  {
    "id": "124-debug/slow-queries#statz",
    "title": "Slow Queries",
    "searchTitle": "/statz",
    "sectionTitle": "/statz",
    "sectionId": "statz",
    "url": "/docs/debug/slow-queries",
    "content": "zero-cache makes some internal health statistics available via the /statz endpoint of zero-cache. In order to access this, you must configure an admin password.",
    "kind": "section"
  },
  {
    "id": "9-debug/zero-out",
    "title": "zero-out",
    "searchTitle": "zero-out",
    "url": "/docs/debug/zero-out",
    "content": "Run the zero-out tool to completely remove all traces of Zero from your Postgres database. This is useful for debugging issues with Zero and/or resetting to a clean state. npx zero-out zero-out reads the same config as zero-cache does, so you can just run it where you run zero-cache.",
    "headings": [],
    "kind": "page"
  },
  {
    "id": "10-deployment",
    "title": "Deploying Zero",
    "searchTitle": "Deploying Zero",
    "url": "/docs/deployment",
    "content": "So you've built your app with Zero - congratulations! Now you need to run it on a server somewhere. You will need to deploy zero-cache, a Postgres database, your frontend, and your API server. Zero-cache is made up of two main components: One or more view-syncers: serving client queries using a SQLite replica. One replication-manager: bridge between the Postgres replication stream and view-syncers. These components have the following characteristics: You will also need to deploy a Postgres database, your frontend, and your API server for the query and mutate endpoints. Minimum Viable Strategy The simplest way to deploy Zero is to run everything on a single node. This is the least expensive way to run Zero, and it can take you surprisingly far. Here is an example docker-compose.yml file for a single-node deployment (try it out!): services: upstream-db: image: postgres:18 environment: POSTGRES_DB: zero POSTGRES_PASSWORD: pass ports: - 5432:5432 command: postgres -c wal_level=logical healthcheck: test: pg_isready interval: 10s your-api: build: ./your-api ports: - 3000:3000 environment: # Your API handles mutations and writes to the PG db # This should be a pooled connection (e.g. pgbouncer) ZERO_UPSTREAM_DB: postgres://postgres:pass@upstream-db:5432/zero depends_on: upstream-db: condition: service_healthy zero-cache: image: rocicorp/zero:{version} ports: - 4848:4848 environment: # Used for replication from postgres # This must be a direct connection (not via pgbouncer) ZERO_UPSTREAM_DB: postgres://postgres:pass@upstream-db:5432/zero # Path to the SQLite replica ZERO_REPLICA_FILE: /data/zero.db # Password used to access the inspector and /statz ZERO_ADMIN_PASSWORD: pickanewpassword # URLs for your API's query and mutate endpoints ZERO_QUERY_URL: http://your-api:3000/api/zero/query ZERO_MUTATE_URL: http://your-api:3000/api/zero/mutate volumes: # Disk for the SQLite replica should be high IOPS - zero-cache-data:/data depends_on: your-api: condition: service_started healthcheck: test: curl -f http://localhost:4848/keepalive interval: 5s Maximal Strategy Once you reach the limits of the single-node deployment, you can split zero-cache into a multi-node topology. This is more expensive to run, but it gives you more flexibility and scalability. Here is an example docker-compose.yml file for a multi-node deployment (try it out!): services: upstream-db: image: postgres:18 environment: POSTGRES_DB: zero POSTGRES_PASSWORD: pass ports: - 5432:5432 command: postgres -c wal_level=logical healthcheck: test: pg_isready interval: 10s your-api: build: ./your-api ports: - 3000:3000 environment: # Your API handles mutations and writes to the PG db # This should be a pooled connection (e.g. pgbouncer) ZERO_UPSTREAM_DB: postgres://postgres:pass@upstream-db:5432/zero depends_on: upstream-db: condition: service_healthy # \"Mini S3\" (MinIO) provides a working s3://... `ZERO_LITESTREAM_BACKUP_URL` # This should be an S3-compatible object storage service in production. mini-s3: image: minio/minio:latest command: server /data --console-address \":9001\" healthcheck: test: curl -f http://localhost:9000/minio/health/live interval: 5s # Creates the bucket used by `ZERO_LITESTREAM_BACKUP_URL` # This is only needed for local development. mini-s3-create-bucket: image: minio/mc:latest depends_on: mini-s3: condition: service_healthy entrypoint: - /bin/sh - -lc - mc alias set local http://mini-s3:9000 \"minioadmin\" \"minioadmin\" && mc mb -p local/zero-backups || true replication-manager: image: rocicorp/zero:{version} ports: - 4849:4849 depends_on: upstream-db: condition: service_healthy your-api: condition: service_started mini-s3-create-bucket: condition: service_started environment: # Used for replication from postgres # this must be a direct connection (not via pgbouncer) ZERO_UPSTREAM_DB: postgres://postgres:pass@upstream-db:5432/zero # Used for storing client view records ZERO_CVR_DB: postgres://postgres:pass@upstream-db:5432/zero # Used for storing recent replication log entries ZERO_CHANGE_DB: postgres://postgres:pass@upstream-db:5432/zero # Path to the SQLite replica ZERO_REPLICA_FILE: /data/replica.db # Password used to access the inspector and /statz ZERO_ADMIN_PASSWORD: pickanewpassword # Dedicated replication-manager; disable view syncing. ZERO_NUM_SYNC_WORKERS: 0 # URL for backing up the SQLite replica # (include a simple version number for future cleanup) # Required in multi-node so view-syncers can reserve snapshots. ZERO_LITESTREAM_BACKUP_URL: s3://zero-backups/replica-v1 # S3 creds + Mini S3 endpoint (replication-manager backs up to S3) AWS_ACCESS_KEY_ID: minioadmin AWS_SECRET_ACCESS_KEY: minioadmin ZERO_LITESTREAM_ENDPOINT: http://mini-s3:9000 volumes: # storage for the SQLite replica should be high IOPS - replication-manager-data:/data healthcheck: test: curl -f http://localhost:4849/keepalive interval: 5s # Only one view-syncer in this example, but there can be N. view-syncer: image: rocicorp/zero:{version} ports: - 4848:4848 depends_on: replication-manager: condition: service_healthy environment: # Used for writing to the upstream database ZERO_UPSTREAM_DB: postgres://postgres:pass@upstream-db:5432/zero # Used for storing client view records ZERO_CVR_DB: postgres://postgres:pass@upstream-db:5432/zero # Used for storing recent replication log entries ZERO_CHANGE_DB: postgres://postgres:pass@upstream-db:5432/zero # Path to the SQLite replica ZERO_REPLICA_FILE: /data/replica.db # Password used to access the inspector and /statz ZERO_ADMIN_PASSWORD: pickanewpassword # URLs for your API's query and mutate endpoints ZERO_QUERY_URL: http://your-api:3000/api/zero/query ZERO_MUTATE_URL: http://your-api:3000/api/zero/mutate # URL for connecting to the replication-manager ZERO_CHANGE_STREAMER_URI: http://replication-manager:4849 # S3 creds + Mini S3 endpoint (view-syncers restore from S3 on startup) AWS_ACCESS_KEY_ID: minioadmin AWS_SECRET_ACCESS_KEY: minioadmin ZERO_LITESTREAM_ENDPOINT: http://mini-s3:9000 volumes: # Storage for the SQLite replica should be high IOPS - view-syncer-data:/data healthcheck: test: curl -f http://localhost:4848/keepalive interval: 5s The view-syncers in the multi-node topology can be horizontally scaled as needed. You can also override the number of sync workers per view-syncer with ZERO_NUM_SYNC_WORKERS. Replica Lifecycle Zero-cache is backed by a SQLite replica of your database. The SQLite replica uses upstream Postgres as the source of truth. If the replica is missing or a litestream restore fails, the replication-manager will resync the replica from upstream on the next start. Performance You want to optimize disk IOPS for the serving replica, since this is the file that is read by the view-syncers to run IVM-based queries, and one of the main bottlenecks for query hydration performance. View syncer's IVM is \"hydrate once, then incrementally push diffs\" against the ZQL pipeline, so performance is mostly about: How fast the server can materialize a subscription the first time (hydration). How fast it can keep it up to date (IVM advancement). Different bottlenecks dominate each phase. Hydration SQLite read cost: hydration is essentially \"run the query against the replica and stream all matching rows into the pipeline\", so it's bounded by SQLite scan/index performance + result size. Churn / TTL eviction: if queries get evicted (inactive long enough) and then get re-requested, you pay hydration again. Custom query transform latency: the HTTP request from zero-cache to your API at ZERO_QUERY_URL does transform/authorization for queries, adding network + CPU before hydration starts. IVM advancement Replication throughput: the view-syncer can only advance when the replicator commits and emits version-ready. If upstream replication is behind, query advancement is capped by how fast the replica advances. Change volume per transaction: advancement cost scales with number of changed rows, not number of queries. Circuit breaker behavior: if advancement looks like it'll take longer than rehydrating, zero-cache intentionally aborts and resets pipelines (which trades \"slow incremental\" for \"rehydrate\"). System-level Number of client groups per sync worker: each client group has its own pipelines; CPU and memory per group limits how many can be \"fast\" at once. Since Node is single-threaded, one client group can technically starve other groups. This is handled with time slicing and can be configured with the yield parameters, e.g. ZERO_YIELD_THRESHOLD_MS. SQLite concurrency limits: it's designed here for one writer (replicator) + many concurrent readers (view-syncer snapshots). It scales, but very heavy read workloads can still contend on cache/IO. Network to clients: even if IVM is fast, it can take time to send data over websocket. This can be improved by using CDNs (like CloudFront) that improve routing. Network between services: for a single-region deployment, all services should be colocated. Load Balancing View syncers must be publicly reachable by port 4848 by clients, and the replication-manager can have internal networking with the view-syncers on port 4849. The external load balancer must support websockets, and can use the health check at /keepalive to verify view-syncers and replication-managers are healthy. Sticky Sessions View syncers are designed to be disposable, but since they keep hydrated query pipelines in memory, it's important to try to keep clients connected to the same instance. If a reconnect/refresh lands on a different instance, that instance usually has to rehydrate instead of reusing warm state. If you are seeing a lot of Rehome errors, you may need to enable sticky sessions. Two instances can end up doing redundant hydration/advancement work for the same clientGroupID, and the \"loser\" will eventually force clients to reconnect. Rolling Updates You can roll out updates in the following order: Run additive database migrations (the expand/migrate part of the expand/migrate/contract pattern) and wait for replication to catch up. Upgrade replication-manager. Upgrade view-syncers (if they come up before the replication-manager, they'll sit in retry loops until the manager is updated). The replication-manager requires a full handoff, since it is the single owner of the changelog DB state. The view-syncers are simply drained and reconnected, since they are designed to be disposable. Update the API servers (your mutate and query endpoints). Update client(s). After most clients have refreshed, run contract migrations to drop or rename obsolete columns/tables. Contract migrations are destructive (dropping or renaming columns/tables). Make sure the API and clients are already updated and have had time to refresh before you remove columns. For renames, add the new column, backfill, deploy the app to use it, then drop the old column later. Client/Server Version Compatibility Servers are compatible with any client of same major version, and with clients one major version back. So for example: Server 0.2.* is compatible with client 0.2.* Server 0.2.* is compatible with client 0.1.* Server 2.*.* is compatible with client 2.*.* Server 2.*.* is compatible with client 1.*.* To upgrade Zero to a new major version, first deploy the new zero-cache, then the new frontend. Configuration The zero-cache image is configured via environment variables. See zero-cache Config for available options.",
    "headings": [
      {
        "text": "Minimum Viable Strategy",
        "id": "minimum-viable-strategy"
      },
      {
        "text": "Maximal Strategy",
        "id": "maximal-strategy"
      },
      {
        "text": "Replica Lifecycle",
        "id": "replica-lifecycle"
      },
      {
        "text": "Performance",
        "id": "performance"
      },
      {
        "text": "Hydration",
        "id": "hydration"
      },
      {
        "text": "IVM advancement",
        "id": "ivm-advancement"
      },
      {
        "text": "System-level",
        "id": "system-level"
      },
      {
        "text": "Load Balancing",
        "id": "load-balancing"
      },
      {
        "text": "Sticky Sessions",
        "id": "sticky-sessions"
      },
      {
        "text": "Rolling Updates",
        "id": "rolling-updates"
      },
      {
        "text": "Client/Server Version Compatibility",
        "id": "clientserver-version-compatibility"
      },
      {
        "text": "Configuration",
        "id": "configuration"
      }
    ],
    "kind": "page"
  },
  {
    "id": "125-deployment#minimum-viable-strategy",
    "title": "Deploying Zero",
    "searchTitle": "Minimum Viable Strategy",
    "sectionTitle": "Minimum Viable Strategy",
    "sectionId": "minimum-viable-strategy",
    "url": "/docs/deployment",
    "content": "The simplest way to deploy Zero is to run everything on a single node. This is the least expensive way to run Zero, and it can take you surprisingly far. Here is an example docker-compose.yml file for a single-node deployment (try it out!): services: upstream-db: image: postgres:18 environment: POSTGRES_DB: zero POSTGRES_PASSWORD: pass ports: - 5432:5432 command: postgres -c wal_level=logical healthcheck: test: pg_isready interval: 10s your-api: build: ./your-api ports: - 3000:3000 environment: # Your API handles mutations and writes to the PG db # This should be a pooled connection (e.g. pgbouncer) ZERO_UPSTREAM_DB: postgres://postgres:pass@upstream-db:5432/zero depends_on: upstream-db: condition: service_healthy zero-cache: image: rocicorp/zero:{version} ports: - 4848:4848 environment: # Used for replication from postgres # This must be a direct connection (not via pgbouncer) ZERO_UPSTREAM_DB: postgres://postgres:pass@upstream-db:5432/zero # Path to the SQLite replica ZERO_REPLICA_FILE: /data/zero.db # Password used to access the inspector and /statz ZERO_ADMIN_PASSWORD: pickanewpassword # URLs for your API's query and mutate endpoints ZERO_QUERY_URL: http://your-api:3000/api/zero/query ZERO_MUTATE_URL: http://your-api:3000/api/zero/mutate volumes: # Disk for the SQLite replica should be high IOPS - zero-cache-data:/data depends_on: your-api: condition: service_started healthcheck: test: curl -f http://localhost:4848/keepalive interval: 5s",
    "kind": "section"
  },
  {
    "id": "126-deployment#maximal-strategy",
    "title": "Deploying Zero",
    "searchTitle": "Maximal Strategy",
    "sectionTitle": "Maximal Strategy",
    "sectionId": "maximal-strategy",
    "url": "/docs/deployment",
    "content": "Once you reach the limits of the single-node deployment, you can split zero-cache into a multi-node topology. This is more expensive to run, but it gives you more flexibility and scalability. Here is an example docker-compose.yml file for a multi-node deployment (try it out!): services: upstream-db: image: postgres:18 environment: POSTGRES_DB: zero POSTGRES_PASSWORD: pass ports: - 5432:5432 command: postgres -c wal_level=logical healthcheck: test: pg_isready interval: 10s your-api: build: ./your-api ports: - 3000:3000 environment: # Your API handles mutations and writes to the PG db # This should be a pooled connection (e.g. pgbouncer) ZERO_UPSTREAM_DB: postgres://postgres:pass@upstream-db:5432/zero depends_on: upstream-db: condition: service_healthy # \"Mini S3\" (MinIO) provides a working s3://... `ZERO_LITESTREAM_BACKUP_URL` # This should be an S3-compatible object storage service in production. mini-s3: image: minio/minio:latest command: server /data --console-address \":9001\" healthcheck: test: curl -f http://localhost:9000/minio/health/live interval: 5s # Creates the bucket used by `ZERO_LITESTREAM_BACKUP_URL` # This is only needed for local development. mini-s3-create-bucket: image: minio/mc:latest depends_on: mini-s3: condition: service_healthy entrypoint: - /bin/sh - -lc - mc alias set local http://mini-s3:9000 \"minioadmin\" \"minioadmin\" && mc mb -p local/zero-backups || true replication-manager: image: rocicorp/zero:{version} ports: - 4849:4849 depends_on: upstream-db: condition: service_healthy your-api: condition: service_started mini-s3-create-bucket: condition: service_started environment: # Used for replication from postgres # this must be a direct connection (not via pgbouncer) ZERO_UPSTREAM_DB: postgres://postgres:pass@upstream-db:5432/zero # Used for storing client view records ZERO_CVR_DB: postgres://postgres:pass@upstream-db:5432/zero # Used for storing recent replication log entries ZERO_CHANGE_DB: postgres://postgres:pass@upstream-db:5432/zero # Path to the SQLite replica ZERO_REPLICA_FILE: /data/replica.db # Password used to access the inspector and /statz ZERO_ADMIN_PASSWORD: pickanewpassword # Dedicated replication-manager; disable view syncing. ZERO_NUM_SYNC_WORKERS: 0 # URL for backing up the SQLite replica # (include a simple version number for future cleanup) # Required in multi-node so view-syncers can reserve snapshots. ZERO_LITESTREAM_BACKUP_URL: s3://zero-backups/replica-v1 # S3 creds + Mini S3 endpoint (replication-manager backs up to S3) AWS_ACCESS_KEY_ID: minioadmin AWS_SECRET_ACCESS_KEY: minioadmin ZERO_LITESTREAM_ENDPOINT: http://mini-s3:9000 volumes: # storage for the SQLite replica should be high IOPS - replication-manager-data:/data healthcheck: test: curl -f http://localhost:4849/keepalive interval: 5s # Only one view-syncer in this example, but there can be N. view-syncer: image: rocicorp/zero:{version} ports: - 4848:4848 depends_on: replication-manager: condition: service_healthy environment: # Used for writing to the upstream database ZERO_UPSTREAM_DB: postgres://postgres:pass@upstream-db:5432/zero # Used for storing client view records ZERO_CVR_DB: postgres://postgres:pass@upstream-db:5432/zero # Used for storing recent replication log entries ZERO_CHANGE_DB: postgres://postgres:pass@upstream-db:5432/zero # Path to the SQLite replica ZERO_REPLICA_FILE: /data/replica.db # Password used to access the inspector and /statz ZERO_ADMIN_PASSWORD: pickanewpassword # URLs for your API's query and mutate endpoints ZERO_QUERY_URL: http://your-api:3000/api/zero/query ZERO_MUTATE_URL: http://your-api:3000/api/zero/mutate # URL for connecting to the replication-manager ZERO_CHANGE_STREAMER_URI: http://replication-manager:4849 # S3 creds + Mini S3 endpoint (view-syncers restore from S3 on startup) AWS_ACCESS_KEY_ID: minioadmin AWS_SECRET_ACCESS_KEY: minioadmin ZERO_LITESTREAM_ENDPOINT: http://mini-s3:9000 volumes: # Storage for the SQLite replica should be high IOPS - view-syncer-data:/data healthcheck: test: curl -f http://localhost:4848/keepalive interval: 5s The view-syncers in the multi-node topology can be horizontally scaled as needed. You can also override the number of sync workers per view-syncer with ZERO_NUM_SYNC_WORKERS.",
    "kind": "section"
  },
  {
    "id": "127-deployment#replica-lifecycle",
    "title": "Deploying Zero",
    "searchTitle": "Replica Lifecycle",
    "sectionTitle": "Replica Lifecycle",
    "sectionId": "replica-lifecycle",
    "url": "/docs/deployment",
    "content": "Zero-cache is backed by a SQLite replica of your database. The SQLite replica uses upstream Postgres as the source of truth. If the replica is missing or a litestream restore fails, the replication-manager will resync the replica from upstream on the next start.",
    "kind": "section"
  },
  {
    "id": "128-deployment#performance",
    "title": "Deploying Zero",
    "searchTitle": "Performance",
    "sectionTitle": "Performance",
    "sectionId": "performance",
    "url": "/docs/deployment",
    "content": "You want to optimize disk IOPS for the serving replica, since this is the file that is read by the view-syncers to run IVM-based queries, and one of the main bottlenecks for query hydration performance. View syncer's IVM is \"hydrate once, then incrementally push diffs\" against the ZQL pipeline, so performance is mostly about: How fast the server can materialize a subscription the first time (hydration). How fast it can keep it up to date (IVM advancement). Different bottlenecks dominate each phase. Hydration SQLite read cost: hydration is essentially \"run the query against the replica and stream all matching rows into the pipeline\", so it's bounded by SQLite scan/index performance + result size. Churn / TTL eviction: if queries get evicted (inactive long enough) and then get re-requested, you pay hydration again. Custom query transform latency: the HTTP request from zero-cache to your API at ZERO_QUERY_URL does transform/authorization for queries, adding network + CPU before hydration starts. IVM advancement Replication throughput: the view-syncer can only advance when the replicator commits and emits version-ready. If upstream replication is behind, query advancement is capped by how fast the replica advances. Change volume per transaction: advancement cost scales with number of changed rows, not number of queries. Circuit breaker behavior: if advancement looks like it'll take longer than rehydrating, zero-cache intentionally aborts and resets pipelines (which trades \"slow incremental\" for \"rehydrate\"). System-level Number of client groups per sync worker: each client group has its own pipelines; CPU and memory per group limits how many can be \"fast\" at once. Since Node is single-threaded, one client group can technically starve other groups. This is handled with time slicing and can be configured with the yield parameters, e.g. ZERO_YIELD_THRESHOLD_MS. SQLite concurrency limits: it's designed here for one writer (replicator) + many concurrent readers (view-syncer snapshots). It scales, but very heavy read workloads can still contend on cache/IO. Network to clients: even if IVM is fast, it can take time to send data over websocket. This can be improved by using CDNs (like CloudFront) that improve routing. Network between services: for a single-region deployment, all services should be colocated.",
    "kind": "section"
  },
  {
    "id": "129-deployment#hydration",
    "title": "Deploying Zero",
    "searchTitle": "Hydration",
    "sectionTitle": "Hydration",
    "sectionId": "hydration",
    "url": "/docs/deployment",
    "content": "SQLite read cost: hydration is essentially \"run the query against the replica and stream all matching rows into the pipeline\", so it's bounded by SQLite scan/index performance + result size. Churn / TTL eviction: if queries get evicted (inactive long enough) and then get re-requested, you pay hydration again. Custom query transform latency: the HTTP request from zero-cache to your API at ZERO_QUERY_URL does transform/authorization for queries, adding network + CPU before hydration starts.",
    "kind": "section"
  },
  {
    "id": "130-deployment#ivm-advancement",
    "title": "Deploying Zero",
    "searchTitle": "IVM advancement",
    "sectionTitle": "IVM advancement",
    "sectionId": "ivm-advancement",
    "url": "/docs/deployment",
    "content": "Replication throughput: the view-syncer can only advance when the replicator commits and emits version-ready. If upstream replication is behind, query advancement is capped by how fast the replica advances. Change volume per transaction: advancement cost scales with number of changed rows, not number of queries. Circuit breaker behavior: if advancement looks like it'll take longer than rehydrating, zero-cache intentionally aborts and resets pipelines (which trades \"slow incremental\" for \"rehydrate\").",
    "kind": "section"
  },
  {
    "id": "131-deployment#system-level",
    "title": "Deploying Zero",
    "searchTitle": "System-level",
    "sectionTitle": "System-level",
    "sectionId": "system-level",
    "url": "/docs/deployment",
    "content": "Number of client groups per sync worker: each client group has its own pipelines; CPU and memory per group limits how many can be \"fast\" at once. Since Node is single-threaded, one client group can technically starve other groups. This is handled with time slicing and can be configured with the yield parameters, e.g. ZERO_YIELD_THRESHOLD_MS. SQLite concurrency limits: it's designed here for one writer (replicator) + many concurrent readers (view-syncer snapshots). It scales, but very heavy read workloads can still contend on cache/IO. Network to clients: even if IVM is fast, it can take time to send data over websocket. This can be improved by using CDNs (like CloudFront) that improve routing. Network between services: for a single-region deployment, all services should be colocated.",
    "kind": "section"
  },
  {
    "id": "132-deployment#load-balancing",
    "title": "Deploying Zero",
    "searchTitle": "Load Balancing",
    "sectionTitle": "Load Balancing",
    "sectionId": "load-balancing",
    "url": "/docs/deployment",
    "content": "View syncers must be publicly reachable by port 4848 by clients, and the replication-manager can have internal networking with the view-syncers on port 4849. The external load balancer must support websockets, and can use the health check at /keepalive to verify view-syncers and replication-managers are healthy. Sticky Sessions View syncers are designed to be disposable, but since they keep hydrated query pipelines in memory, it's important to try to keep clients connected to the same instance. If a reconnect/refresh lands on a different instance, that instance usually has to rehydrate instead of reusing warm state. If you are seeing a lot of Rehome errors, you may need to enable sticky sessions. Two instances can end up doing redundant hydration/advancement work for the same clientGroupID, and the \"loser\" will eventually force clients to reconnect.",
    "kind": "section"
  },
  {
    "id": "133-deployment#sticky-sessions",
    "title": "Deploying Zero",
    "searchTitle": "Sticky Sessions",
    "sectionTitle": "Sticky Sessions",
    "sectionId": "sticky-sessions",
    "url": "/docs/deployment",
    "content": "View syncers are designed to be disposable, but since they keep hydrated query pipelines in memory, it's important to try to keep clients connected to the same instance. If a reconnect/refresh lands on a different instance, that instance usually has to rehydrate instead of reusing warm state. If you are seeing a lot of Rehome errors, you may need to enable sticky sessions. Two instances can end up doing redundant hydration/advancement work for the same clientGroupID, and the \"loser\" will eventually force clients to reconnect.",
    "kind": "section"
  },
  {
    "id": "134-deployment#rolling-updates",
    "title": "Deploying Zero",
    "searchTitle": "Rolling Updates",
    "sectionTitle": "Rolling Updates",
    "sectionId": "rolling-updates",
    "url": "/docs/deployment",
    "content": "You can roll out updates in the following order: Run additive database migrations (the expand/migrate part of the expand/migrate/contract pattern) and wait for replication to catch up. Upgrade replication-manager. Upgrade view-syncers (if they come up before the replication-manager, they'll sit in retry loops until the manager is updated). The replication-manager requires a full handoff, since it is the single owner of the changelog DB state. The view-syncers are simply drained and reconnected, since they are designed to be disposable. Update the API servers (your mutate and query endpoints). Update client(s). After most clients have refreshed, run contract migrations to drop or rename obsolete columns/tables. Contract migrations are destructive (dropping or renaming columns/tables). Make sure the API and clients are already updated and have had time to refresh before you remove columns. For renames, add the new column, backfill, deploy the app to use it, then drop the old column later. Client/Server Version Compatibility Servers are compatible with any client of same major version, and with clients one major version back. So for example: Server 0.2.* is compatible with client 0.2.* Server 0.2.* is compatible with client 0.1.* Server 2.*.* is compatible with client 2.*.* Server 2.*.* is compatible with client 1.*.* To upgrade Zero to a new major version, first deploy the new zero-cache, then the new frontend. Configuration The zero-cache image is configured via environment variables. See zero-cache Config for available options.",
    "kind": "section"
  },
  {
    "id": "135-deployment#clientserver-version-compatibility",
    "title": "Deploying Zero",
    "searchTitle": "Client/Server Version Compatibility",
    "sectionTitle": "Client/Server Version Compatibility",
    "sectionId": "clientserver-version-compatibility",
    "url": "/docs/deployment",
    "content": "Servers are compatible with any client of same major version, and with clients one major version back. So for example: Server 0.2.* is compatible with client 0.2.* Server 0.2.* is compatible with client 0.1.* Server 2.*.* is compatible with client 2.*.* Server 2.*.* is compatible with client 1.*.* To upgrade Zero to a new major version, first deploy the new zero-cache, then the new frontend.",
    "kind": "section"
  },
  {
    "id": "136-deployment#configuration",
    "title": "Deploying Zero",
    "searchTitle": "Configuration",
    "sectionTitle": "Configuration",
    "sectionId": "configuration",
    "url": "/docs/deployment",
    "content": "The zero-cache image is configured via environment variables. See zero-cache Config for available options.",
    "kind": "section"
  },
  {
    "id": "11-deprecated/ad-hoc-queries",
    "title": "Ad-Hoc Queries (Deprecated)",
    "searchTitle": "Ad-Hoc Queries (Deprecated)",
    "url": "/docs/deprecated/ad-hoc-queries",
    "content": "It will be removed in a future release of Zero. Please move to the new queries API. Overview Zero generates a query API for every table you sync. To enable, set the enableLegacyQueries option to true in your schema: const schema = createSchema({ // ... enableLegacyQueries: true }) Once enabled, queries are available at z.query.<tablename>. const zero = new Zero(...); const issues = await zero.query.issue.where('priority', 'high').run(); Each table is a ZQL builder object. See ZQL for details.",
    "headings": [
      {
        "text": "Overview",
        "id": "overview"
      }
    ],
    "kind": "page"
  },
  {
    "id": "137-deprecated/ad-hoc-queries#overview",
    "title": "Ad-Hoc Queries (Deprecated)",
    "searchTitle": "Overview",
    "sectionTitle": "Overview",
    "sectionId": "overview",
    "url": "/docs/deprecated/ad-hoc-queries",
    "content": "Zero generates a query API for every table you sync. To enable, set the enableLegacyQueries option to true in your schema: const schema = createSchema({ // ... enableLegacyQueries: true }) Once enabled, queries are available at z.query.<tablename>. const zero = new Zero(...); const issues = await zero.query.issue.where('priority', 'high').run(); Each table is a ZQL builder object. See ZQL for details.",
    "kind": "section"
  },
  {
    "id": "12-deprecated/crud-mutators",
    "title": "CRUD Mutators (Deprecated)",
    "searchTitle": "CRUD Mutators (Deprecated)",
    "url": "/docs/deprecated/crud-mutators",
    "content": "It will be removed in a future release of Zero. Please move to the new mutator API. Overview Zero generates basic CRUD mutators for every table you sync. To enable, set the enableLegacyMutators option to true in your schema: const schema = createSchema({ // ... enableLegacyMutators: true }) Once enabled, mutators are available at z.mutate.<tablename>: const zero = new Zero(...); zero.mutate.user.insert({ id: nanoid(), username: 'abby', language: 'en-us', }); See Writing Data with Mutators for more information. z.mutate contains the same methods as tx.mutate but is available on the Zero instance instead of the Transaction instance.",
    "headings": [
      {
        "text": "Overview",
        "id": "overview"
      }
    ],
    "kind": "page"
  },
  {
    "id": "138-deprecated/crud-mutators#overview",
    "title": "CRUD Mutators (Deprecated)",
    "searchTitle": "Overview",
    "sectionTitle": "Overview",
    "sectionId": "overview",
    "url": "/docs/deprecated/crud-mutators",
    "content": "Zero generates basic CRUD mutators for every table you sync. To enable, set the enableLegacyMutators option to true in your schema: const schema = createSchema({ // ... enableLegacyMutators: true }) Once enabled, mutators are available at z.mutate.<tablename>: const zero = new Zero(...); zero.mutate.user.insert({ id: nanoid(), username: 'abby', language: 'en-us', }); See Writing Data with Mutators for more information. z.mutate contains the same methods as tx.mutate but is available on the Zero instance instead of the Transaction instance.",
    "kind": "section"
  },
  {
    "id": "13-deprecated/rls-permissions",
    "title": "RLS Permissions (Deprecated)",
    "searchTitle": "RLS Permissions (Deprecated)",
    "url": "/docs/deprecated/rls-permissions",
    "content": "It will be removed in a future release of Zero. See Permissions for more information. Permissions are expressed using ZQL and run automatically with every read and write. Permissions are currently row based. Zero will eventually also have column permissions. Define Permissions Permissions are defined in schema.ts using the definePermissions function. Here's an example of limiting reads to members of an organization and deletes to only the creator of an issue: // The decoded value of your JWT. type AuthData = { // The logged-in user. sub: string } export const permissions = definePermissions< AuthData, Schema >(schema, () => { // Checks if the user exists in a related organization const allowIfInOrganization = ( authData: AuthData, eb: ExpressionBuilder<Schema, 'issue'> ) => eb.exists('organization', q => q.whereExists('user', q => q.where('id', authData.sub) ) ) // Checks if the user is the creator const allowIfIssueCreator = ( authData: AuthData, {cmp}: ExpressionBuilder<Schema, 'issue'> ) => cmp('creatorID', authData.sub) return { issue: { row: { select: [allowIfInOrganization], delete: [allowIfIssueCreator] } } } satisfies PermissionsConfig<AuthData, Schema> }) definePermission returns a policy object for each table in the schema. Each policy defines a ruleset for the operations that are possible on a table: select, insert, update, and delete. Access is Denied by Default If you don't specify any rules for an operation, it is denied by default. This is an important safety feature that helps ensure data isn't accidentally exposed. To enable full access to an action (i.e., during development) use the ANYONE_CAN helper: import {ANYONE_CAN} from '@rocicorp/zero' const permissions = definePermissions<AuthData, Schema>( schema, () => { return { issue: { row: { select: ANYONE_CAN // Other operations are denied by default. } } // Other tables are denied by default. } satisfies PermissionsConfig<AuthData, Schema> } ) To do this for all actions, use ANYONE_CAN_DO_ANYTHING: import {ANYONE_CAN_DO_ANYTHING} from '@rocicorp/zero' const permissions = definePermissions<AuthData, Schema>( schema, () => { return { // All operations on issue are allowed to all users. issue: ANYONE_CAN_DO_ANYTHING // Other tables are denied by default. } satisfies PermissionsConfig<AuthData, Schema> } ) Permission Evaluation Zero permissions are \"compiled\" into a JSON-based format at build-time. This file is stored in the {ZERO_APP_ID}.permissions table of your upstream database. Like other tables, it replicates live down to zero-cache. zero-cache then parses this file, and applies the encoded rules to every read and write operation. The compilation process is very simple-minded (read: dumb). Despite looking like normal TypeScript functions that receive an AuthData parameter, rule functions are not actually invoked at runtime. Instead, they are invoked with a \"placeholder\" AuthData at build time. We track which fields of this placeholder are accessed and construct a ZQL expression that accesses the right field of AuthData at runtime. The end result is that you can't really use most features of JS in these rules. Specifically you cannot: Iterate over properties or array elements in the auth token Use any JS features beyond property access of AuthData Use any conditional or global state Basically only property access is allowed. This is really confusing and we're working on a better solution. Permission Deployment During development, permissions are compiled and uploaded to your database completely automatically as part of the zero-cache-dev script. For production, you need to call npx zero-deploy-permissions within your app to update the permissions in the production database whenever they change. You would typically do this as part of your normal schema migration or CI process. For example, the SST deployment script for zbugs looks like this: new command.local.Command( 'zero-deploy-permissions', { create: `npx zero-deploy-permissions -p ../../src/schema.ts`, // Run the Command on every deploy ... triggers: [Date.now()], environment: { ZERO_UPSTREAM_DB: commonEnv.ZERO_UPSTREAM_DB, // If the application has a non-default App ID ... ZERO_APP_ID: commonEnv.ZERO_APP_ID } }, // after the view-syncer is deployed. {dependsOn: viewSyncer} ) See the SST Deployment Guide for more details. Rules Each operation on a policy has a ruleset containing zero or more rules. A rule is just a TypeScript function that receives the logged in user's AuthData and generates a ZQL where expression. At least one rule in a ruleset must return a row for the operation to be allowed. Select Permissions You can limit the data a user can read by specifying a select ruleset. Select permissions act like filters. If a user does not have permission to read a row, it will be filtered out of the result set. It will not generate an error. For example, imagine a select permission that restricts reads to only issues created by the user: definePermissions<AuthData, Schema>(schema, () => { const allowIfIssueCreator = ( authData: AuthData, {cmp}: ExpressionBuilder<Schema, 'issue'> ) => cmp('creatorID', authData.sub) return { issue: { row: { select: [allowIfIssueCreator] } } } satisfies PermissionsConfig<AuthData, Schema> }) If the issue table has two rows, one created by the user and one by someone else, the user will only see the row they created in any queries. Select permission applies to every column. The recommended approach for now is to factor out private fields into a separate table, e.g. user_private. Column permissions are planned but currently not a high priority. Note that although the same limitation applies to declarative insert/update permissions, custom mutators support arbitrary server-side logic and so can easily control which columns are writable. Insert Permissions You can limit what rows can be inserted and by whom by specifying an insert ruleset. Insert rules are evaluated after the entity is inserted. So if they query the database, they will see the inserted row present. If any rule in the insert ruleset returns a row, the insert is allowed. Here's an example of an insert rule that disallows inserting users that have the role 'admin'. definePermissions<AuthData, Schema>(schema, () => { const allowIfNonAdmin = ( authData: AuthData, {cmp}: ExpressionBuilder<Schema, 'user'> ) => cmp('role', '!=', 'admin') return { user: { row: { insert: [allowIfNonAdmin] } } } satisfies PermissionsConfig<AuthData, Schema> }) Update Permissions There are two types of update rulesets: preMutation and postMutation. Both rulesets must pass for an update to be allowed. preMutation rules see the version of a row before the mutation is applied. This is useful for things like checking whether a user owns an entity before editing it. postMutation rules see the version of a row after the mutation is applied. This is useful for things like ensuring a user can only mark themselves as the creator of an entity and not other users. Like other rulesets, preMutation and postMutation default to NOBODY_CAN. This means that every table must define both these rulesets in order for any updates to be allowed. For example, the following ruleset allows an issue's owner to edit, but not re-assign the issue. The postMutation rule enforces that the current user still own the issue after edit. definePermissions<AuthData, Schema>(schema, () => { const allowIfIssueOwner = ( authData: AuthData, {cmp}: ExpressionBuilder<Schema, 'issue'> ) => cmp('ownerID', authData.sub) return { issue: { row: { update: { preMutation: [allowIfIssueOwner], postMutation: [allowIfIssueOwner] } } } } satisfies PermissionsConfig<AuthData, Schema> }) This ruleset allows an issue's owner to edit and re-assign the issue: definePermissions<AuthData, Schema>(schema, () => { const allowIfIssueOwner = ( authData: AuthData, {cmp}: ExpressionBuilder<Schema, 'issue'> ) => cmp('ownerID', authData.sub) return { issue: { row: { update: { preMutation: [allowIfIssueOwner], postMutation: ANYONE_CAN } } } } satisfies PermissionsConfig<AuthData, Schema> }) And this allows anyone to edit an issue, but only if they also assign it to themselves. Useful for enforcing \"patches welcome\"? üôÉ definePermissions<AuthData, Schema>(schema, () => { const allowIfIssueOwner = ( authData: AuthData, {cmp}: ExpressionBuilder<Schema, 'issue'> ) => cmp('ownerID', authData.sub) return { issue: { row: { update: { preMutation: ANYONE_CAN, postMutation: [allowIfIssueOwner] } } } } satisfies PermissionsConfig<AuthData, Schema> }) Delete Permissions Delete permissions work in the same way as insert permissions except they run before the delete is applied. So if a delete rule queries the database, it will see that the deleted row is present. If any rule in the ruleset returns a row, the delete is allowed. Permissions Based on Auth Data You can use the cmpLit helper to define permissions based on a field of the authData parameter: definePermissions<AuthData, Schema>(schema, () => { const allowIfAdmin = ( authData: AuthData, {cmpLit}: ExpressionBuilder<Schema, 'issue'> ) => cmpLit(authData.role, 'admin') return { issue: { row: { select: [allowIfAdmin] } } } satisfies PermissionsConfig<AuthData, Schema> }) Debugging Given that permissions are defined in their own file and internally applied to queries, it can be hard to figure out if or why a permission check is failing. Read Permissions You can use the analyze-query utility with the --apply-permissions flag to see the complete query Zero runs, including read permissions. npx analyze-query --schema-path='./shared/schema.ts' --query='issue.related(\"comments\")' --apply-permissions --auth-data='{\"userId\":\"user-123\"}' If the result looks right, the problem may be that Zero is not receiving the AuthData that you think it is. You can retrieve a query hash from websocket or server logs, then ask Zero for the details on that specific query. Run this command with the same environment you run zero-cache with. It will use your upstream or cvr configuration to look up the query hash in the cvr database. npx analyze-query --schema-path='./shared/schema.ts' --hash='3rhuw19xt9vry' --apply-permissions --auth-data='{\"userId\":\"user-123\"}' The printed query can be different than the source ZQL string, because it is rebuilt from the query AST. But it should be logically equivalent to the query you wrote. Write Permissions Look for a WARN level log in the output from zero-cache like this: Permission check failed for {\"op\":\"update\",\"tableName\":\"message\",...}, action update, phase preMutation, authData: {...}, rowPolicies: [...], cellPolicies: [] Zero prints the row, auth data, and permission policies that was applied to any failed writes. The ZQL query is printed in AST format. See Query ASTs to convert it to a more readable format.",
    "headings": [
      {
        "text": "Define Permissions",
        "id": "define-permissions"
      },
      {
        "text": "Access is Denied by Default",
        "id": "access-is-denied-by-default"
      },
      {
        "text": "Permission Evaluation",
        "id": "permission-evaluation"
      },
      {
        "text": "Permission Deployment",
        "id": "permission-deployment"
      },
      {
        "text": "Rules",
        "id": "rules"
      },
      {
        "text": "Select Permissions",
        "id": "select-permissions"
      },
      {
        "text": "Insert Permissions",
        "id": "insert-permissions"
      },
      {
        "text": "Update Permissions",
        "id": "update-permissions"
      },
      {
        "text": "Delete Permissions",
        "id": "delete-permissions"
      },
      {
        "text": "Permissions Based on Auth Data",
        "id": "permissions-based-on-auth-data"
      },
      {
        "text": "Debugging",
        "id": "debugging"
      },
      {
        "text": "Read Permissions",
        "id": "read-permissions"
      },
      {
        "text": "Write Permissions",
        "id": "write-permissions"
      }
    ],
    "kind": "page"
  },
  {
    "id": "139-deprecated/rls-permissions#define-permissions",
    "title": "RLS Permissions (Deprecated)",
    "searchTitle": "Define Permissions",
    "sectionTitle": "Define Permissions",
    "sectionId": "define-permissions",
    "url": "/docs/deprecated/rls-permissions",
    "content": "Permissions are defined in schema.ts using the definePermissions function. Here's an example of limiting reads to members of an organization and deletes to only the creator of an issue: // The decoded value of your JWT. type AuthData = { // The logged-in user. sub: string } export const permissions = definePermissions< AuthData, Schema >(schema, () => { // Checks if the user exists in a related organization const allowIfInOrganization = ( authData: AuthData, eb: ExpressionBuilder<Schema, 'issue'> ) => eb.exists('organization', q => q.whereExists('user', q => q.where('id', authData.sub) ) ) // Checks if the user is the creator const allowIfIssueCreator = ( authData: AuthData, {cmp}: ExpressionBuilder<Schema, 'issue'> ) => cmp('creatorID', authData.sub) return { issue: { row: { select: [allowIfInOrganization], delete: [allowIfIssueCreator] } } } satisfies PermissionsConfig<AuthData, Schema> }) definePermission returns a policy object for each table in the schema. Each policy defines a ruleset for the operations that are possible on a table: select, insert, update, and delete.",
    "kind": "section"
  },
  {
    "id": "140-deprecated/rls-permissions#access-is-denied-by-default",
    "title": "RLS Permissions (Deprecated)",
    "searchTitle": "Access is Denied by Default",
    "sectionTitle": "Access is Denied by Default",
    "sectionId": "access-is-denied-by-default",
    "url": "/docs/deprecated/rls-permissions",
    "content": "If you don't specify any rules for an operation, it is denied by default. This is an important safety feature that helps ensure data isn't accidentally exposed. To enable full access to an action (i.e., during development) use the ANYONE_CAN helper: import {ANYONE_CAN} from '@rocicorp/zero' const permissions = definePermissions<AuthData, Schema>( schema, () => { return { issue: { row: { select: ANYONE_CAN // Other operations are denied by default. } } // Other tables are denied by default. } satisfies PermissionsConfig<AuthData, Schema> } ) To do this for all actions, use ANYONE_CAN_DO_ANYTHING: import {ANYONE_CAN_DO_ANYTHING} from '@rocicorp/zero' const permissions = definePermissions<AuthData, Schema>( schema, () => { return { // All operations on issue are allowed to all users. issue: ANYONE_CAN_DO_ANYTHING // Other tables are denied by default. } satisfies PermissionsConfig<AuthData, Schema> } )",
    "kind": "section"
  },
  {
    "id": "141-deprecated/rls-permissions#permission-evaluation",
    "title": "RLS Permissions (Deprecated)",
    "searchTitle": "Permission Evaluation",
    "sectionTitle": "Permission Evaluation",
    "sectionId": "permission-evaluation",
    "url": "/docs/deprecated/rls-permissions",
    "content": "Zero permissions are \"compiled\" into a JSON-based format at build-time. This file is stored in the {ZERO_APP_ID}.permissions table of your upstream database. Like other tables, it replicates live down to zero-cache. zero-cache then parses this file, and applies the encoded rules to every read and write operation. The compilation process is very simple-minded (read: dumb). Despite looking like normal TypeScript functions that receive an AuthData parameter, rule functions are not actually invoked at runtime. Instead, they are invoked with a \"placeholder\" AuthData at build time. We track which fields of this placeholder are accessed and construct a ZQL expression that accesses the right field of AuthData at runtime. The end result is that you can't really use most features of JS in these rules. Specifically you cannot: Iterate over properties or array elements in the auth token Use any JS features beyond property access of AuthData Use any conditional or global state Basically only property access is allowed. This is really confusing and we're working on a better solution.",
    "kind": "section"
  },
  {
    "id": "142-deprecated/rls-permissions#permission-deployment",
    "title": "RLS Permissions (Deprecated)",
    "searchTitle": "Permission Deployment",
    "sectionTitle": "Permission Deployment",
    "sectionId": "permission-deployment",
    "url": "/docs/deprecated/rls-permissions",
    "content": "During development, permissions are compiled and uploaded to your database completely automatically as part of the zero-cache-dev script. For production, you need to call npx zero-deploy-permissions within your app to update the permissions in the production database whenever they change. You would typically do this as part of your normal schema migration or CI process. For example, the SST deployment script for zbugs looks like this: new command.local.Command( 'zero-deploy-permissions', { create: `npx zero-deploy-permissions -p ../../src/schema.ts`, // Run the Command on every deploy ... triggers: [Date.now()], environment: { ZERO_UPSTREAM_DB: commonEnv.ZERO_UPSTREAM_DB, // If the application has a non-default App ID ... ZERO_APP_ID: commonEnv.ZERO_APP_ID } }, // after the view-syncer is deployed. {dependsOn: viewSyncer} ) See the SST Deployment Guide for more details.",
    "kind": "section"
  },
  {
    "id": "143-deprecated/rls-permissions#rules",
    "title": "RLS Permissions (Deprecated)",
    "searchTitle": "Rules",
    "sectionTitle": "Rules",
    "sectionId": "rules",
    "url": "/docs/deprecated/rls-permissions",
    "content": "Each operation on a policy has a ruleset containing zero or more rules. A rule is just a TypeScript function that receives the logged in user's AuthData and generates a ZQL where expression. At least one rule in a ruleset must return a row for the operation to be allowed.",
    "kind": "section"
  },
  {
    "id": "144-deprecated/rls-permissions#select-permissions",
    "title": "RLS Permissions (Deprecated)",
    "searchTitle": "Select Permissions",
    "sectionTitle": "Select Permissions",
    "sectionId": "select-permissions",
    "url": "/docs/deprecated/rls-permissions",
    "content": "You can limit the data a user can read by specifying a select ruleset. Select permissions act like filters. If a user does not have permission to read a row, it will be filtered out of the result set. It will not generate an error. For example, imagine a select permission that restricts reads to only issues created by the user: definePermissions<AuthData, Schema>(schema, () => { const allowIfIssueCreator = ( authData: AuthData, {cmp}: ExpressionBuilder<Schema, 'issue'> ) => cmp('creatorID', authData.sub) return { issue: { row: { select: [allowIfIssueCreator] } } } satisfies PermissionsConfig<AuthData, Schema> }) If the issue table has two rows, one created by the user and one by someone else, the user will only see the row they created in any queries. Select permission applies to every column. The recommended approach for now is to factor out private fields into a separate table, e.g. user_private. Column permissions are planned but currently not a high priority. Note that although the same limitation applies to declarative insert/update permissions, custom mutators support arbitrary server-side logic and so can easily control which columns are writable.",
    "kind": "section"
  },
  {
    "id": "145-deprecated/rls-permissions#insert-permissions",
    "title": "RLS Permissions (Deprecated)",
    "searchTitle": "Insert Permissions",
    "sectionTitle": "Insert Permissions",
    "sectionId": "insert-permissions",
    "url": "/docs/deprecated/rls-permissions",
    "content": "You can limit what rows can be inserted and by whom by specifying an insert ruleset. Insert rules are evaluated after the entity is inserted. So if they query the database, they will see the inserted row present. If any rule in the insert ruleset returns a row, the insert is allowed. Here's an example of an insert rule that disallows inserting users that have the role 'admin'. definePermissions<AuthData, Schema>(schema, () => { const allowIfNonAdmin = ( authData: AuthData, {cmp}: ExpressionBuilder<Schema, 'user'> ) => cmp('role', '!=', 'admin') return { user: { row: { insert: [allowIfNonAdmin] } } } satisfies PermissionsConfig<AuthData, Schema> })",
    "kind": "section"
  },
  {
    "id": "146-deprecated/rls-permissions#update-permissions",
    "title": "RLS Permissions (Deprecated)",
    "searchTitle": "Update Permissions",
    "sectionTitle": "Update Permissions",
    "sectionId": "update-permissions",
    "url": "/docs/deprecated/rls-permissions",
    "content": "There are two types of update rulesets: preMutation and postMutation. Both rulesets must pass for an update to be allowed. preMutation rules see the version of a row before the mutation is applied. This is useful for things like checking whether a user owns an entity before editing it. postMutation rules see the version of a row after the mutation is applied. This is useful for things like ensuring a user can only mark themselves as the creator of an entity and not other users. Like other rulesets, preMutation and postMutation default to NOBODY_CAN. This means that every table must define both these rulesets in order for any updates to be allowed. For example, the following ruleset allows an issue's owner to edit, but not re-assign the issue. The postMutation rule enforces that the current user still own the issue after edit. definePermissions<AuthData, Schema>(schema, () => { const allowIfIssueOwner = ( authData: AuthData, {cmp}: ExpressionBuilder<Schema, 'issue'> ) => cmp('ownerID', authData.sub) return { issue: { row: { update: { preMutation: [allowIfIssueOwner], postMutation: [allowIfIssueOwner] } } } } satisfies PermissionsConfig<AuthData, Schema> }) This ruleset allows an issue's owner to edit and re-assign the issue: definePermissions<AuthData, Schema>(schema, () => { const allowIfIssueOwner = ( authData: AuthData, {cmp}: ExpressionBuilder<Schema, 'issue'> ) => cmp('ownerID', authData.sub) return { issue: { row: { update: { preMutation: [allowIfIssueOwner], postMutation: ANYONE_CAN } } } } satisfies PermissionsConfig<AuthData, Schema> }) And this allows anyone to edit an issue, but only if they also assign it to themselves. Useful for enforcing \"patches welcome\"? üôÉ definePermissions<AuthData, Schema>(schema, () => { const allowIfIssueOwner = ( authData: AuthData, {cmp}: ExpressionBuilder<Schema, 'issue'> ) => cmp('ownerID', authData.sub) return { issue: { row: { update: { preMutation: ANYONE_CAN, postMutation: [allowIfIssueOwner] } } } } satisfies PermissionsConfig<AuthData, Schema> })",
    "kind": "section"
  },
  {
    "id": "147-deprecated/rls-permissions#delete-permissions",
    "title": "RLS Permissions (Deprecated)",
    "searchTitle": "Delete Permissions",
    "sectionTitle": "Delete Permissions",
    "sectionId": "delete-permissions",
    "url": "/docs/deprecated/rls-permissions",
    "content": "Delete permissions work in the same way as insert permissions except they run before the delete is applied. So if a delete rule queries the database, it will see that the deleted row is present. If any rule in the ruleset returns a row, the delete is allowed.",
    "kind": "section"
  },
  {
    "id": "148-deprecated/rls-permissions#permissions-based-on-auth-data",
    "title": "RLS Permissions (Deprecated)",
    "searchTitle": "Permissions Based on Auth Data",
    "sectionTitle": "Permissions Based on Auth Data",
    "sectionId": "permissions-based-on-auth-data",
    "url": "/docs/deprecated/rls-permissions",
    "content": "You can use the cmpLit helper to define permissions based on a field of the authData parameter: definePermissions<AuthData, Schema>(schema, () => { const allowIfAdmin = ( authData: AuthData, {cmpLit}: ExpressionBuilder<Schema, 'issue'> ) => cmpLit(authData.role, 'admin') return { issue: { row: { select: [allowIfAdmin] } } } satisfies PermissionsConfig<AuthData, Schema> })",
    "kind": "section"
  },
  {
    "id": "149-deprecated/rls-permissions#debugging",
    "title": "RLS Permissions (Deprecated)",
    "searchTitle": "Debugging",
    "sectionTitle": "Debugging",
    "sectionId": "debugging",
    "url": "/docs/deprecated/rls-permissions",
    "content": "Given that permissions are defined in their own file and internally applied to queries, it can be hard to figure out if or why a permission check is failing. Read Permissions You can use the analyze-query utility with the --apply-permissions flag to see the complete query Zero runs, including read permissions. npx analyze-query --schema-path='./shared/schema.ts' --query='issue.related(\"comments\")' --apply-permissions --auth-data='{\"userId\":\"user-123\"}' If the result looks right, the problem may be that Zero is not receiving the AuthData that you think it is. You can retrieve a query hash from websocket or server logs, then ask Zero for the details on that specific query. Run this command with the same environment you run zero-cache with. It will use your upstream or cvr configuration to look up the query hash in the cvr database. npx analyze-query --schema-path='./shared/schema.ts' --hash='3rhuw19xt9vry' --apply-permissions --auth-data='{\"userId\":\"user-123\"}' The printed query can be different than the source ZQL string, because it is rebuilt from the query AST. But it should be logically equivalent to the query you wrote. Write Permissions Look for a WARN level log in the output from zero-cache like this: Permission check failed for {\"op\":\"update\",\"tableName\":\"message\",...}, action update, phase preMutation, authData: {...}, rowPolicies: [...], cellPolicies: [] Zero prints the row, auth data, and permission policies that was applied to any failed writes. The ZQL query is printed in AST format. See Query ASTs to convert it to a more readable format.",
    "kind": "section"
  },
  {
    "id": "150-deprecated/rls-permissions#read-permissions",
    "title": "RLS Permissions (Deprecated)",
    "searchTitle": "Read Permissions",
    "sectionTitle": "Read Permissions",
    "sectionId": "read-permissions",
    "url": "/docs/deprecated/rls-permissions",
    "content": "You can use the analyze-query utility with the --apply-permissions flag to see the complete query Zero runs, including read permissions. npx analyze-query --schema-path='./shared/schema.ts' --query='issue.related(\"comments\")' --apply-permissions --auth-data='{\"userId\":\"user-123\"}' If the result looks right, the problem may be that Zero is not receiving the AuthData that you think it is. You can retrieve a query hash from websocket or server logs, then ask Zero for the details on that specific query. Run this command with the same environment you run zero-cache with. It will use your upstream or cvr configuration to look up the query hash in the cvr database. npx analyze-query --schema-path='./shared/schema.ts' --hash='3rhuw19xt9vry' --apply-permissions --auth-data='{\"userId\":\"user-123\"}' The printed query can be different than the source ZQL string, because it is rebuilt from the query AST. But it should be logically equivalent to the query you wrote.",
    "kind": "section"
  },
  {
    "id": "151-deprecated/rls-permissions#write-permissions",
    "title": "RLS Permissions (Deprecated)",
    "searchTitle": "Write Permissions",
    "sectionTitle": "Write Permissions",
    "sectionId": "write-permissions",
    "url": "/docs/deprecated/rls-permissions",
    "content": "Look for a WARN level log in the output from zero-cache like this: Permission check failed for {\"op\":\"update\",\"tableName\":\"message\",...}, action update, phase preMutation, authData: {...}, rowPolicies: [...], cellPolicies: [] Zero prints the row, auth data, and permission policies that was applied to any failed writes. The ZQL query is printed in AST format. See Query ASTs to convert it to a more readable format.",
    "kind": "section"
  },
  {
    "id": "14-install",
    "title": "Install Zero",
    "searchTitle": "Install Zero",
    "url": "/docs/install",
    "content": "This guide walks you through adding Zero to any TypeScript-based web app. It should take about 20 minutes to complete. When you're done, you'll have Zero up and running and will understand its core ideas. Integrate Zero Set Up Your Database You'll need a local Postgres database for development. If you don't have a preferred method, we recommend using Docker: docker run -d --name zero-postgres \\ -e POSTGRES_PASSWORD=\"password\" \\ -p 5432:5432 \\ postgres:16-alpine \\ # IMPORTANT: logical WAL level is required for Zero # to sync data to its SQLite replica postgres -c wal_level=logical This will start a Postgres database running in the background. See Connecting to Postgres for more details on what Postgres features are required for Zero to work. Install and Run Zero-Cache Add Zero to your project: npm install @rocicorp/zeropnpm add @rocicorp/zero # Note: pnpm disables postinstall scripts by default for security. # Either approve the build: pnpm approve-builds # Or add to package.json: # \"pnpm\": { # \"onlyBuiltDependencies\": [\"@rocicorp/zero-sqlite3\"] # }bun add @rocicorp/zero # Note: Bun disables postinstall scripts by default for security. # Add to package.json: # \"trustedDependencies\": [\"@rocicorp/zero-sqlite3\"]yarn add @rocicorp/zero Start the development zero-cache by running the following command: export ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" npx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" pnpm exec zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" bunx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" yarn exec zero-cache-dev Zero works by continuously replicating your upstream database into a SQLite replica. Zero-cache runs client queries against the replica. If there are tables or columns that will not be queried by Zero clients ever, you can exclude them. You can use the zero-sqlite3 tool to explore zero.db. Try it out by connecting to Postgres and the Zero replica in two different terminals. If you change something in Postgres, you'll see it immediately show up in the replica: Set Up Your Zero Schema Zero uses a file called schema.ts to provide a type-safe query API. If you use Drizzle or Prisma, you can generate schema.ts automatically. Otherwise, you can create it manually. npm install -D drizzle-zero npx drizzle-zero generatepnpm add -D drizzle-zero pnpm dlx drizzle-zero generatebun add -D drizzle-zero bunx drizzle-zero generateyarn add -D drizzle-zero yarn dlx drizzle-zero generatenpm install -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } npx prisma generatepnpm add -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } pnpx prisma generatebun add -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } bunx prisma generateyarn add -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } yarn prisma generate// zero/schema.ts import {table, string, createSchema} from '@rocicorp/zero' const user = table('user') .columns({ id: string(), name: string() // ... other columns ... }) .primaryKey('id') // ... more tables and relationships ... // See \"Schema\" page in left nav for complete schema API. export const schema = createSchema({ tables: [user] }) // Register the schema for type safety declare module '@rocicorp/zero' { interface DefaultTypes { schema: typeof schema } } Zero has some restrictions on the Postgres features it supports. You can continue this tutorial with a sample schema and seed data to evaluate it. Set Up the Zero Client Zero has first-class support for React and SolidJS, and community support for Svelte and Vue. There is also a low-level API you can use in any TypeScript-based project. // root.tsx import {ZeroProvider} from '@rocicorp/zero/react' import type {ZeroOptions} from '@rocicorp/zero' import {schema} from './zero/schema.ts' const opts: ZeroOptions = { userID: 'anon', cacheURL: 'http://localhost:4848', schema } function Root() { return ( <ZeroProvider {...opts}> <App /> </ZeroProvider> ) } // mycomponent.tsx import {useZero} from '@rocicorp/zero/react' function MyComponent() { const zero = useZero() console.log('clientID', zero.clientID) }// root.tsx import {ZeroProvider} from '@rocicorp/zero/solid' import type {ZeroOptions} from '@rocicorp/zero' import {schema} from './zero/schema.ts' const opts: ZeroOptions = { userID: 'anon', cacheURL: 'http://localhost:4848', schema } function Root() { return ( <ZeroProvider {...opts}> <App /> </ZeroProvider> ) } // mycomponent.tsx import {useZero} from '@rocicorp/zero/solid' function MyComponent() { const zero = useZero() console.log('clientID', zero().clientID) }// zero.ts import {Zero} from '@rocicorp/zero' import {schema} from './zero/schema.ts' const opts: ZeroOptions = { userID: 'anon', cacheURL: 'http://localhost:4848', schema } const zero = new Zero(opts) console.log('clientID', zero.clientID) export {zero} Sync Data Define Query Alright, let's sync some data! In Zero, we do this with queries. Queries are conventionally found in a queries.ts file. Here is an example of how queries are defined - you can adapt this to your own schema: // zero/queries.ts import {defineQueries, defineQuery} from '@rocicorp/zero' import {z} from 'zod' import {zql} from './schema.ts' export const queries = defineQueries({ albums: { byArtist: defineQuery( z.object({artistID: z.string()}), ({args: {artistID}}) => zql.albums .where('artistId', artistID) .orderBy('createdAt', 'asc') .limit(10) .related('artist', q => q.one()) ) } }) Use zql from schema.ts to construct and return a ZQL query. ZQL is quite powerful and allows you to build queries with filters, sorts, relationships, and more: See queries for more information on defining queries. Invoke Query Querying for data is framework-specific. Most of the time, you will use a helper like useQuery that integrates into your framework's rendering model: // mycomponent.tsx import {useQuery} from '@rocicorp/zero/react' import {queries} from './zero/queries.ts' function MyComponent() { const [albums] = useQuery( queries.albums.byArtist({artistID: 'artist_1'}) ) return albums.map(a => <div key={a.id}>{a.title}</div>) }// mycomponent.tsx import {useQuery} from '@rocicorp/zero/solid' import {queries} from './zero/queries.ts' function MyComponent() { const [albums] = useQuery(() => queries.albums.byArtist({artistID: 'artist_1'}) ) return ( <For each={albums()}> {album => <div key={album.id}>{album.title}</div>} </For> ) }// albums.ts import {zero} from './zero.ts' import {queries} from './zero/queries.ts' const albums = await zero.run( queries.albums.byArtist({artistID: 'artist_1'}) ) console.log('albums', albums) When you reload your app, you should see an error like: > npx zero-cache-dev ... no ZERO_QUERY_URL is configured for Zero Cache> pnpm exec zero-cache-dev ... no ZERO_QUERY_URL is configured for Zero Cache> bunx zero-cache-dev ... no ZERO_QUERY_URL is configured for Zero Cache> yarn exec zero-cache-dev ... no ZERO_QUERY_URL is configured for Zero Cache This is expected. We now need to implement a queries endpoint so that zero-cache can get the ZQL for the albums.byArtist query. Implement Query Backend Zero doesn't allow clients to run any arbitrary ZQL against zero-cache, for both security and performance reasons. Instead, Zero sends the name and arguments of the query to a queries endpoint on your server that is responsible for transforming the named query into ZQL. Zero provides utilities to make it easy to implement the queries endpoint in any full-stack framework: // src/routes/api/query.ts import {createFileRoute} from '@tanstack/react-router' import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from '../../zero/queries.ts' import {schema} from '../../zero/schema.ts' export const Route = createFileRoute('/api/query')({ server: { handlers: { POST: async ({request}) => { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, request ) return Response.json(result) } } } })// app/api/query/route.ts import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from '../../zero/queries.ts' import {schema} from '../../zero/schema.ts' export async function POST(req: Request) { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, req ) return Response.json(result) }// src/routes/api/query.ts import type {APIEvent} from '@solidjs/start/server' import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from '../../zero/queries.ts' import {schema} from '../../zero/schema.ts' export async function POST(event: APIEvent) { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, event.request ) return new Response.json(result) }// api/app.ts import {Hono} from 'hono' import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from '../zero/queries.ts' import {schema} from '../zero/schema.ts' const app = new Hono() app.post('/api/query', async c => { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, c.req.raw ) return c.json(result) }) Stop and re-run zero-cache with the URL of the queries endpoint: export ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" npx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" pnpm exec zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" bunx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" yarn exec zero-cache-dev If you reload the page, you will see data! Zero queries update live, so if you edit data in Postgres directly, you will see it update in the Zero replica AND the UI: More about Queries You now know the basics, but there are a few more important pieces you'll need to learn for your first real app: How authentication and permissions work. Preloading queries to create instantly responsive UI. For these details and more, see Reading Data. But for now, let's move on to writes! Mutate Data Define Mutators Data is written in Zero apps using mutators. Similar to queries, we use a shared mutators.ts file: // zero/mutators.ts import {defineMutators, defineMutator} from '@rocicorp/zero' import {z} from 'zod' export const mutators = defineMutators({ albums: { create: defineMutator( z.object({ id: z.string(), artistID: z.string(), title: z.string(), year: z.number(), createdAt: z.number() }), async ({args, tx}) => { await tx.mutate.albums.insert({ id: args.id, artistId: args.artistID, title: args.title, releaseYear: args.year, createdAt: args.createdAt }) } ) } }) You can use the CRUD-style API with tx.mutate.<table>.<method>() to write data. You can also use tx.run(zql.<table>.<method>) to run queries within your mutator. Mutators almost always run in the same frame on the client, against local data. The reason mutators are marked async is because on the server, reading from the tx object goes over the network to Postgres. Also, in edge cases on the client, reads and writes can go to local storage (IndexedDB or SQLite). Once you've defined your mutators, you must register them with Zero before you can use them: import {mutators} from './zero/mutators.ts' const opts: ZeroOptions = { // ... userID, cacheURL, etc. // add mutators mutators } Invoke Mutators You can now call mutators via zero.mutate: // mycomponent.tsx import {useZero} from '@rocicorp/zero/react' import {mutators} from './zero/mutators.ts' import {nanoid} from 'nanoid' function MyComponent() { const zero = useZero() const onClick = async () => { const result = zero.mutate( mutators.albums.create({ id: nanoid(), artistID: 'artist_1', title: 'Please Please Me', year: 1963, createdAt: Date.now() }) ) const clientResult = await result.client if (clientResult.type === 'error') { console.error( 'Failed to create album', clientResult.error.message ) } else { console.log('Album created!') } } return <button onClick={onClick}>Create Album</button> }// mycomponent.tsx import {useZero} from '@rocicorp/zero/solid' import {mutators} from './zero/mutators.ts' import {nanoid} from 'nanoid' function MyComponent() { const zero = useZero() const onClick = async () => { const result = zero().mutate( mutators.albums.create({ id: nanoid(), artistID: 'artist_1', title: 'Please Please Me', year: 1963, createdAt: Date.now() }) ) const clientResult = await result.client if (clientResult.type === 'error') { console.error( 'Failed to create album', clientResult.error.message ) } else { console.log('Album created!') } } return <button onClick={onClick}>Create Album</button> }// albums.ts import {zero} from './zero.ts' import {mutators} from './zero/mutators.ts' import {nanoid} from 'nanoid' const result = await zero.mutate( mutators.albums.create({ id: nanoid(), artistID: 'artist_1', title: 'Please Please Me', year: 1963, createdAt: Date.now() }) ) const clientResult = await result.client if (clientResult.type === 'error') { console.error( 'Failed to create album', clientResult.error.message ) } else { console.log('Album created!') } Client-generated random IDs (from libraries like uuid, ulid, or nanoid) work much better than auto-incrementing integers in sync engines like Zero. Learn more. If you run this app now, you should be able to see the UI update optimistically, but you'll also see an error in zero-cache: > npx zero-cache-dev ... A ZERO_MUTATE_URL must be set in order to process mutations> pnpm exec zero-cache-dev ... A ZERO_MUTATE_URL must be set in order to process mutations> bunx zero-cache-dev ... A ZERO_MUTATE_URL must be set in order to process mutations> yarn exec zero-cache-dev ... A ZERO_MUTATE_URL must be set in order to process mutations Similar to queries, we need to wire up a mutate endpoint in our API. Let's do that now. Implement Mutate Endpoint Zero requires a mutate endpoint which runs on your server and connects directly to your Postgres database. Zero provides helpers to implement this easily. Use the Zero Postgres adapters to create a dbProvider instance: // src/db-provider.ts import {zeroDrizzle} from '@rocicorp/zero/server/adapters/drizzle' import {drizzle} from 'drizzle-orm/node-postgres' import {Pool} from 'pg' import {schema} from '../../zero/schema.ts' import * as drizzleSchema from '../../drizzle/schema.ts' // pass a drizzle client instance. for example: const pool = new Pool({ connectionString: process.env.ZERO_UPSTREAM_DB! }) export const drizzleClient = drizzle(pool, { schema: drizzleSchema }) export const dbProvider = zeroDrizzle(schema, drizzleClient) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } }// src/db-provider.ts import {PrismaPg} from '@prisma/adapter-pg' import {PrismaClient} from '@prisma/client' import {zeroPrisma} from '@rocicorp/zero/server/adapters/prisma' import {schema} from '../../zero/schema.ts' // pass a prisma client instance. for example: const prisma = new PrismaClient({ adapter: new PrismaPg({ connectionString: process.env.ZERO_UPSTREAM_DB! }) }) export const dbProvider = zeroPrisma(schema, prisma) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } }// src/db-provider.ts import {zeroNodePg} from '@rocicorp/zero/server/adapters/pg' import {Pool} from 'pg' import {schema} from '../../zero/schema.ts' const pool = new Pool({ connectionString: process.env.ZERO_UPSTREAM_DB! }) export const dbProvider = zeroNodePg(schema, pool) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } }// src/db-provider.ts import {zeroPostgresJS} from '@rocicorp/zero/server/adapters/postgresjs' import postgres from 'postgres' import {schema} from '../../zero/schema.ts' const sql = postgres(process.env.ZERO_UPSTREAM_DB!) export const dbProvider = zeroPostgresJS(schema, sql) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } } Then, use the dbProvider to handle the mutate request: // src/routes/api/mutate.ts import {createFileRoute} from '@tanstack/react-router' import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from '../../zero/mutators.ts' import {dbProvider} from '../../db-provider.ts' export const Route = createFileRoute('/api/mutate')({ server: { handlers: { POST: async ({request}) => { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({ args, tx, ctx: {userId: 'anon'} }) }), request ) return Response.json(result) } } } })// app/api/mutate/route.ts import {handleMutateRequest} from '@rocicorp/zero/server' import {mutators} from '../../zero/mutators.ts' import {dbProvider} from '../../db-provider.ts' import {mustGetMutator} from '@rocicorp/zero' export async function POST(req: Request) { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({args, tx, ctx: {userId: 'anon'}}) }), req ) return Response.json(result) }// src/routes/api/mutate.ts import type {APIEvent} from '@solidjs/start/server' import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from '../../zero/mutators.ts' import {dbProvider} from '../../db-provider.ts' export async function POST(event: APIEvent) { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({args, tx, ctx: {userId: 'anon'}}) }), event.request ) return new Response.json(result) }// api/app.ts import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from '../zero/mutators.ts' import {dbProvider} from './db-provider.ts' app.post('/api/mutate', async c => { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({args, tx, ctx: {userId: 'anon'}}) }), c.req.raw ) return c.json(result) }) Restart zero-cache to tell it about this new endpoint: export ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" export ZERO_MUTATE_URL=\"http://localhost:3000/api/mutate\" npx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" export ZERO_MUTATE_URL=\"http://localhost:3000/api/mutate\" pnpm exec zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" export ZERO_MUTATE_URL=\"http://localhost:3000/api/mutate\" bunx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" export ZERO_MUTATE_URL=\"http://localhost:3000/api/mutate\" yarn exec zero-cache-dev If you refresh the page, your mutation should commit to the database and sync to other clients: More about Mutators Just as with queries, the separate server implementation of mutators extends elegantly to enable write permissions. Zero also has built-in helpers to do work after a mutator runs on the server, like send notifications. For these details and more, see Writing Data. That's It! Congratulations! You now know the basics for building with Zero ü§Ø. Possible next steps: Learn about authentication and permissions See some samples of built-out Zero apps Learn how to deploy your app to production",
    "headings": [
      {
        "text": "Integrate Zero",
        "id": "integrate-zero"
      },
      {
        "text": "Set Up Your Database",
        "id": "set-up-your-database"
      },
      {
        "text": "Install and Run Zero-Cache",
        "id": "install-and-run-zero-cache"
      },
      {
        "text": "Set Up Your Zero Schema",
        "id": "set-up-your-zero-schema"
      },
      {
        "text": "Set Up the Zero Client",
        "id": "set-up-the-zero-client"
      },
      {
        "text": "Sync Data",
        "id": "sync-data"
      },
      {
        "text": "Define Query",
        "id": "define-query"
      },
      {
        "text": "Invoke Query",
        "id": "invoke-query"
      },
      {
        "text": "Implement Query Backend",
        "id": "implement-query-backend"
      },
      {
        "text": "More about Queries",
        "id": "more-about-queries"
      },
      {
        "text": "Mutate Data",
        "id": "mutate-data"
      },
      {
        "text": "Define Mutators",
        "id": "define-mutators"
      },
      {
        "text": "Invoke Mutators",
        "id": "invoke-mutators"
      },
      {
        "text": "Implement Mutate Endpoint",
        "id": "implement-mutate-endpoint"
      },
      {
        "text": "More about Mutators",
        "id": "more-about-mutators"
      },
      {
        "text": "That's It!",
        "id": "thats-it"
      }
    ],
    "kind": "page"
  },
  {
    "id": "152-install#integrate-zero",
    "title": "Install Zero",
    "searchTitle": "Integrate Zero",
    "sectionTitle": "Integrate Zero",
    "sectionId": "integrate-zero",
    "url": "/docs/install",
    "content": "Set Up Your Database You'll need a local Postgres database for development. If you don't have a preferred method, we recommend using Docker: docker run -d --name zero-postgres \\ -e POSTGRES_PASSWORD=\"password\" \\ -p 5432:5432 \\ postgres:16-alpine \\ # IMPORTANT: logical WAL level is required for Zero # to sync data to its SQLite replica postgres -c wal_level=logical This will start a Postgres database running in the background. See Connecting to Postgres for more details on what Postgres features are required for Zero to work. Install and Run Zero-Cache Add Zero to your project: npm install @rocicorp/zeropnpm add @rocicorp/zero # Note: pnpm disables postinstall scripts by default for security. # Either approve the build: pnpm approve-builds # Or add to package.json: # \"pnpm\": { # \"onlyBuiltDependencies\": [\"@rocicorp/zero-sqlite3\"] # }bun add @rocicorp/zero # Note: Bun disables postinstall scripts by default for security. # Add to package.json: # \"trustedDependencies\": [\"@rocicorp/zero-sqlite3\"]yarn add @rocicorp/zero Start the development zero-cache by running the following command: export ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" npx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" pnpm exec zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" bunx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" yarn exec zero-cache-dev Zero works by continuously replicating your upstream database into a SQLite replica. Zero-cache runs client queries against the replica. If there are tables or columns that will not be queried by Zero clients ever, you can exclude them. You can use the zero-sqlite3 tool to explore zero.db. Try it out by connecting to Postgres and the Zero replica in two different terminals. If you change something in Postgres, you'll see it immediately show up in the replica: Set Up Your Zero Schema Zero uses a file called schema.ts to provide a type-safe query API. If you use Drizzle or Prisma, you can generate schema.ts automatically. Otherwise, you can create it manually. npm install -D drizzle-zero npx drizzle-zero generatepnpm add -D drizzle-zero pnpm dlx drizzle-zero generatebun add -D drizzle-zero bunx drizzle-zero generateyarn add -D drizzle-zero yarn dlx drizzle-zero generatenpm install -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } npx prisma generatepnpm add -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } pnpx prisma generatebun add -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } bunx prisma generateyarn add -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } yarn prisma generate// zero/schema.ts import {table, string, createSchema} from '@rocicorp/zero' const user = table('user') .columns({ id: string(), name: string() // ... other columns ... }) .primaryKey('id') // ... more tables and relationships ... // See \"Schema\" page in left nav for complete schema API. export const schema = createSchema({ tables: [user] }) // Register the schema for type safety declare module '@rocicorp/zero' { interface DefaultTypes { schema: typeof schema } } Zero has some restrictions on the Postgres features it supports. You can continue this tutorial with a sample schema and seed data to evaluate it. Set Up the Zero Client Zero has first-class support for React and SolidJS, and community support for Svelte and Vue. There is also a low-level API you can use in any TypeScript-based project. // root.tsx import {ZeroProvider} from '@rocicorp/zero/react' import type {ZeroOptions} from '@rocicorp/zero' import {schema} from './zero/schema.ts' const opts: ZeroOptions = { userID: 'anon', cacheURL: 'http://localhost:4848', schema } function Root() { return ( <ZeroProvider {...opts}> <App /> </ZeroProvider> ) } // mycomponent.tsx import {useZero} from '@rocicorp/zero/react' function MyComponent() { const zero = useZero() console.log('clientID', zero.clientID) }// root.tsx import {ZeroProvider} from '@rocicorp/zero/solid' import type {ZeroOptions} from '@rocicorp/zero' import {schema} from './zero/schema.ts' const opts: ZeroOptions = { userID: 'anon', cacheURL: 'http://localhost:4848', schema } function Root() { return ( <ZeroProvider {...opts}> <App /> </ZeroProvider> ) } // mycomponent.tsx import {useZero} from '@rocicorp/zero/solid' function MyComponent() { const zero = useZero() console.log('clientID', zero().clientID) }// zero.ts import {Zero} from '@rocicorp/zero' import {schema} from './zero/schema.ts' const opts: ZeroOptions = { userID: 'anon', cacheURL: 'http://localhost:4848', schema } const zero = new Zero(opts) console.log('clientID', zero.clientID) export {zero}",
    "kind": "section"
  },
  {
    "id": "153-install#set-up-your-database",
    "title": "Install Zero",
    "searchTitle": "Set Up Your Database",
    "sectionTitle": "Set Up Your Database",
    "sectionId": "set-up-your-database",
    "url": "/docs/install",
    "content": "You'll need a local Postgres database for development. If you don't have a preferred method, we recommend using Docker: docker run -d --name zero-postgres \\ -e POSTGRES_PASSWORD=\"password\" \\ -p 5432:5432 \\ postgres:16-alpine \\ # IMPORTANT: logical WAL level is required for Zero # to sync data to its SQLite replica postgres -c wal_level=logical This will start a Postgres database running in the background. See Connecting to Postgres for more details on what Postgres features are required for Zero to work.",
    "kind": "section"
  },
  {
    "id": "154-install#install-and-run-zero-cache",
    "title": "Install Zero",
    "searchTitle": "Install and Run Zero-Cache",
    "sectionTitle": "Install and Run Zero-Cache",
    "sectionId": "install-and-run-zero-cache",
    "url": "/docs/install",
    "content": "Add Zero to your project: npm install @rocicorp/zeropnpm add @rocicorp/zero # Note: pnpm disables postinstall scripts by default for security. # Either approve the build: pnpm approve-builds # Or add to package.json: # \"pnpm\": { # \"onlyBuiltDependencies\": [\"@rocicorp/zero-sqlite3\"] # }bun add @rocicorp/zero # Note: Bun disables postinstall scripts by default for security. # Add to package.json: # \"trustedDependencies\": [\"@rocicorp/zero-sqlite3\"]yarn add @rocicorp/zero Start the development zero-cache by running the following command: export ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" npx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" pnpm exec zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" bunx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" yarn exec zero-cache-dev Zero works by continuously replicating your upstream database into a SQLite replica. Zero-cache runs client queries against the replica. If there are tables or columns that will not be queried by Zero clients ever, you can exclude them. You can use the zero-sqlite3 tool to explore zero.db. Try it out by connecting to Postgres and the Zero replica in two different terminals. If you change something in Postgres, you'll see it immediately show up in the replica:",
    "kind": "section"
  },
  {
    "id": "155-install#set-up-your-zero-schema",
    "title": "Install Zero",
    "searchTitle": "Set Up Your Zero Schema",
    "sectionTitle": "Set Up Your Zero Schema",
    "sectionId": "set-up-your-zero-schema",
    "url": "/docs/install",
    "content": "Zero uses a file called schema.ts to provide a type-safe query API. If you use Drizzle or Prisma, you can generate schema.ts automatically. Otherwise, you can create it manually. npm install -D drizzle-zero npx drizzle-zero generatepnpm add -D drizzle-zero pnpm dlx drizzle-zero generatebun add -D drizzle-zero bunx drizzle-zero generateyarn add -D drizzle-zero yarn dlx drizzle-zero generatenpm install -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } npx prisma generatepnpm add -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } pnpx prisma generatebun add -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } bunx prisma generateyarn add -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } yarn prisma generate// zero/schema.ts import {table, string, createSchema} from '@rocicorp/zero' const user = table('user') .columns({ id: string(), name: string() // ... other columns ... }) .primaryKey('id') // ... more tables and relationships ... // See \"Schema\" page in left nav for complete schema API. export const schema = createSchema({ tables: [user] }) // Register the schema for type safety declare module '@rocicorp/zero' { interface DefaultTypes { schema: typeof schema } } Zero has some restrictions on the Postgres features it supports. You can continue this tutorial with a sample schema and seed data to evaluate it.",
    "kind": "section"
  },
  {
    "id": "156-install#set-up-the-zero-client",
    "title": "Install Zero",
    "searchTitle": "Set Up the Zero Client",
    "sectionTitle": "Set Up the Zero Client",
    "sectionId": "set-up-the-zero-client",
    "url": "/docs/install",
    "content": "Zero has first-class support for React and SolidJS, and community support for Svelte and Vue. There is also a low-level API you can use in any TypeScript-based project. // root.tsx import {ZeroProvider} from '@rocicorp/zero/react' import type {ZeroOptions} from '@rocicorp/zero' import {schema} from './zero/schema.ts' const opts: ZeroOptions = { userID: 'anon', cacheURL: 'http://localhost:4848', schema } function Root() { return ( <ZeroProvider {...opts}> <App /> </ZeroProvider> ) } // mycomponent.tsx import {useZero} from '@rocicorp/zero/react' function MyComponent() { const zero = useZero() console.log('clientID', zero.clientID) }// root.tsx import {ZeroProvider} from '@rocicorp/zero/solid' import type {ZeroOptions} from '@rocicorp/zero' import {schema} from './zero/schema.ts' const opts: ZeroOptions = { userID: 'anon', cacheURL: 'http://localhost:4848', schema } function Root() { return ( <ZeroProvider {...opts}> <App /> </ZeroProvider> ) } // mycomponent.tsx import {useZero} from '@rocicorp/zero/solid' function MyComponent() { const zero = useZero() console.log('clientID', zero().clientID) }// zero.ts import {Zero} from '@rocicorp/zero' import {schema} from './zero/schema.ts' const opts: ZeroOptions = { userID: 'anon', cacheURL: 'http://localhost:4848', schema } const zero = new Zero(opts) console.log('clientID', zero.clientID) export {zero}",
    "kind": "section"
  },
  {
    "id": "157-install#sync-data",
    "title": "Install Zero",
    "searchTitle": "Sync Data",
    "sectionTitle": "Sync Data",
    "sectionId": "sync-data",
    "url": "/docs/install",
    "content": "Define Query Alright, let's sync some data! In Zero, we do this with queries. Queries are conventionally found in a queries.ts file. Here is an example of how queries are defined - you can adapt this to your own schema: // zero/queries.ts import {defineQueries, defineQuery} from '@rocicorp/zero' import {z} from 'zod' import {zql} from './schema.ts' export const queries = defineQueries({ albums: { byArtist: defineQuery( z.object({artistID: z.string()}), ({args: {artistID}}) => zql.albums .where('artistId', artistID) .orderBy('createdAt', 'asc') .limit(10) .related('artist', q => q.one()) ) } }) Use zql from schema.ts to construct and return a ZQL query. ZQL is quite powerful and allows you to build queries with filters, sorts, relationships, and more: See queries for more information on defining queries. Invoke Query Querying for data is framework-specific. Most of the time, you will use a helper like useQuery that integrates into your framework's rendering model: // mycomponent.tsx import {useQuery} from '@rocicorp/zero/react' import {queries} from './zero/queries.ts' function MyComponent() { const [albums] = useQuery( queries.albums.byArtist({artistID: 'artist_1'}) ) return albums.map(a => <div key={a.id}>{a.title}</div>) }// mycomponent.tsx import {useQuery} from '@rocicorp/zero/solid' import {queries} from './zero/queries.ts' function MyComponent() { const [albums] = useQuery(() => queries.albums.byArtist({artistID: 'artist_1'}) ) return ( <For each={albums()}> {album => <div key={album.id}>{album.title}</div>} </For> ) }// albums.ts import {zero} from './zero.ts' import {queries} from './zero/queries.ts' const albums = await zero.run( queries.albums.byArtist({artistID: 'artist_1'}) ) console.log('albums', albums) When you reload your app, you should see an error like: > npx zero-cache-dev ... no ZERO_QUERY_URL is configured for Zero Cache> pnpm exec zero-cache-dev ... no ZERO_QUERY_URL is configured for Zero Cache> bunx zero-cache-dev ... no ZERO_QUERY_URL is configured for Zero Cache> yarn exec zero-cache-dev ... no ZERO_QUERY_URL is configured for Zero Cache This is expected. We now need to implement a queries endpoint so that zero-cache can get the ZQL for the albums.byArtist query. Implement Query Backend Zero doesn't allow clients to run any arbitrary ZQL against zero-cache, for both security and performance reasons. Instead, Zero sends the name and arguments of the query to a queries endpoint on your server that is responsible for transforming the named query into ZQL. Zero provides utilities to make it easy to implement the queries endpoint in any full-stack framework: // src/routes/api/query.ts import {createFileRoute} from '@tanstack/react-router' import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from '../../zero/queries.ts' import {schema} from '../../zero/schema.ts' export const Route = createFileRoute('/api/query')({ server: { handlers: { POST: async ({request}) => { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, request ) return Response.json(result) } } } })// app/api/query/route.ts import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from '../../zero/queries.ts' import {schema} from '../../zero/schema.ts' export async function POST(req: Request) { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, req ) return Response.json(result) }// src/routes/api/query.ts import type {APIEvent} from '@solidjs/start/server' import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from '../../zero/queries.ts' import {schema} from '../../zero/schema.ts' export async function POST(event: APIEvent) { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, event.request ) return new Response.json(result) }// api/app.ts import {Hono} from 'hono' import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from '../zero/queries.ts' import {schema} from '../zero/schema.ts' const app = new Hono() app.post('/api/query', async c => { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, c.req.raw ) return c.json(result) }) Stop and re-run zero-cache with the URL of the queries endpoint: export ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" npx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" pnpm exec zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" bunx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" yarn exec zero-cache-dev If you reload the page, you will see data! Zero queries update live, so if you edit data in Postgres directly, you will see it update in the Zero replica AND the UI: More about Queries You now know the basics, but there are a few more important pieces you'll need to learn for your first real app: How authentication and permissions work. Preloading queries to create instantly responsive UI. For these details and more, see Reading Data. But for now, let's move on to writes!",
    "kind": "section"
  },
  {
    "id": "158-install#define-query",
    "title": "Install Zero",
    "searchTitle": "Define Query",
    "sectionTitle": "Define Query",
    "sectionId": "define-query",
    "url": "/docs/install",
    "content": "Alright, let's sync some data! In Zero, we do this with queries. Queries are conventionally found in a queries.ts file. Here is an example of how queries are defined - you can adapt this to your own schema: // zero/queries.ts import {defineQueries, defineQuery} from '@rocicorp/zero' import {z} from 'zod' import {zql} from './schema.ts' export const queries = defineQueries({ albums: { byArtist: defineQuery( z.object({artistID: z.string()}), ({args: {artistID}}) => zql.albums .where('artistId', artistID) .orderBy('createdAt', 'asc') .limit(10) .related('artist', q => q.one()) ) } }) Use zql from schema.ts to construct and return a ZQL query. ZQL is quite powerful and allows you to build queries with filters, sorts, relationships, and more: See queries for more information on defining queries.",
    "kind": "section"
  },
  {
    "id": "159-install#invoke-query",
    "title": "Install Zero",
    "searchTitle": "Invoke Query",
    "sectionTitle": "Invoke Query",
    "sectionId": "invoke-query",
    "url": "/docs/install",
    "content": "Querying for data is framework-specific. Most of the time, you will use a helper like useQuery that integrates into your framework's rendering model: // mycomponent.tsx import {useQuery} from '@rocicorp/zero/react' import {queries} from './zero/queries.ts' function MyComponent() { const [albums] = useQuery( queries.albums.byArtist({artistID: 'artist_1'}) ) return albums.map(a => <div key={a.id}>{a.title}</div>) }// mycomponent.tsx import {useQuery} from '@rocicorp/zero/solid' import {queries} from './zero/queries.ts' function MyComponent() { const [albums] = useQuery(() => queries.albums.byArtist({artistID: 'artist_1'}) ) return ( <For each={albums()}> {album => <div key={album.id}>{album.title}</div>} </For> ) }// albums.ts import {zero} from './zero.ts' import {queries} from './zero/queries.ts' const albums = await zero.run( queries.albums.byArtist({artistID: 'artist_1'}) ) console.log('albums', albums) When you reload your app, you should see an error like: > npx zero-cache-dev ... no ZERO_QUERY_URL is configured for Zero Cache> pnpm exec zero-cache-dev ... no ZERO_QUERY_URL is configured for Zero Cache> bunx zero-cache-dev ... no ZERO_QUERY_URL is configured for Zero Cache> yarn exec zero-cache-dev ... no ZERO_QUERY_URL is configured for Zero Cache This is expected. We now need to implement a queries endpoint so that zero-cache can get the ZQL for the albums.byArtist query.",
    "kind": "section"
  },
  {
    "id": "160-install#implement-query-backend",
    "title": "Install Zero",
    "searchTitle": "Implement Query Backend",
    "sectionTitle": "Implement Query Backend",
    "sectionId": "implement-query-backend",
    "url": "/docs/install",
    "content": "Zero doesn't allow clients to run any arbitrary ZQL against zero-cache, for both security and performance reasons. Instead, Zero sends the name and arguments of the query to a queries endpoint on your server that is responsible for transforming the named query into ZQL. Zero provides utilities to make it easy to implement the queries endpoint in any full-stack framework: // src/routes/api/query.ts import {createFileRoute} from '@tanstack/react-router' import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from '../../zero/queries.ts' import {schema} from '../../zero/schema.ts' export const Route = createFileRoute('/api/query')({ server: { handlers: { POST: async ({request}) => { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, request ) return Response.json(result) } } } })// app/api/query/route.ts import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from '../../zero/queries.ts' import {schema} from '../../zero/schema.ts' export async function POST(req: Request) { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, req ) return Response.json(result) }// src/routes/api/query.ts import type {APIEvent} from '@solidjs/start/server' import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from '../../zero/queries.ts' import {schema} from '../../zero/schema.ts' export async function POST(event: APIEvent) { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, event.request ) return new Response.json(result) }// api/app.ts import {Hono} from 'hono' import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from '../zero/queries.ts' import {schema} from '../zero/schema.ts' const app = new Hono() app.post('/api/query', async c => { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, c.req.raw ) return c.json(result) }) Stop and re-run zero-cache with the URL of the queries endpoint: export ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" npx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" pnpm exec zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" bunx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" yarn exec zero-cache-dev If you reload the page, you will see data! Zero queries update live, so if you edit data in Postgres directly, you will see it update in the Zero replica AND the UI:",
    "kind": "section"
  },
  {
    "id": "161-install#more-about-queries",
    "title": "Install Zero",
    "searchTitle": "More about Queries",
    "sectionTitle": "More about Queries",
    "sectionId": "more-about-queries",
    "url": "/docs/install",
    "content": "You now know the basics, but there are a few more important pieces you'll need to learn for your first real app: How authentication and permissions work. Preloading queries to create instantly responsive UI. For these details and more, see Reading Data. But for now, let's move on to writes!",
    "kind": "section"
  },
  {
    "id": "162-install#mutate-data",
    "title": "Install Zero",
    "searchTitle": "Mutate Data",
    "sectionTitle": "Mutate Data",
    "sectionId": "mutate-data",
    "url": "/docs/install",
    "content": "Define Mutators Data is written in Zero apps using mutators. Similar to queries, we use a shared mutators.ts file: // zero/mutators.ts import {defineMutators, defineMutator} from '@rocicorp/zero' import {z} from 'zod' export const mutators = defineMutators({ albums: { create: defineMutator( z.object({ id: z.string(), artistID: z.string(), title: z.string(), year: z.number(), createdAt: z.number() }), async ({args, tx}) => { await tx.mutate.albums.insert({ id: args.id, artistId: args.artistID, title: args.title, releaseYear: args.year, createdAt: args.createdAt }) } ) } }) You can use the CRUD-style API with tx.mutate.<table>.<method>() to write data. You can also use tx.run(zql.<table>.<method>) to run queries within your mutator. Mutators almost always run in the same frame on the client, against local data. The reason mutators are marked async is because on the server, reading from the tx object goes over the network to Postgres. Also, in edge cases on the client, reads and writes can go to local storage (IndexedDB or SQLite). Once you've defined your mutators, you must register them with Zero before you can use them: import {mutators} from './zero/mutators.ts' const opts: ZeroOptions = { // ... userID, cacheURL, etc. // add mutators mutators } Invoke Mutators You can now call mutators via zero.mutate: // mycomponent.tsx import {useZero} from '@rocicorp/zero/react' import {mutators} from './zero/mutators.ts' import {nanoid} from 'nanoid' function MyComponent() { const zero = useZero() const onClick = async () => { const result = zero.mutate( mutators.albums.create({ id: nanoid(), artistID: 'artist_1', title: 'Please Please Me', year: 1963, createdAt: Date.now() }) ) const clientResult = await result.client if (clientResult.type === 'error') { console.error( 'Failed to create album', clientResult.error.message ) } else { console.log('Album created!') } } return <button onClick={onClick}>Create Album</button> }// mycomponent.tsx import {useZero} from '@rocicorp/zero/solid' import {mutators} from './zero/mutators.ts' import {nanoid} from 'nanoid' function MyComponent() { const zero = useZero() const onClick = async () => { const result = zero().mutate( mutators.albums.create({ id: nanoid(), artistID: 'artist_1', title: 'Please Please Me', year: 1963, createdAt: Date.now() }) ) const clientResult = await result.client if (clientResult.type === 'error') { console.error( 'Failed to create album', clientResult.error.message ) } else { console.log('Album created!') } } return <button onClick={onClick}>Create Album</button> }// albums.ts import {zero} from './zero.ts' import {mutators} from './zero/mutators.ts' import {nanoid} from 'nanoid' const result = await zero.mutate( mutators.albums.create({ id: nanoid(), artistID: 'artist_1', title: 'Please Please Me', year: 1963, createdAt: Date.now() }) ) const clientResult = await result.client if (clientResult.type === 'error') { console.error( 'Failed to create album', clientResult.error.message ) } else { console.log('Album created!') } Client-generated random IDs (from libraries like uuid, ulid, or nanoid) work much better than auto-incrementing integers in sync engines like Zero. Learn more. If you run this app now, you should be able to see the UI update optimistically, but you'll also see an error in zero-cache: > npx zero-cache-dev ... A ZERO_MUTATE_URL must be set in order to process mutations> pnpm exec zero-cache-dev ... A ZERO_MUTATE_URL must be set in order to process mutations> bunx zero-cache-dev ... A ZERO_MUTATE_URL must be set in order to process mutations> yarn exec zero-cache-dev ... A ZERO_MUTATE_URL must be set in order to process mutations Similar to queries, we need to wire up a mutate endpoint in our API. Let's do that now. Implement Mutate Endpoint Zero requires a mutate endpoint which runs on your server and connects directly to your Postgres database. Zero provides helpers to implement this easily. Use the Zero Postgres adapters to create a dbProvider instance: // src/db-provider.ts import {zeroDrizzle} from '@rocicorp/zero/server/adapters/drizzle' import {drizzle} from 'drizzle-orm/node-postgres' import {Pool} from 'pg' import {schema} from '../../zero/schema.ts' import * as drizzleSchema from '../../drizzle/schema.ts' // pass a drizzle client instance. for example: const pool = new Pool({ connectionString: process.env.ZERO_UPSTREAM_DB! }) export const drizzleClient = drizzle(pool, { schema: drizzleSchema }) export const dbProvider = zeroDrizzle(schema, drizzleClient) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } }// src/db-provider.ts import {PrismaPg} from '@prisma/adapter-pg' import {PrismaClient} from '@prisma/client' import {zeroPrisma} from '@rocicorp/zero/server/adapters/prisma' import {schema} from '../../zero/schema.ts' // pass a prisma client instance. for example: const prisma = new PrismaClient({ adapter: new PrismaPg({ connectionString: process.env.ZERO_UPSTREAM_DB! }) }) export const dbProvider = zeroPrisma(schema, prisma) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } }// src/db-provider.ts import {zeroNodePg} from '@rocicorp/zero/server/adapters/pg' import {Pool} from 'pg' import {schema} from '../../zero/schema.ts' const pool = new Pool({ connectionString: process.env.ZERO_UPSTREAM_DB! }) export const dbProvider = zeroNodePg(schema, pool) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } }// src/db-provider.ts import {zeroPostgresJS} from '@rocicorp/zero/server/adapters/postgresjs' import postgres from 'postgres' import {schema} from '../../zero/schema.ts' const sql = postgres(process.env.ZERO_UPSTREAM_DB!) export const dbProvider = zeroPostgresJS(schema, sql) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } } Then, use the dbProvider to handle the mutate request: // src/routes/api/mutate.ts import {createFileRoute} from '@tanstack/react-router' import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from '../../zero/mutators.ts' import {dbProvider} from '../../db-provider.ts' export const Route = createFileRoute('/api/mutate')({ server: { handlers: { POST: async ({request}) => { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({ args, tx, ctx: {userId: 'anon'} }) }), request ) return Response.json(result) } } } })// app/api/mutate/route.ts import {handleMutateRequest} from '@rocicorp/zero/server' import {mutators} from '../../zero/mutators.ts' import {dbProvider} from '../../db-provider.ts' import {mustGetMutator} from '@rocicorp/zero' export async function POST(req: Request) { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({args, tx, ctx: {userId: 'anon'}}) }), req ) return Response.json(result) }// src/routes/api/mutate.ts import type {APIEvent} from '@solidjs/start/server' import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from '../../zero/mutators.ts' import {dbProvider} from '../../db-provider.ts' export async function POST(event: APIEvent) { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({args, tx, ctx: {userId: 'anon'}}) }), event.request ) return new Response.json(result) }// api/app.ts import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from '../zero/mutators.ts' import {dbProvider} from './db-provider.ts' app.post('/api/mutate', async c => { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({args, tx, ctx: {userId: 'anon'}}) }), c.req.raw ) return c.json(result) }) Restart zero-cache to tell it about this new endpoint: export ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" export ZERO_MUTATE_URL=\"http://localhost:3000/api/mutate\" npx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" export ZERO_MUTATE_URL=\"http://localhost:3000/api/mutate\" pnpm exec zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" export ZERO_MUTATE_URL=\"http://localhost:3000/api/mutate\" bunx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" export ZERO_MUTATE_URL=\"http://localhost:3000/api/mutate\" yarn exec zero-cache-dev If you refresh the page, your mutation should commit to the database and sync to other clients: More about Mutators Just as with queries, the separate server implementation of mutators extends elegantly to enable write permissions. Zero also has built-in helpers to do work after a mutator runs on the server, like send notifications. For these details and more, see Writing Data.",
    "kind": "section"
  },
  {
    "id": "163-install#define-mutators",
    "title": "Install Zero",
    "searchTitle": "Define Mutators",
    "sectionTitle": "Define Mutators",
    "sectionId": "define-mutators",
    "url": "/docs/install",
    "content": "Data is written in Zero apps using mutators. Similar to queries, we use a shared mutators.ts file: // zero/mutators.ts import {defineMutators, defineMutator} from '@rocicorp/zero' import {z} from 'zod' export const mutators = defineMutators({ albums: { create: defineMutator( z.object({ id: z.string(), artistID: z.string(), title: z.string(), year: z.number(), createdAt: z.number() }), async ({args, tx}) => { await tx.mutate.albums.insert({ id: args.id, artistId: args.artistID, title: args.title, releaseYear: args.year, createdAt: args.createdAt }) } ) } }) You can use the CRUD-style API with tx.mutate.<table>.<method>() to write data. You can also use tx.run(zql.<table>.<method>) to run queries within your mutator. Mutators almost always run in the same frame on the client, against local data. The reason mutators are marked async is because on the server, reading from the tx object goes over the network to Postgres. Also, in edge cases on the client, reads and writes can go to local storage (IndexedDB or SQLite). Once you've defined your mutators, you must register them with Zero before you can use them: import {mutators} from './zero/mutators.ts' const opts: ZeroOptions = { // ... userID, cacheURL, etc. // add mutators mutators }",
    "kind": "section"
  },
  {
    "id": "164-install#invoke-mutators",
    "title": "Install Zero",
    "searchTitle": "Invoke Mutators",
    "sectionTitle": "Invoke Mutators",
    "sectionId": "invoke-mutators",
    "url": "/docs/install",
    "content": "You can now call mutators via zero.mutate: // mycomponent.tsx import {useZero} from '@rocicorp/zero/react' import {mutators} from './zero/mutators.ts' import {nanoid} from 'nanoid' function MyComponent() { const zero = useZero() const onClick = async () => { const result = zero.mutate( mutators.albums.create({ id: nanoid(), artistID: 'artist_1', title: 'Please Please Me', year: 1963, createdAt: Date.now() }) ) const clientResult = await result.client if (clientResult.type === 'error') { console.error( 'Failed to create album', clientResult.error.message ) } else { console.log('Album created!') } } return <button onClick={onClick}>Create Album</button> }// mycomponent.tsx import {useZero} from '@rocicorp/zero/solid' import {mutators} from './zero/mutators.ts' import {nanoid} from 'nanoid' function MyComponent() { const zero = useZero() const onClick = async () => { const result = zero().mutate( mutators.albums.create({ id: nanoid(), artistID: 'artist_1', title: 'Please Please Me', year: 1963, createdAt: Date.now() }) ) const clientResult = await result.client if (clientResult.type === 'error') { console.error( 'Failed to create album', clientResult.error.message ) } else { console.log('Album created!') } } return <button onClick={onClick}>Create Album</button> }// albums.ts import {zero} from './zero.ts' import {mutators} from './zero/mutators.ts' import {nanoid} from 'nanoid' const result = await zero.mutate( mutators.albums.create({ id: nanoid(), artistID: 'artist_1', title: 'Please Please Me', year: 1963, createdAt: Date.now() }) ) const clientResult = await result.client if (clientResult.type === 'error') { console.error( 'Failed to create album', clientResult.error.message ) } else { console.log('Album created!') } Client-generated random IDs (from libraries like uuid, ulid, or nanoid) work much better than auto-incrementing integers in sync engines like Zero. Learn more. If you run this app now, you should be able to see the UI update optimistically, but you'll also see an error in zero-cache: > npx zero-cache-dev ... A ZERO_MUTATE_URL must be set in order to process mutations> pnpm exec zero-cache-dev ... A ZERO_MUTATE_URL must be set in order to process mutations> bunx zero-cache-dev ... A ZERO_MUTATE_URL must be set in order to process mutations> yarn exec zero-cache-dev ... A ZERO_MUTATE_URL must be set in order to process mutations Similar to queries, we need to wire up a mutate endpoint in our API. Let's do that now.",
    "kind": "section"
  },
  {
    "id": "165-install#implement-mutate-endpoint",
    "title": "Install Zero",
    "searchTitle": "Implement Mutate Endpoint",
    "sectionTitle": "Implement Mutate Endpoint",
    "sectionId": "implement-mutate-endpoint",
    "url": "/docs/install",
    "content": "Zero requires a mutate endpoint which runs on your server and connects directly to your Postgres database. Zero provides helpers to implement this easily. Use the Zero Postgres adapters to create a dbProvider instance: // src/db-provider.ts import {zeroDrizzle} from '@rocicorp/zero/server/adapters/drizzle' import {drizzle} from 'drizzle-orm/node-postgres' import {Pool} from 'pg' import {schema} from '../../zero/schema.ts' import * as drizzleSchema from '../../drizzle/schema.ts' // pass a drizzle client instance. for example: const pool = new Pool({ connectionString: process.env.ZERO_UPSTREAM_DB! }) export const drizzleClient = drizzle(pool, { schema: drizzleSchema }) export const dbProvider = zeroDrizzle(schema, drizzleClient) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } }// src/db-provider.ts import {PrismaPg} from '@prisma/adapter-pg' import {PrismaClient} from '@prisma/client' import {zeroPrisma} from '@rocicorp/zero/server/adapters/prisma' import {schema} from '../../zero/schema.ts' // pass a prisma client instance. for example: const prisma = new PrismaClient({ adapter: new PrismaPg({ connectionString: process.env.ZERO_UPSTREAM_DB! }) }) export const dbProvider = zeroPrisma(schema, prisma) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } }// src/db-provider.ts import {zeroNodePg} from '@rocicorp/zero/server/adapters/pg' import {Pool} from 'pg' import {schema} from '../../zero/schema.ts' const pool = new Pool({ connectionString: process.env.ZERO_UPSTREAM_DB! }) export const dbProvider = zeroNodePg(schema, pool) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } }// src/db-provider.ts import {zeroPostgresJS} from '@rocicorp/zero/server/adapters/postgresjs' import postgres from 'postgres' import {schema} from '../../zero/schema.ts' const sql = postgres(process.env.ZERO_UPSTREAM_DB!) export const dbProvider = zeroPostgresJS(schema, sql) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } } Then, use the dbProvider to handle the mutate request: // src/routes/api/mutate.ts import {createFileRoute} from '@tanstack/react-router' import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from '../../zero/mutators.ts' import {dbProvider} from '../../db-provider.ts' export const Route = createFileRoute('/api/mutate')({ server: { handlers: { POST: async ({request}) => { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({ args, tx, ctx: {userId: 'anon'} }) }), request ) return Response.json(result) } } } })// app/api/mutate/route.ts import {handleMutateRequest} from '@rocicorp/zero/server' import {mutators} from '../../zero/mutators.ts' import {dbProvider} from '../../db-provider.ts' import {mustGetMutator} from '@rocicorp/zero' export async function POST(req: Request) { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({args, tx, ctx: {userId: 'anon'}}) }), req ) return Response.json(result) }// src/routes/api/mutate.ts import type {APIEvent} from '@solidjs/start/server' import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from '../../zero/mutators.ts' import {dbProvider} from '../../db-provider.ts' export async function POST(event: APIEvent) { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({args, tx, ctx: {userId: 'anon'}}) }), event.request ) return new Response.json(result) }// api/app.ts import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from '../zero/mutators.ts' import {dbProvider} from './db-provider.ts' app.post('/api/mutate', async c => { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({args, tx, ctx: {userId: 'anon'}}) }), c.req.raw ) return c.json(result) }) Restart zero-cache to tell it about this new endpoint: export ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" export ZERO_MUTATE_URL=\"http://localhost:3000/api/mutate\" npx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" export ZERO_MUTATE_URL=\"http://localhost:3000/api/mutate\" pnpm exec zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" export ZERO_MUTATE_URL=\"http://localhost:3000/api/mutate\" bunx zero-cache-devexport ZERO_UPSTREAM_DB=\"postgres://postgres:password@localhost:5432/postgres\" export ZERO_QUERY_URL=\"http://localhost:3000/api/query\" export ZERO_MUTATE_URL=\"http://localhost:3000/api/mutate\" yarn exec zero-cache-dev If you refresh the page, your mutation should commit to the database and sync to other clients:",
    "kind": "section"
  },
  {
    "id": "166-install#more-about-mutators",
    "title": "Install Zero",
    "searchTitle": "More about Mutators",
    "sectionTitle": "More about Mutators",
    "sectionId": "more-about-mutators",
    "url": "/docs/install",
    "content": "Just as with queries, the separate server implementation of mutators extends elegantly to enable write permissions. Zero also has built-in helpers to do work after a mutator runs on the server, like send notifications. For these details and more, see Writing Data.",
    "kind": "section"
  },
  {
    "id": "167-install#thats-it",
    "title": "Install Zero",
    "searchTitle": "That's It!",
    "sectionTitle": "That's It!",
    "sectionId": "thats-it",
    "url": "/docs/install",
    "content": "Congratulations! You now know the basics for building with Zero ü§Ø. Possible next steps: Learn about authentication and permissions See some samples of built-out Zero apps Learn how to deploy your app to production",
    "kind": "section"
  },
  {
    "id": "15-introduction",
    "title": "Welcome to Zero",
    "searchTitle": "Welcome to Zero",
    "url": "/docs/introduction",
    "content": "Zero enables instant web applications by syncing a subset of data to the client, before it is needed. Reads and writes happen directly against this local datastore, and are synced with the server continuously in the background. Unlike previous sync engines that sync entire tables to the client, or use static rules to control what syncs, you control what to sync in Zero by writing normal queries directly in your application code. This provides a huge amount of control over what data is synced and when. Zero caches query results in a normalized client-side datastore, and reuses that data automatically to answer future queries whenever possible. The result is that for typical applications, queries are almost always instantly resolved using local data. It feels like you have access to the entire backend database directly from the client in memory. Occasionally, when you do a more specific query, Zero falls back to the server. But this happens automatically without any extra work required. Ready to get started? Choose your adventure: Install Zero into an existing project Clone a minimal quickstart app Play with a full-featured sample",
    "headings": [
      {
        "text": "Ready to get started?",
        "id": "ready-to-get-started"
      }
    ],
    "kind": "page"
  },
  {
    "id": "168-introduction#ready-to-get-started",
    "title": "Welcome to Zero",
    "searchTitle": "Ready to get started?",
    "sectionTitle": "Ready to get started?",
    "sectionId": "ready-to-get-started",
    "url": "/docs/introduction",
    "content": "Choose your adventure: Install Zero into an existing project Clone a minimal quickstart app Play with a full-featured sample",
    "kind": "section"
  },
  {
    "id": "16-llms",
    "title": "Welcome, ü§ñ!",
    "searchTitle": "Welcome, ü§ñ!",
    "url": "/docs/llms",
    "content": "Are you an LLM? Do you like long walks through vector space and late-night tokenization? Or maybe you're a friend of an LLM, just trying to make life a little easier for the contextually challenged? Either way, you're in the right place! Stream on over to llms.txt for the text-only version of these docs.",
    "headings": [],
    "kind": "page"
  },
  {
    "id": "17-mutators",
    "title": "Mutators",
    "searchTitle": "Mutators",
    "url": "/docs/mutators",
    "content": "Mutators are how you write data with Zero. Here's a simple example: import {defineMutators, defineMutator} from '@rocicorp/zero' import {z} from 'zod' export const mutators = defineMutators({ updateIssue: defineMutator( z.object({ id: z.string(), title: z.string() }), async ({tx, args: {id, title}}) => { if (title.length > 100) { throw new Error(`Title is too long`) } await tx.mutate.issue.update({ id, title }) } ) }) Architecture A copy of each mutator exists on both the client and on your server: Often the implementations will be the same, and you can just share their code. This is easy with full-stack frameworks like TanStack Start or Next.js. But the implementations don't have to be the same, or even compute the same result. For example, the server can add extra checks to enforce permissions, or send notifications or interact with other systems. Life of a Mutation When a mutator is invoked, it initially runs on the client, against the client-side datastore. Any changes are immediately applied to open queries and the user sees the changes. In the background, Zero sends a mutation (a record of the mutator having run with certain arguments) to your server's push endpoint. Your push endpoint runs the push protocol, executing the server-side mutator in a transaction against your database and recording the fact that the mutation ran. The @rocicorp/zero package contains utilities to make it easy to implement this endpoint in TypeScript. The changes to the database are then replicated to zero-cache using logical replication. zero-cache calculates the updates to active queries and sends rows that have changed to each client. It also sends information about the mutations that have been applied to the database. Clients receive row updates and apply them to their local cache. Any pending mutations which have been applied to the server have their local effects rolled back. Client-side queries are updated and the user sees the changes. Defining Mutators Basics Create a mutator using defineMutator. The only required argument is a MutatorFn, which must be async: import {defineMutator} from '@rocicorp/zero' const myMutator = defineMutator(async () => { // ... }) Mutators almost always complete in the same frame on the client, within milliseconds. The reason they are marked async is because on the server, reading from the tx object goes over the network to Postgres. Writing Data The MutatorFn receives a tx parameter which can be used to write data with a CRUD-style API. Each table in your Zero schema has a corresponding field on tx.mutate: const myMutator = defineMutator(async ({tx}) => { // This is here because there's a `user` table in your schema. await tx.mutate.user.insert(...) }) Failing to do so allows the transaction to commit early, causing runtime errors when writes are attempted later. Insert Create new records with insert: tx.mutate.user.insert({ id: 'user-123', username: 'sam', language: 'js' }) Optional fields can be set to null to explicitly set the new field to null. They can also be set to undefined to take the default value (which is often null but can also be some generated value server-side): // Sets language to `null` specifically tx.mutate.user.insert({ id: 'user-123', username: 'sam', language: null }) // Sets language to the default server-side value. // Could be null, or some generated or constant default value too. tx.mutate.user.insert({ id: 'user-123', username: 'sam' }) // Same as above tx.mutate.user.insert({ id: 'user-123', username: 'sam', language: undefined }) Upsert Create new records or update existing ones with upsert: tx.mutate.user.upsert({ id: samID, username: 'sam', language: 'ts' }) upsert supports the same null / undefined semantics for optional fields that insert does (see above). Update Update an existing record. Does nothing if the specified record (by PK) does not exist. You can pass a partial object, leaving fields out that you don‚Äôt want to change. For example here we leave the username the same: // Leaves username field to previous value. tx.mutate.user.update({ id: samID, language: 'golang' }) // Same as above tx.mutate.user.update({ id: samID, username: undefined, language: 'haskell' }) // Reset language field to `null` tx.mutate.user.update({ id: samID, language: null }) Delete Delete an existing record. Does nothing if specified record does not exist. tx.mutate.user.delete({ id: samID }) Arguments The MutatorFn can take a single args parameter. To enable this, pass a validator to defineMutator: import {defineMutator} from '@rocicorp/zero' const initStats = defineMutator( z.object({issueCount: z.number()}), async ({ tx, args: {issueCount} }) => { if (issueCount < 0) { throw new Error(`issueCount cannot be negative`) } await tx.mutate.stats.insert({ id: 'global', issueCount }) } ) We use Zod in these examples, but you can use any validation library that implements Standard Schema. It's most common for mutators to be a pure function of the database state plus arguments. But it's not required. Impure mutators can be useful, e.g., to consult some external system on the server for authorization or validation. Reading Data You can read data within a mutator by passing ZQL to tx.run: const updateIssue = defineMutator( z.object({id: z.string(), title: z.string()}), async ({tx, args: {id, title}}) => { const issue = await tx.run( zql.issue.where('id', id).one() ) if (issue?.status === 'closed') { throw new Error(`Cannot update closed issue`) } await tx.mutate.issue.update({ id, title }) } ) You have the full power of ZQL at your disposal, including relationships, filters, ordering, and limits. Reads and writes within a mutator are transactional, meaning that the datastore is guaranteed to not change while your mutator is running. And if the mutator throws, the entire mutation is rolled back. Unlike zero.run(), there is no type parameter that can be used to wait for server results inside mutators. This is because waiting for server results in mutators makes no sense ‚Äì it would defeat the purpose of running optimistically to begin with. When a mutator runs on the client (tx.location === \"client\"), ZQL reads only return data already cached on the client. When mutators run on the server (tx.location === \"server\"), ZQL reads always return all data. Context Mutator parameters are supplied by the client application and passed to the server automatically by Zero. This makes them unsuitable for credentials, since the user could modify them. For this reason, Zero mutators also support the concept of a context object. Access your context with the ctx parameter to your mutator: const createIssue = defineMutator( z.object({id: z.string(), title: z.string()}), async ({tx, ctx: {userID}, args: {id, title}}) => { // Note: User cannot control ctx.userID, so this // enforces authorship of created issue. await tx.mutate.issue.insert({ id, title, authorID: userID }) } ) Mutator Registries The result of defineMutator is a MutatorDefinition. By itself this isn't super useful. You need to register it using defineMutators: export const mutators = defineMutators({ issue: { update: updateIssue } }) Typically these are done together in one step: export const mutators = defineMutators({ issue: { update: defineMutator( z.object({id: z.string(), title: z.string()}), async ({tx, args: {id, title}}) => { await tx.mutate.issue.update({ id, title }) } ) } }) The result of defineMutators is called a MutatorRegistry. Each field in the registry is a callable Mutator that you can use to perform mutations: import {mutators} from 'mutators.ts' zero.mutate( mutators.issue.update({ id: 'issue-123', title: 'New title' }) ) Mutator Names Each Mutator has a mutatorName which is computed by defineMutators. When you run a mutator, Zero sends this name along with the arguments to your server to execute the server-side mutation. console.log(mutators.issue.update.mutatorName) // \"issue.update\" mutators.ts By convention, mutators are listed in a central mutators.ts file. This allows them to be easily used on both the client and server: import {defineMutators, defineMutator} from '@rocicorp/zero' import {zql} from './schema.ts' import {z} from 'zod' export const mutators = defineMutators({ posts: { create: defineMutator( z.object({ id: z.string(), title: z.string() }), async ({ tx, context: {userID}, args: {id, title} }) => { await tx.mutate.post.insert({ id, title, authorID: userID }) } ), update: defineMutator( z.object({ id: z.string(), title: z.string().optional() }), async ({ tx, context: {userID}, args: {id, title} }) => { const prev = await tx.run( zql.post.where('id', id).one() ) if (prev?.authorID !== userID) { throw new Error(`Access denied`) } await tx.mutate.post.update({ id, title, authorID: userID }) } ) } }) You can use as many levels of nesting as you want to organize your mutators. As your application grows, you can move mutators to different files to keep them organized: // posts.ts export const postMutators = { create: defineMutator( z.object({ id: z.string(), title: z.string(), }), async ({tx, context: {userID}, args: {id, title}}) => { await tx.mutate.post.insert({ id, title, authorID: userID, }) }, ), } // user.ts export const userMutators = { updateRole: defineMutator( z.object({ role: z.string(), }), async ({tx, ctx: {userID}, args: {role}}) => { await tx.mutate.user.update({ id: userID, role, }) }, ), } // mutators.ts import {postMutators} from 'zero/mutators/posts.ts' import {userMutators} from 'zero/mutators/users.ts' export const mutators = defineMutators{{ posts: postMutators, users: userMutators, }) defineMutators establishes the full name for each mutator (i.e., posts.create, users.updateRole), which is later sent to the server. So this should only be used once at the top level of your mutators.ts file. Registration Before you can use your mutators, you need to register them with Zero: import {ZeroProvider} from '@rocicorp/zero/react' import type {ZeroOptions} from '@rocicorp/zero' import {mutators} from 'zero/mutators.ts' const opts: ZeroOptions = { // ... cacheURL, schema, etc. mutators } return ( <ZeroProvider {...opts}> <App /> </ZeroProvider> )import {ZeroProvider} from '@rocicorp/zero/solid' import type {ZeroOptions} from '@rocicorp/zero' import {mutators} from 'zero/mutators.ts' const opts: ZeroOptions = { // ... cacheURL, schema, etc. mutators } return ( <ZeroProvider {...opts}> <App /> </ZeroProvider> )import {Zero} from '@rocicorp/zero' import type {ZeroOptions} from '@rocicorp/zero' import {mutators} from 'zero/mutators.ts' const opts: ZeroOptions = { // ... cacheURL, schema, etc. mutators } const zero = new Zero(opts) Mutators need to be registered with Zero because Zero calls them during sync for conflict resolution. If you invoke a mutator that is not registered, Zero will throw an error. Server Setup In order for mutations to sync, you must provide an implementation of the mutate endpoint on your server. zero-cache calls this endpoint to process each mutation. Registering the Endpoint Use ZERO_MUTATE_URL to tell zero-cache where to find your mutate implementation: export ZERO_MUTATE_URL=\"http://localhost:3000/api/zero/mutate\" # run zero-cache, e.g. `npx zero-cache-dev` Implementing the Endpoint You can use the handleMutateRequest and mustGetMutator functions to implement the endpoint. Plug in whatever dbProvider you set up (see server-zql or the install guide). // src/routes/api/zero/mutate.ts import {createFileRoute} from '@tanstack/react-router' import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from 'mutators.ts' import {dbProvider} from 'db-provider.ts' export const Route = createFileRoute('/api/zero/mutate')({ server: { handlers: { POST: async ({request}) => { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({ args, tx, ctx: {userId: 'anon'} }) }), request ) return Response.json(result) } } } })// app/api/zero/mutate/route.ts import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from 'mutators.ts' import {dbProvider} from 'db-provider.ts' export async function POST(req: Request) { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({args, tx, ctx: {userId: 'anon'}}) }), req ) return Response.json(result) }// src/routes/api/zero/mutate.ts import type {APIEvent} from '@solidjs/start/server' import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from 'mutators.ts' import {dbProvider} from 'db-provider.ts' export async function POST(event: APIEvent) { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({args, tx, ctx: {userId: 'anon'}}) }), event.request ) return new Response.json(result) }// api/app.ts import {Hono} from 'hono' import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from 'mutators.ts' import {dbProvider} from './db-provider.ts' const app = new Hono() app.post('/api/zero/mutate', async c => { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({ args, tx, ctx: {userId: 'anon'} }) }), c.req.raw ) return c.json(result) }) Zero includes several built-in database adapters. You can also easily create your own. See ZQL on the Server for more information. handleMutateRequest accepts a standard Request and returns a JSON object which can be serialized and returned by your server framework of choice. mustGetMutator looks up the mutator in the registry and throws an error if not found. The mutator.fn function is your mutator implementation wrapped in the validator you provided. Handling Errors The handleMutateRequest function skips any mutations that throw: const result = await handleMutateRequest( dbProvider, transact => transact(async (tx, name, args) => { // The mutation is skipped and the next mutation runs as normal. // The optimistic mutation on the client will be reverted. throw new Error('bonk') }), c.req.raw ) handleMutateRequest catches such errors and turns them into a structured response that gets sent back to the client. You can recover the errors and show UI if you want. It is also of course possible for the entire push endpoint to return an HTTP error, or to not reply at all: export const Route = createFileRoute('/api/zero/mutate')({ server: { handlers: { POST: async () => { throw new Error('zonk') // will trigger resend } } } })export async function POST() { throw new Error('zonk') // will trigger resend }export async function POST() { throw new Error('zonk') // will trigger resend }app.post('/api/zero/mutate', async c => { // This will cause the client to resend all queued mutations. throw new Error('zonk') }) If Zero receives any response from the mutate endpoint other than HTTP 200, 401, or 403, it will disconnect and enter the error state. If Zero receives HTTP 401 or 403, the client will enter the needs auth state and require a manual reconnect with zero.connection.connect(), then it will retry all queued mutations. If you want a different behavior, it is possible to implement the mutate endpoint yourself and handle errors differently. Custom Mutate URL By default, Zero sends mutations to the URL specified in the ZERO_MUTATE_URL parameter. However you can customize this on a per-client basis. To do so, list multiple comma-separated URLs in the ZERO_MUTATE_URL parameter: export ZERO_MUTATE_URL=\"https://api.example.com/mutate,https://api.staging.example.com/mutate\" Then choose one of those URLs by passing it to mutateURL on the Zero constructor: const opts: ZeroOptions = { // ... mutateURL: 'https://api.staging.example.com/mutate' } URL Patterns The strings listed in ZERO_MUTATE_URL can also be URLPatterns: export ZERO_MUTATE_URL=\"https://mybranch-*.preview.myapp.com/mutate\" For more information, see the URLPattern section of the Queries docs. It works the same way for mutations. Server-Specific Code To implement server-specific code, just run different mutators in your mutate endpoint. Server authority to the rescue! defineMutators accepts a baseMutators parameter that makes this easy. The returned mutator registry will contain all the mutators from baseMutators, plus any new ones you define or override: // server-mutators.ts import {defineMutators, defineMutator} from '@rocicorp/zero' import {z} from 'zod' import {zql} from 'schema.ts' import {mutators as sharedMutators} from 'mutators.ts' export const serverMutators = defineMutators( sharedMutators, { posts: { // Overrides the shared mutator definition with same name. update: defineMutator( z.object({ id: z.string(), title: z.string().optional(), priority: z.number().optional() }), async ({ tx, ctx: {userID}, args: {id, title, priority} }) => { // Run the shared mutator first. await sharedMutators.posts.update.fn({ tx, ctx, args }) // Record a history of this operation happening in an audit log table. await tx.mutate.auditLog.insert({ issueId: id, action: 'update-title', timestamp: Date.getTime() }) } ) } } ) For simple things, we also expose a location field on the transaction object that you can use to branch your code: const myMutator = defineMutator(async ({tx}) => { if (tx.location === 'client') { // Client-side code } else { // Server-side code } }) Running Mutators Once you have registered your mutators, you can invoke them with zero.mutate: import {mutators} from 'mutators.ts' import {nanoid} from 'nanoid' zero.mutate( mutators.issue.update({ id: nanoid(), title: 'New title' }) ) Client-generated random IDs from libraries like uuid, ulid, or nanoid work much better with sync engines like Zero. See IDs for more details. Waiting for Results We typically recommend that you \"fire and forget\" mutators. Optimistic mutations make sense when the common case is that a mutation succeeds. If a mutation frequently fails, then showing the user an optimistic result isn't very useful, because it will likely be wrong. That said there are cases where it is nice to know when a write succeeded on either the client or server. One example is if you need to read a row directly after writing it. Zero's local writes are very fast (almost always < 1 frame), but because Zero is backed by IndexedDB, writes are still technically asynchronous and reads directly after a write may not return the new data. You can use the .client promise in this case to wait for a write to complete on the client side: const write = zero.mutate( mutators.issue.insert({ id: nanoid(), title: 'New title' }) ) // issue-123 not guaranteed to be present here. read1 may be undefined. const read1 = await zero.run( queries.issue.byId('issue-123').one() ) // Await client write ‚Äì almost always less than 1 frame, and same // macrotask, so no browser paint will occur here. const res = await write.client if (res.type === 'error') { console.error('Mutator failed on client', res.error) } // issue-123 definitely can be read now. const read2 = await zero.run( queries.issue.byId('issue-123').one() ) You can also wait for the server write to succeed: const write = zero.mutate( mutators.issue.insert({ id: nanoid(), title: 'New title' }) ) const clientRes = await write.client if (clientRes.type === 'error') { throw new Error( `Mutator failed on client`, clientRes.error ) } // optimistic write guaranteed to be present here, but not // server write. const read1 = await zero.run( queries.issue.byId('issue-123').one() ) // Await server write ‚Äì this involves a round-trip. const serverRes = await write.server if (serverRes.type === 'error') { throw new Error( `Mutator failed on server`, serverRes.error ) } // issue-123 is written to server and any results are // synced to this client. // read2 could potentially be undefined here, for example if the // server mutator rejected the write. const read2 = await zero.run( queries.issue.byId('issue-123').one() ) If the client-side mutator fails, the .server promise is also rejected with the same error. You don't have to listen to both promises, the server promise covers both cases. There is not yet a way to return data from mutators in the success case. Let us know if you need this. Permissions Because mutators are just normal TypeScript functions that run server-side, there is no need for a special permissions system. You can implement whatever permission checks you want using plain TypeScript code. See Permissions for more information. Dropping Down to Raw SQL The ServerTransaction interface has a dbTransaction property that exposes the underlying database connection. This allows you to run raw SQL queries directly against the database. This is useful for complex queries, or for using Postgres features that Zero doesn't support yet: const markAllAsRead = defineMutator( z.object({ userId: z.string() }), async ({tx, args: {userId}}) => { // shared stuff ... if (tx.location === 'server') { // `tx` is now narrowed to `ServerTransaction`. // Do special server-only stuff with raw SQL. await tx.dbTransaction.query( ` UPDATE notification SET read = true WHERE user_id = $1 `, [userId] ) } } ) See ZQL on the Server for more information. Notifications and Async Work The best way to handle notifications and async work is a transactional outbox. This ensures that notifications actually do eventually get sent, without holding open database transactions to talk over the network. This can be implemented very easily in Zero by writing notifications to an outbox table as part of your mutator, then processing that table periodically with a background job. However sometimes it's still nice to do a quick and dirty async send as part of a mutation, for example early on in development, or to record metrics. For this, the createMutators pattern is useful: // server-mutators.ts import {defineMutator} from '@rocicorp/zero' import z from 'zod' import {zql} from 'schema.ts' import {mutators as clientMutators} from 'mutators.ts' // Instead of defining server mutators as a constant, // define them as a function of a list of async tasks. export function createMutators( asyncTasks: Array<() => Promise<void>> ) { return defineMutators(clientMutators, { issue: { update: defineMutator( z.object({ id: z.string(), title: z.string() }), async (tx, {id, title}) => { await tx.mutate.issue.update({id, title}) asyncTasks.push(() => sendEmailToSubscribers(id)) } ) } }) } Then in your mutate handler: export const Route = createFileRoute('/api/zero/mutate')({ server: { handlers: { POST: async ({request}) => { const asyncTasks: Array<() => Promise<void>> = [] const mutators = createMutators(asyncTasks) const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({ tx, ctx: {userId: 'anon'}, args }) }), request ) // Run all async tasks // If any fail, do not block the response, since the // mutation result has already been written to the database. await Promise.allSettled( asyncTasks.map(task => task()) ) return json(result) } } } })export async function POST(req: Request) { const asyncTasks: Array<() => Promise<void>> = [] const mutators = createMutators(asyncTasks) const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({tx, ctx: {userId: 'anon'}, args}) }), req ) // Run all async tasks // If any fail, do not block the response, since the // mutation result has already been written to the database. await Promise.allSettled(asyncTasks.map(task => task())) return Response.json(result) }export async function POST(event: APIEvent) { const asyncTasks: Array<() => Promise<void>> = [] const mutators = createMutators(asyncTasks) const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({tx, ctx: {userId: 'anon'}, args}) }), event.request ) // Run all async tasks // If any fail, do not block the response, since the // mutation result has already been written to the database. await Promise.allSettled(asyncTasks.map(task => task())) return new Response.json(result) }app.post('/api/zero/mutate', async c => { const asyncTasks: Array<() => Promise<void>> = [] const mutators = createMutators(asyncTasks) const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({ tx, ctx: {userId: 'anon'}, args }) }), c.req.raw ) // Run all async tasks // If any fail, do not block the response, since the // mutation result has already been written to the database. await Promise.allSettled(asyncTasks.map(task => task())) return c.json(result) }) Custom Mutate Implementation You can manually implement the mutate endpoint in any programming language. This will be documented in the future, but you can refer to the handleMutateRequest source code for an example for now.",
    "headings": [
      {
        "text": "Architecture",
        "id": "architecture"
      },
      {
        "text": "Life of a Mutation",
        "id": "life-of-a-mutation"
      },
      {
        "text": "Defining Mutators",
        "id": "defining-mutators"
      },
      {
        "text": "Basics",
        "id": "basics"
      },
      {
        "text": "Writing Data",
        "id": "writing-data"
      },
      {
        "text": "Insert",
        "id": "insert"
      },
      {
        "text": "Upsert",
        "id": "upsert"
      },
      {
        "text": "Update",
        "id": "update"
      },
      {
        "text": "Delete",
        "id": "delete"
      },
      {
        "text": "Arguments",
        "id": "arguments"
      },
      {
        "text": "Reading Data",
        "id": "reading-data"
      },
      {
        "text": "Context",
        "id": "context"
      },
      {
        "text": "Mutator Registries",
        "id": "mutator-registries"
      },
      {
        "text": "Mutator Names",
        "id": "mutator-names"
      },
      {
        "text": "mutators.ts",
        "id": "mutatorsts"
      },
      {
        "text": "Registration",
        "id": "registration"
      },
      {
        "text": "Server Setup",
        "id": "server-setup"
      },
      {
        "text": "Registering the Endpoint",
        "id": "registering-the-endpoint"
      },
      {
        "text": "Implementing the Endpoint",
        "id": "implementing-the-endpoint"
      },
      {
        "text": "Handling Errors",
        "id": "handling-errors"
      },
      {
        "text": "Custom Mutate URL",
        "id": "custom-mutate-url"
      },
      {
        "text": "URL Patterns",
        "id": "url-patterns"
      },
      {
        "text": "Server-Specific Code",
        "id": "server-specific-code"
      },
      {
        "text": "Running Mutators",
        "id": "running-mutators"
      },
      {
        "text": "Waiting for Results",
        "id": "waiting-for-results"
      },
      {
        "text": "Permissions",
        "id": "permissions"
      },
      {
        "text": "Dropping Down to Raw SQL",
        "id": "dropping-down-to-raw-sql"
      },
      {
        "text": "Notifications and Async Work",
        "id": "notifications-and-async-work"
      },
      {
        "text": "Custom Mutate Implementation",
        "id": "custom-mutate-implementation"
      }
    ],
    "kind": "page"
  },
  {
    "id": "169-mutators#architecture",
    "title": "Mutators",
    "searchTitle": "Architecture",
    "sectionTitle": "Architecture",
    "sectionId": "architecture",
    "url": "/docs/mutators",
    "content": "A copy of each mutator exists on both the client and on your server: Often the implementations will be the same, and you can just share their code. This is easy with full-stack frameworks like TanStack Start or Next.js. But the implementations don't have to be the same, or even compute the same result. For example, the server can add extra checks to enforce permissions, or send notifications or interact with other systems. Life of a Mutation When a mutator is invoked, it initially runs on the client, against the client-side datastore. Any changes are immediately applied to open queries and the user sees the changes. In the background, Zero sends a mutation (a record of the mutator having run with certain arguments) to your server's push endpoint. Your push endpoint runs the push protocol, executing the server-side mutator in a transaction against your database and recording the fact that the mutation ran. The @rocicorp/zero package contains utilities to make it easy to implement this endpoint in TypeScript. The changes to the database are then replicated to zero-cache using logical replication. zero-cache calculates the updates to active queries and sends rows that have changed to each client. It also sends information about the mutations that have been applied to the database. Clients receive row updates and apply them to their local cache. Any pending mutations which have been applied to the server have their local effects rolled back. Client-side queries are updated and the user sees the changes.",
    "kind": "section"
  },
  {
    "id": "170-mutators#life-of-a-mutation",
    "title": "Mutators",
    "searchTitle": "Life of a Mutation",
    "sectionTitle": "Life of a Mutation",
    "sectionId": "life-of-a-mutation",
    "url": "/docs/mutators",
    "content": "When a mutator is invoked, it initially runs on the client, against the client-side datastore. Any changes are immediately applied to open queries and the user sees the changes. In the background, Zero sends a mutation (a record of the mutator having run with certain arguments) to your server's push endpoint. Your push endpoint runs the push protocol, executing the server-side mutator in a transaction against your database and recording the fact that the mutation ran. The @rocicorp/zero package contains utilities to make it easy to implement this endpoint in TypeScript. The changes to the database are then replicated to zero-cache using logical replication. zero-cache calculates the updates to active queries and sends rows that have changed to each client. It also sends information about the mutations that have been applied to the database. Clients receive row updates and apply them to their local cache. Any pending mutations which have been applied to the server have their local effects rolled back. Client-side queries are updated and the user sees the changes.",
    "kind": "section"
  },
  {
    "id": "171-mutators#defining-mutators",
    "title": "Mutators",
    "searchTitle": "Defining Mutators",
    "sectionTitle": "Defining Mutators",
    "sectionId": "defining-mutators",
    "url": "/docs/mutators",
    "content": "Basics Create a mutator using defineMutator. The only required argument is a MutatorFn, which must be async: import {defineMutator} from '@rocicorp/zero' const myMutator = defineMutator(async () => { // ... }) Mutators almost always complete in the same frame on the client, within milliseconds. The reason they are marked async is because on the server, reading from the tx object goes over the network to Postgres. Writing Data The MutatorFn receives a tx parameter which can be used to write data with a CRUD-style API. Each table in your Zero schema has a corresponding field on tx.mutate: const myMutator = defineMutator(async ({tx}) => { // This is here because there's a `user` table in your schema. await tx.mutate.user.insert(...) }) Failing to do so allows the transaction to commit early, causing runtime errors when writes are attempted later. Insert Create new records with insert: tx.mutate.user.insert({ id: 'user-123', username: 'sam', language: 'js' }) Optional fields can be set to null to explicitly set the new field to null. They can also be set to undefined to take the default value (which is often null but can also be some generated value server-side): // Sets language to `null` specifically tx.mutate.user.insert({ id: 'user-123', username: 'sam', language: null }) // Sets language to the default server-side value. // Could be null, or some generated or constant default value too. tx.mutate.user.insert({ id: 'user-123', username: 'sam' }) // Same as above tx.mutate.user.insert({ id: 'user-123', username: 'sam', language: undefined }) Upsert Create new records or update existing ones with upsert: tx.mutate.user.upsert({ id: samID, username: 'sam', language: 'ts' }) upsert supports the same null / undefined semantics for optional fields that insert does (see above). Update Update an existing record. Does nothing if the specified record (by PK) does not exist. You can pass a partial object, leaving fields out that you don‚Äôt want to change. For example here we leave the username the same: // Leaves username field to previous value. tx.mutate.user.update({ id: samID, language: 'golang' }) // Same as above tx.mutate.user.update({ id: samID, username: undefined, language: 'haskell' }) // Reset language field to `null` tx.mutate.user.update({ id: samID, language: null }) Delete Delete an existing record. Does nothing if specified record does not exist. tx.mutate.user.delete({ id: samID }) Arguments The MutatorFn can take a single args parameter. To enable this, pass a validator to defineMutator: import {defineMutator} from '@rocicorp/zero' const initStats = defineMutator( z.object({issueCount: z.number()}), async ({ tx, args: {issueCount} }) => { if (issueCount < 0) { throw new Error(`issueCount cannot be negative`) } await tx.mutate.stats.insert({ id: 'global', issueCount }) } ) We use Zod in these examples, but you can use any validation library that implements Standard Schema. It's most common for mutators to be a pure function of the database state plus arguments. But it's not required. Impure mutators can be useful, e.g., to consult some external system on the server for authorization or validation. Reading Data You can read data within a mutator by passing ZQL to tx.run: const updateIssue = defineMutator( z.object({id: z.string(), title: z.string()}), async ({tx, args: {id, title}}) => { const issue = await tx.run( zql.issue.where('id', id).one() ) if (issue?.status === 'closed') { throw new Error(`Cannot update closed issue`) } await tx.mutate.issue.update({ id, title }) } ) You have the full power of ZQL at your disposal, including relationships, filters, ordering, and limits. Reads and writes within a mutator are transactional, meaning that the datastore is guaranteed to not change while your mutator is running. And if the mutator throws, the entire mutation is rolled back. Unlike zero.run(), there is no type parameter that can be used to wait for server results inside mutators. This is because waiting for server results in mutators makes no sense ‚Äì it would defeat the purpose of running optimistically to begin with. When a mutator runs on the client (tx.location === \"client\"), ZQL reads only return data already cached on the client. When mutators run on the server (tx.location === \"server\"), ZQL reads always return all data. Context Mutator parameters are supplied by the client application and passed to the server automatically by Zero. This makes them unsuitable for credentials, since the user could modify them. For this reason, Zero mutators also support the concept of a context object. Access your context with the ctx parameter to your mutator: const createIssue = defineMutator( z.object({id: z.string(), title: z.string()}), async ({tx, ctx: {userID}, args: {id, title}}) => { // Note: User cannot control ctx.userID, so this // enforces authorship of created issue. await tx.mutate.issue.insert({ id, title, authorID: userID }) } ) Mutator Registries The result of defineMutator is a MutatorDefinition. By itself this isn't super useful. You need to register it using defineMutators: export const mutators = defineMutators({ issue: { update: updateIssue } }) Typically these are done together in one step: export const mutators = defineMutators({ issue: { update: defineMutator( z.object({id: z.string(), title: z.string()}), async ({tx, args: {id, title}}) => { await tx.mutate.issue.update({ id, title }) } ) } }) The result of defineMutators is called a MutatorRegistry. Each field in the registry is a callable Mutator that you can use to perform mutations: import {mutators} from 'mutators.ts' zero.mutate( mutators.issue.update({ id: 'issue-123', title: 'New title' }) ) Mutator Names Each Mutator has a mutatorName which is computed by defineMutators. When you run a mutator, Zero sends this name along with the arguments to your server to execute the server-side mutation. console.log(mutators.issue.update.mutatorName) // \"issue.update\" mutators.ts By convention, mutators are listed in a central mutators.ts file. This allows them to be easily used on both the client and server: import {defineMutators, defineMutator} from '@rocicorp/zero' import {zql} from './schema.ts' import {z} from 'zod' export const mutators = defineMutators({ posts: { create: defineMutator( z.object({ id: z.string(), title: z.string() }), async ({ tx, context: {userID}, args: {id, title} }) => { await tx.mutate.post.insert({ id, title, authorID: userID }) } ), update: defineMutator( z.object({ id: z.string(), title: z.string().optional() }), async ({ tx, context: {userID}, args: {id, title} }) => { const prev = await tx.run( zql.post.where('id', id).one() ) if (prev?.authorID !== userID) { throw new Error(`Access denied`) } await tx.mutate.post.update({ id, title, authorID: userID }) } ) } }) You can use as many levels of nesting as you want to organize your mutators. As your application grows, you can move mutators to different files to keep them organized: // posts.ts export const postMutators = { create: defineMutator( z.object({ id: z.string(), title: z.string(), }), async ({tx, context: {userID}, args: {id, title}}) => { await tx.mutate.post.insert({ id, title, authorID: userID, }) }, ), } // user.ts export const userMutators = { updateRole: defineMutator( z.object({ role: z.string(), }), async ({tx, ctx: {userID}, args: {role}}) => { await tx.mutate.user.update({ id: userID, role, }) }, ), } // mutators.ts import {postMutators} from 'zero/mutators/posts.ts' import {userMutators} from 'zero/mutators/users.ts' export const mutators = defineMutators{{ posts: postMutators, users: userMutators, }) defineMutators establishes the full name for each mutator (i.e., posts.create, users.updateRole), which is later sent to the server. So this should only be used once at the top level of your mutators.ts file.",
    "kind": "section"
  },
  {
    "id": "172-mutators#basics",
    "title": "Mutators",
    "searchTitle": "Basics",
    "sectionTitle": "Basics",
    "sectionId": "basics",
    "url": "/docs/mutators",
    "content": "Create a mutator using defineMutator. The only required argument is a MutatorFn, which must be async: import {defineMutator} from '@rocicorp/zero' const myMutator = defineMutator(async () => { // ... }) Mutators almost always complete in the same frame on the client, within milliseconds. The reason they are marked async is because on the server, reading from the tx object goes over the network to Postgres.",
    "kind": "section"
  },
  {
    "id": "173-mutators#writing-data",
    "title": "Mutators",
    "searchTitle": "Writing Data",
    "sectionTitle": "Writing Data",
    "sectionId": "writing-data",
    "url": "/docs/mutators",
    "content": "The MutatorFn receives a tx parameter which can be used to write data with a CRUD-style API. Each table in your Zero schema has a corresponding field on tx.mutate: const myMutator = defineMutator(async ({tx}) => { // This is here because there's a `user` table in your schema. await tx.mutate.user.insert(...) }) Failing to do so allows the transaction to commit early, causing runtime errors when writes are attempted later. Insert Create new records with insert: tx.mutate.user.insert({ id: 'user-123', username: 'sam', language: 'js' }) Optional fields can be set to null to explicitly set the new field to null. They can also be set to undefined to take the default value (which is often null but can also be some generated value server-side): // Sets language to `null` specifically tx.mutate.user.insert({ id: 'user-123', username: 'sam', language: null }) // Sets language to the default server-side value. // Could be null, or some generated or constant default value too. tx.mutate.user.insert({ id: 'user-123', username: 'sam' }) // Same as above tx.mutate.user.insert({ id: 'user-123', username: 'sam', language: undefined }) Upsert Create new records or update existing ones with upsert: tx.mutate.user.upsert({ id: samID, username: 'sam', language: 'ts' }) upsert supports the same null / undefined semantics for optional fields that insert does (see above). Update Update an existing record. Does nothing if the specified record (by PK) does not exist. You can pass a partial object, leaving fields out that you don‚Äôt want to change. For example here we leave the username the same: // Leaves username field to previous value. tx.mutate.user.update({ id: samID, language: 'golang' }) // Same as above tx.mutate.user.update({ id: samID, username: undefined, language: 'haskell' }) // Reset language field to `null` tx.mutate.user.update({ id: samID, language: null }) Delete Delete an existing record. Does nothing if specified record does not exist. tx.mutate.user.delete({ id: samID })",
    "kind": "section"
  },
  {
    "id": "174-mutators#insert",
    "title": "Mutators",
    "searchTitle": "Insert",
    "sectionTitle": "Insert",
    "sectionId": "insert",
    "url": "/docs/mutators",
    "content": "Create new records with insert: tx.mutate.user.insert({ id: 'user-123', username: 'sam', language: 'js' }) Optional fields can be set to null to explicitly set the new field to null. They can also be set to undefined to take the default value (which is often null but can also be some generated value server-side): // Sets language to `null` specifically tx.mutate.user.insert({ id: 'user-123', username: 'sam', language: null }) // Sets language to the default server-side value. // Could be null, or some generated or constant default value too. tx.mutate.user.insert({ id: 'user-123', username: 'sam' }) // Same as above tx.mutate.user.insert({ id: 'user-123', username: 'sam', language: undefined })",
    "kind": "section"
  },
  {
    "id": "175-mutators#upsert",
    "title": "Mutators",
    "searchTitle": "Upsert",
    "sectionTitle": "Upsert",
    "sectionId": "upsert",
    "url": "/docs/mutators",
    "content": "Create new records or update existing ones with upsert: tx.mutate.user.upsert({ id: samID, username: 'sam', language: 'ts' }) upsert supports the same null / undefined semantics for optional fields that insert does (see above).",
    "kind": "section"
  },
  {
    "id": "176-mutators#update",
    "title": "Mutators",
    "searchTitle": "Update",
    "sectionTitle": "Update",
    "sectionId": "update",
    "url": "/docs/mutators",
    "content": "Update an existing record. Does nothing if the specified record (by PK) does not exist. You can pass a partial object, leaving fields out that you don‚Äôt want to change. For example here we leave the username the same: // Leaves username field to previous value. tx.mutate.user.update({ id: samID, language: 'golang' }) // Same as above tx.mutate.user.update({ id: samID, username: undefined, language: 'haskell' }) // Reset language field to `null` tx.mutate.user.update({ id: samID, language: null })",
    "kind": "section"
  },
  {
    "id": "177-mutators#delete",
    "title": "Mutators",
    "searchTitle": "Delete",
    "sectionTitle": "Delete",
    "sectionId": "delete",
    "url": "/docs/mutators",
    "content": "Delete an existing record. Does nothing if specified record does not exist. tx.mutate.user.delete({ id: samID })",
    "kind": "section"
  },
  {
    "id": "178-mutators#arguments",
    "title": "Mutators",
    "searchTitle": "Arguments",
    "sectionTitle": "Arguments",
    "sectionId": "arguments",
    "url": "/docs/mutators",
    "content": "The MutatorFn can take a single args parameter. To enable this, pass a validator to defineMutator: import {defineMutator} from '@rocicorp/zero' const initStats = defineMutator( z.object({issueCount: z.number()}), async ({ tx, args: {issueCount} }) => { if (issueCount < 0) { throw new Error(`issueCount cannot be negative`) } await tx.mutate.stats.insert({ id: 'global', issueCount }) } ) We use Zod in these examples, but you can use any validation library that implements Standard Schema. It's most common for mutators to be a pure function of the database state plus arguments. But it's not required. Impure mutators can be useful, e.g., to consult some external system on the server for authorization or validation.",
    "kind": "section"
  },
  {
    "id": "179-mutators#reading-data",
    "title": "Mutators",
    "searchTitle": "Reading Data",
    "sectionTitle": "Reading Data",
    "sectionId": "reading-data",
    "url": "/docs/mutators",
    "content": "You can read data within a mutator by passing ZQL to tx.run: const updateIssue = defineMutator( z.object({id: z.string(), title: z.string()}), async ({tx, args: {id, title}}) => { const issue = await tx.run( zql.issue.where('id', id).one() ) if (issue?.status === 'closed') { throw new Error(`Cannot update closed issue`) } await tx.mutate.issue.update({ id, title }) } ) You have the full power of ZQL at your disposal, including relationships, filters, ordering, and limits. Reads and writes within a mutator are transactional, meaning that the datastore is guaranteed to not change while your mutator is running. And if the mutator throws, the entire mutation is rolled back. Unlike zero.run(), there is no type parameter that can be used to wait for server results inside mutators. This is because waiting for server results in mutators makes no sense ‚Äì it would defeat the purpose of running optimistically to begin with. When a mutator runs on the client (tx.location === \"client\"), ZQL reads only return data already cached on the client. When mutators run on the server (tx.location === \"server\"), ZQL reads always return all data.",
    "kind": "section"
  },
  {
    "id": "180-mutators#context",
    "title": "Mutators",
    "searchTitle": "Context",
    "sectionTitle": "Context",
    "sectionId": "context",
    "url": "/docs/mutators",
    "content": "Mutator parameters are supplied by the client application and passed to the server automatically by Zero. This makes them unsuitable for credentials, since the user could modify them. For this reason, Zero mutators also support the concept of a context object. Access your context with the ctx parameter to your mutator: const createIssue = defineMutator( z.object({id: z.string(), title: z.string()}), async ({tx, ctx: {userID}, args: {id, title}}) => { // Note: User cannot control ctx.userID, so this // enforces authorship of created issue. await tx.mutate.issue.insert({ id, title, authorID: userID }) } )",
    "kind": "section"
  },
  {
    "id": "181-mutators#mutator-registries",
    "title": "Mutators",
    "searchTitle": "Mutator Registries",
    "sectionTitle": "Mutator Registries",
    "sectionId": "mutator-registries",
    "url": "/docs/mutators",
    "content": "The result of defineMutator is a MutatorDefinition. By itself this isn't super useful. You need to register it using defineMutators: export const mutators = defineMutators({ issue: { update: updateIssue } }) Typically these are done together in one step: export const mutators = defineMutators({ issue: { update: defineMutator( z.object({id: z.string(), title: z.string()}), async ({tx, args: {id, title}}) => { await tx.mutate.issue.update({ id, title }) } ) } }) The result of defineMutators is called a MutatorRegistry. Each field in the registry is a callable Mutator that you can use to perform mutations: import {mutators} from 'mutators.ts' zero.mutate( mutators.issue.update({ id: 'issue-123', title: 'New title' }) )",
    "kind": "section"
  },
  {
    "id": "182-mutators#mutator-names",
    "title": "Mutators",
    "searchTitle": "Mutator Names",
    "sectionTitle": "Mutator Names",
    "sectionId": "mutator-names",
    "url": "/docs/mutators",
    "content": "Each Mutator has a mutatorName which is computed by defineMutators. When you run a mutator, Zero sends this name along with the arguments to your server to execute the server-side mutation. console.log(mutators.issue.update.mutatorName) // \"issue.update\"",
    "kind": "section"
  },
  {
    "id": "183-mutators#mutatorsts",
    "title": "Mutators",
    "searchTitle": "mutators.ts",
    "sectionTitle": "mutators.ts",
    "sectionId": "mutatorsts",
    "url": "/docs/mutators",
    "content": "By convention, mutators are listed in a central mutators.ts file. This allows them to be easily used on both the client and server: import {defineMutators, defineMutator} from '@rocicorp/zero' import {zql} from './schema.ts' import {z} from 'zod' export const mutators = defineMutators({ posts: { create: defineMutator( z.object({ id: z.string(), title: z.string() }), async ({ tx, context: {userID}, args: {id, title} }) => { await tx.mutate.post.insert({ id, title, authorID: userID }) } ), update: defineMutator( z.object({ id: z.string(), title: z.string().optional() }), async ({ tx, context: {userID}, args: {id, title} }) => { const prev = await tx.run( zql.post.where('id', id).one() ) if (prev?.authorID !== userID) { throw new Error(`Access denied`) } await tx.mutate.post.update({ id, title, authorID: userID }) } ) } }) You can use as many levels of nesting as you want to organize your mutators. As your application grows, you can move mutators to different files to keep them organized: // posts.ts export const postMutators = { create: defineMutator( z.object({ id: z.string(), title: z.string(), }), async ({tx, context: {userID}, args: {id, title}}) => { await tx.mutate.post.insert({ id, title, authorID: userID, }) }, ), } // user.ts export const userMutators = { updateRole: defineMutator( z.object({ role: z.string(), }), async ({tx, ctx: {userID}, args: {role}}) => { await tx.mutate.user.update({ id: userID, role, }) }, ), } // mutators.ts import {postMutators} from 'zero/mutators/posts.ts' import {userMutators} from 'zero/mutators/users.ts' export const mutators = defineMutators{{ posts: postMutators, users: userMutators, }) defineMutators establishes the full name for each mutator (i.e., posts.create, users.updateRole), which is later sent to the server. So this should only be used once at the top level of your mutators.ts file.",
    "kind": "section"
  },
  {
    "id": "184-mutators#registration",
    "title": "Mutators",
    "searchTitle": "Registration",
    "sectionTitle": "Registration",
    "sectionId": "registration",
    "url": "/docs/mutators",
    "content": "Before you can use your mutators, you need to register them with Zero: import {ZeroProvider} from '@rocicorp/zero/react' import type {ZeroOptions} from '@rocicorp/zero' import {mutators} from 'zero/mutators.ts' const opts: ZeroOptions = { // ... cacheURL, schema, etc. mutators } return ( <ZeroProvider {...opts}> <App /> </ZeroProvider> )import {ZeroProvider} from '@rocicorp/zero/solid' import type {ZeroOptions} from '@rocicorp/zero' import {mutators} from 'zero/mutators.ts' const opts: ZeroOptions = { // ... cacheURL, schema, etc. mutators } return ( <ZeroProvider {...opts}> <App /> </ZeroProvider> )import {Zero} from '@rocicorp/zero' import type {ZeroOptions} from '@rocicorp/zero' import {mutators} from 'zero/mutators.ts' const opts: ZeroOptions = { // ... cacheURL, schema, etc. mutators } const zero = new Zero(opts) Mutators need to be registered with Zero because Zero calls them during sync for conflict resolution. If you invoke a mutator that is not registered, Zero will throw an error.",
    "kind": "section"
  },
  {
    "id": "185-mutators#server-setup",
    "title": "Mutators",
    "searchTitle": "Server Setup",
    "sectionTitle": "Server Setup",
    "sectionId": "server-setup",
    "url": "/docs/mutators",
    "content": "In order for mutations to sync, you must provide an implementation of the mutate endpoint on your server. zero-cache calls this endpoint to process each mutation. Registering the Endpoint Use ZERO_MUTATE_URL to tell zero-cache where to find your mutate implementation: export ZERO_MUTATE_URL=\"http://localhost:3000/api/zero/mutate\" # run zero-cache, e.g. `npx zero-cache-dev` Implementing the Endpoint You can use the handleMutateRequest and mustGetMutator functions to implement the endpoint. Plug in whatever dbProvider you set up (see server-zql or the install guide). // src/routes/api/zero/mutate.ts import {createFileRoute} from '@tanstack/react-router' import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from 'mutators.ts' import {dbProvider} from 'db-provider.ts' export const Route = createFileRoute('/api/zero/mutate')({ server: { handlers: { POST: async ({request}) => { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({ args, tx, ctx: {userId: 'anon'} }) }), request ) return Response.json(result) } } } })// app/api/zero/mutate/route.ts import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from 'mutators.ts' import {dbProvider} from 'db-provider.ts' export async function POST(req: Request) { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({args, tx, ctx: {userId: 'anon'}}) }), req ) return Response.json(result) }// src/routes/api/zero/mutate.ts import type {APIEvent} from '@solidjs/start/server' import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from 'mutators.ts' import {dbProvider} from 'db-provider.ts' export async function POST(event: APIEvent) { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({args, tx, ctx: {userId: 'anon'}}) }), event.request ) return new Response.json(result) }// api/app.ts import {Hono} from 'hono' import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from 'mutators.ts' import {dbProvider} from './db-provider.ts' const app = new Hono() app.post('/api/zero/mutate', async c => { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({ args, tx, ctx: {userId: 'anon'} }) }), c.req.raw ) return c.json(result) }) Zero includes several built-in database adapters. You can also easily create your own. See ZQL on the Server for more information. handleMutateRequest accepts a standard Request and returns a JSON object which can be serialized and returned by your server framework of choice. mustGetMutator looks up the mutator in the registry and throws an error if not found. The mutator.fn function is your mutator implementation wrapped in the validator you provided. Handling Errors The handleMutateRequest function skips any mutations that throw: const result = await handleMutateRequest( dbProvider, transact => transact(async (tx, name, args) => { // The mutation is skipped and the next mutation runs as normal. // The optimistic mutation on the client will be reverted. throw new Error('bonk') }), c.req.raw ) handleMutateRequest catches such errors and turns them into a structured response that gets sent back to the client. You can recover the errors and show UI if you want. It is also of course possible for the entire push endpoint to return an HTTP error, or to not reply at all: export const Route = createFileRoute('/api/zero/mutate')({ server: { handlers: { POST: async () => { throw new Error('zonk') // will trigger resend } } } })export async function POST() { throw new Error('zonk') // will trigger resend }export async function POST() { throw new Error('zonk') // will trigger resend }app.post('/api/zero/mutate', async c => { // This will cause the client to resend all queued mutations. throw new Error('zonk') }) If Zero receives any response from the mutate endpoint other than HTTP 200, 401, or 403, it will disconnect and enter the error state. If Zero receives HTTP 401 or 403, the client will enter the needs auth state and require a manual reconnect with zero.connection.connect(), then it will retry all queued mutations. If you want a different behavior, it is possible to implement the mutate endpoint yourself and handle errors differently. Custom Mutate URL By default, Zero sends mutations to the URL specified in the ZERO_MUTATE_URL parameter. However you can customize this on a per-client basis. To do so, list multiple comma-separated URLs in the ZERO_MUTATE_URL parameter: export ZERO_MUTATE_URL=\"https://api.example.com/mutate,https://api.staging.example.com/mutate\" Then choose one of those URLs by passing it to mutateURL on the Zero constructor: const opts: ZeroOptions = { // ... mutateURL: 'https://api.staging.example.com/mutate' } URL Patterns The strings listed in ZERO_MUTATE_URL can also be URLPatterns: export ZERO_MUTATE_URL=\"https://mybranch-*.preview.myapp.com/mutate\" For more information, see the URLPattern section of the Queries docs. It works the same way for mutations. Server-Specific Code To implement server-specific code, just run different mutators in your mutate endpoint. Server authority to the rescue! defineMutators accepts a baseMutators parameter that makes this easy. The returned mutator registry will contain all the mutators from baseMutators, plus any new ones you define or override: // server-mutators.ts import {defineMutators, defineMutator} from '@rocicorp/zero' import {z} from 'zod' import {zql} from 'schema.ts' import {mutators as sharedMutators} from 'mutators.ts' export const serverMutators = defineMutators( sharedMutators, { posts: { // Overrides the shared mutator definition with same name. update: defineMutator( z.object({ id: z.string(), title: z.string().optional(), priority: z.number().optional() }), async ({ tx, ctx: {userID}, args: {id, title, priority} }) => { // Run the shared mutator first. await sharedMutators.posts.update.fn({ tx, ctx, args }) // Record a history of this operation happening in an audit log table. await tx.mutate.auditLog.insert({ issueId: id, action: 'update-title', timestamp: Date.getTime() }) } ) } } ) For simple things, we also expose a location field on the transaction object that you can use to branch your code: const myMutator = defineMutator(async ({tx}) => { if (tx.location === 'client') { // Client-side code } else { // Server-side code } })",
    "kind": "section"
  },
  {
    "id": "186-mutators#registering-the-endpoint",
    "title": "Mutators",
    "searchTitle": "Registering the Endpoint",
    "sectionTitle": "Registering the Endpoint",
    "sectionId": "registering-the-endpoint",
    "url": "/docs/mutators",
    "content": "Use ZERO_MUTATE_URL to tell zero-cache where to find your mutate implementation: export ZERO_MUTATE_URL=\"http://localhost:3000/api/zero/mutate\" # run zero-cache, e.g. `npx zero-cache-dev`",
    "kind": "section"
  },
  {
    "id": "187-mutators#implementing-the-endpoint",
    "title": "Mutators",
    "searchTitle": "Implementing the Endpoint",
    "sectionTitle": "Implementing the Endpoint",
    "sectionId": "implementing-the-endpoint",
    "url": "/docs/mutators",
    "content": "You can use the handleMutateRequest and mustGetMutator functions to implement the endpoint. Plug in whatever dbProvider you set up (see server-zql or the install guide). // src/routes/api/zero/mutate.ts import {createFileRoute} from '@tanstack/react-router' import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from 'mutators.ts' import {dbProvider} from 'db-provider.ts' export const Route = createFileRoute('/api/zero/mutate')({ server: { handlers: { POST: async ({request}) => { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({ args, tx, ctx: {userId: 'anon'} }) }), request ) return Response.json(result) } } } })// app/api/zero/mutate/route.ts import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from 'mutators.ts' import {dbProvider} from 'db-provider.ts' export async function POST(req: Request) { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({args, tx, ctx: {userId: 'anon'}}) }), req ) return Response.json(result) }// src/routes/api/zero/mutate.ts import type {APIEvent} from '@solidjs/start/server' import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from 'mutators.ts' import {dbProvider} from 'db-provider.ts' export async function POST(event: APIEvent) { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({args, tx, ctx: {userId: 'anon'}}) }), event.request ) return new Response.json(result) }// api/app.ts import {Hono} from 'hono' import {handleMutateRequest} from '@rocicorp/zero/server' import {mustGetMutator} from '@rocicorp/zero' import {mutators} from 'mutators.ts' import {dbProvider} from './db-provider.ts' const app = new Hono() app.post('/api/zero/mutate', async c => { const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({ args, tx, ctx: {userId: 'anon'} }) }), c.req.raw ) return c.json(result) }) Zero includes several built-in database adapters. You can also easily create your own. See ZQL on the Server for more information. handleMutateRequest accepts a standard Request and returns a JSON object which can be serialized and returned by your server framework of choice. mustGetMutator looks up the mutator in the registry and throws an error if not found. The mutator.fn function is your mutator implementation wrapped in the validator you provided.",
    "kind": "section"
  },
  {
    "id": "188-mutators#handling-errors",
    "title": "Mutators",
    "searchTitle": "Handling Errors",
    "sectionTitle": "Handling Errors",
    "sectionId": "handling-errors",
    "url": "/docs/mutators",
    "content": "The handleMutateRequest function skips any mutations that throw: const result = await handleMutateRequest( dbProvider, transact => transact(async (tx, name, args) => { // The mutation is skipped and the next mutation runs as normal. // The optimistic mutation on the client will be reverted. throw new Error('bonk') }), c.req.raw ) handleMutateRequest catches such errors and turns them into a structured response that gets sent back to the client. You can recover the errors and show UI if you want. It is also of course possible for the entire push endpoint to return an HTTP error, or to not reply at all: export const Route = createFileRoute('/api/zero/mutate')({ server: { handlers: { POST: async () => { throw new Error('zonk') // will trigger resend } } } })export async function POST() { throw new Error('zonk') // will trigger resend }export async function POST() { throw new Error('zonk') // will trigger resend }app.post('/api/zero/mutate', async c => { // This will cause the client to resend all queued mutations. throw new Error('zonk') }) If Zero receives any response from the mutate endpoint other than HTTP 200, 401, or 403, it will disconnect and enter the error state. If Zero receives HTTP 401 or 403, the client will enter the needs auth state and require a manual reconnect with zero.connection.connect(), then it will retry all queued mutations. If you want a different behavior, it is possible to implement the mutate endpoint yourself and handle errors differently.",
    "kind": "section"
  },
  {
    "id": "189-mutators#custom-mutate-url",
    "title": "Mutators",
    "searchTitle": "Custom Mutate URL",
    "sectionTitle": "Custom Mutate URL",
    "sectionId": "custom-mutate-url",
    "url": "/docs/mutators",
    "content": "By default, Zero sends mutations to the URL specified in the ZERO_MUTATE_URL parameter. However you can customize this on a per-client basis. To do so, list multiple comma-separated URLs in the ZERO_MUTATE_URL parameter: export ZERO_MUTATE_URL=\"https://api.example.com/mutate,https://api.staging.example.com/mutate\" Then choose one of those URLs by passing it to mutateURL on the Zero constructor: const opts: ZeroOptions = { // ... mutateURL: 'https://api.staging.example.com/mutate' }",
    "kind": "section"
  },
  {
    "id": "190-mutators#url-patterns",
    "title": "Mutators",
    "searchTitle": "URL Patterns",
    "sectionTitle": "URL Patterns",
    "sectionId": "url-patterns",
    "url": "/docs/mutators",
    "content": "The strings listed in ZERO_MUTATE_URL can also be URLPatterns: export ZERO_MUTATE_URL=\"https://mybranch-*.preview.myapp.com/mutate\" For more information, see the URLPattern section of the Queries docs. It works the same way for mutations.",
    "kind": "section"
  },
  {
    "id": "191-mutators#server-specific-code",
    "title": "Mutators",
    "searchTitle": "Server-Specific Code",
    "sectionTitle": "Server-Specific Code",
    "sectionId": "server-specific-code",
    "url": "/docs/mutators",
    "content": "To implement server-specific code, just run different mutators in your mutate endpoint. Server authority to the rescue! defineMutators accepts a baseMutators parameter that makes this easy. The returned mutator registry will contain all the mutators from baseMutators, plus any new ones you define or override: // server-mutators.ts import {defineMutators, defineMutator} from '@rocicorp/zero' import {z} from 'zod' import {zql} from 'schema.ts' import {mutators as sharedMutators} from 'mutators.ts' export const serverMutators = defineMutators( sharedMutators, { posts: { // Overrides the shared mutator definition with same name. update: defineMutator( z.object({ id: z.string(), title: z.string().optional(), priority: z.number().optional() }), async ({ tx, ctx: {userID}, args: {id, title, priority} }) => { // Run the shared mutator first. await sharedMutators.posts.update.fn({ tx, ctx, args }) // Record a history of this operation happening in an audit log table. await tx.mutate.auditLog.insert({ issueId: id, action: 'update-title', timestamp: Date.getTime() }) } ) } } ) For simple things, we also expose a location field on the transaction object that you can use to branch your code: const myMutator = defineMutator(async ({tx}) => { if (tx.location === 'client') { // Client-side code } else { // Server-side code } })",
    "kind": "section"
  },
  {
    "id": "192-mutators#running-mutators",
    "title": "Mutators",
    "searchTitle": "Running Mutators",
    "sectionTitle": "Running Mutators",
    "sectionId": "running-mutators",
    "url": "/docs/mutators",
    "content": "Once you have registered your mutators, you can invoke them with zero.mutate: import {mutators} from 'mutators.ts' import {nanoid} from 'nanoid' zero.mutate( mutators.issue.update({ id: nanoid(), title: 'New title' }) ) Client-generated random IDs from libraries like uuid, ulid, or nanoid work much better with sync engines like Zero. See IDs for more details. Waiting for Results We typically recommend that you \"fire and forget\" mutators. Optimistic mutations make sense when the common case is that a mutation succeeds. If a mutation frequently fails, then showing the user an optimistic result isn't very useful, because it will likely be wrong. That said there are cases where it is nice to know when a write succeeded on either the client or server. One example is if you need to read a row directly after writing it. Zero's local writes are very fast (almost always < 1 frame), but because Zero is backed by IndexedDB, writes are still technically asynchronous and reads directly after a write may not return the new data. You can use the .client promise in this case to wait for a write to complete on the client side: const write = zero.mutate( mutators.issue.insert({ id: nanoid(), title: 'New title' }) ) // issue-123 not guaranteed to be present here. read1 may be undefined. const read1 = await zero.run( queries.issue.byId('issue-123').one() ) // Await client write ‚Äì almost always less than 1 frame, and same // macrotask, so no browser paint will occur here. const res = await write.client if (res.type === 'error') { console.error('Mutator failed on client', res.error) } // issue-123 definitely can be read now. const read2 = await zero.run( queries.issue.byId('issue-123').one() ) You can also wait for the server write to succeed: const write = zero.mutate( mutators.issue.insert({ id: nanoid(), title: 'New title' }) ) const clientRes = await write.client if (clientRes.type === 'error') { throw new Error( `Mutator failed on client`, clientRes.error ) } // optimistic write guaranteed to be present here, but not // server write. const read1 = await zero.run( queries.issue.byId('issue-123').one() ) // Await server write ‚Äì this involves a round-trip. const serverRes = await write.server if (serverRes.type === 'error') { throw new Error( `Mutator failed on server`, serverRes.error ) } // issue-123 is written to server and any results are // synced to this client. // read2 could potentially be undefined here, for example if the // server mutator rejected the write. const read2 = await zero.run( queries.issue.byId('issue-123').one() ) If the client-side mutator fails, the .server promise is also rejected with the same error. You don't have to listen to both promises, the server promise covers both cases. There is not yet a way to return data from mutators in the success case. Let us know if you need this.",
    "kind": "section"
  },
  {
    "id": "193-mutators#waiting-for-results",
    "title": "Mutators",
    "searchTitle": "Waiting for Results",
    "sectionTitle": "Waiting for Results",
    "sectionId": "waiting-for-results",
    "url": "/docs/mutators",
    "content": "We typically recommend that you \"fire and forget\" mutators. Optimistic mutations make sense when the common case is that a mutation succeeds. If a mutation frequently fails, then showing the user an optimistic result isn't very useful, because it will likely be wrong. That said there are cases where it is nice to know when a write succeeded on either the client or server. One example is if you need to read a row directly after writing it. Zero's local writes are very fast (almost always < 1 frame), but because Zero is backed by IndexedDB, writes are still technically asynchronous and reads directly after a write may not return the new data. You can use the .client promise in this case to wait for a write to complete on the client side: const write = zero.mutate( mutators.issue.insert({ id: nanoid(), title: 'New title' }) ) // issue-123 not guaranteed to be present here. read1 may be undefined. const read1 = await zero.run( queries.issue.byId('issue-123').one() ) // Await client write ‚Äì almost always less than 1 frame, and same // macrotask, so no browser paint will occur here. const res = await write.client if (res.type === 'error') { console.error('Mutator failed on client', res.error) } // issue-123 definitely can be read now. const read2 = await zero.run( queries.issue.byId('issue-123').one() ) You can also wait for the server write to succeed: const write = zero.mutate( mutators.issue.insert({ id: nanoid(), title: 'New title' }) ) const clientRes = await write.client if (clientRes.type === 'error') { throw new Error( `Mutator failed on client`, clientRes.error ) } // optimistic write guaranteed to be present here, but not // server write. const read1 = await zero.run( queries.issue.byId('issue-123').one() ) // Await server write ‚Äì this involves a round-trip. const serverRes = await write.server if (serverRes.type === 'error') { throw new Error( `Mutator failed on server`, serverRes.error ) } // issue-123 is written to server and any results are // synced to this client. // read2 could potentially be undefined here, for example if the // server mutator rejected the write. const read2 = await zero.run( queries.issue.byId('issue-123').one() ) If the client-side mutator fails, the .server promise is also rejected with the same error. You don't have to listen to both promises, the server promise covers both cases. There is not yet a way to return data from mutators in the success case. Let us know if you need this.",
    "kind": "section"
  },
  {
    "id": "194-mutators#permissions",
    "title": "Mutators",
    "searchTitle": "Permissions",
    "sectionTitle": "Permissions",
    "sectionId": "permissions",
    "url": "/docs/mutators",
    "content": "Because mutators are just normal TypeScript functions that run server-side, there is no need for a special permissions system. You can implement whatever permission checks you want using plain TypeScript code. See Permissions for more information.",
    "kind": "section"
  },
  {
    "id": "195-mutators#dropping-down-to-raw-sql",
    "title": "Mutators",
    "searchTitle": "Dropping Down to Raw SQL",
    "sectionTitle": "Dropping Down to Raw SQL",
    "sectionId": "dropping-down-to-raw-sql",
    "url": "/docs/mutators",
    "content": "The ServerTransaction interface has a dbTransaction property that exposes the underlying database connection. This allows you to run raw SQL queries directly against the database. This is useful for complex queries, or for using Postgres features that Zero doesn't support yet: const markAllAsRead = defineMutator( z.object({ userId: z.string() }), async ({tx, args: {userId}}) => { // shared stuff ... if (tx.location === 'server') { // `tx` is now narrowed to `ServerTransaction`. // Do special server-only stuff with raw SQL. await tx.dbTransaction.query( ` UPDATE notification SET read = true WHERE user_id = $1 `, [userId] ) } } ) See ZQL on the Server for more information.",
    "kind": "section"
  },
  {
    "id": "196-mutators#notifications-and-async-work",
    "title": "Mutators",
    "searchTitle": "Notifications and Async Work",
    "sectionTitle": "Notifications and Async Work",
    "sectionId": "notifications-and-async-work",
    "url": "/docs/mutators",
    "content": "The best way to handle notifications and async work is a transactional outbox. This ensures that notifications actually do eventually get sent, without holding open database transactions to talk over the network. This can be implemented very easily in Zero by writing notifications to an outbox table as part of your mutator, then processing that table periodically with a background job. However sometimes it's still nice to do a quick and dirty async send as part of a mutation, for example early on in development, or to record metrics. For this, the createMutators pattern is useful: // server-mutators.ts import {defineMutator} from '@rocicorp/zero' import z from 'zod' import {zql} from 'schema.ts' import {mutators as clientMutators} from 'mutators.ts' // Instead of defining server mutators as a constant, // define them as a function of a list of async tasks. export function createMutators( asyncTasks: Array<() => Promise<void>> ) { return defineMutators(clientMutators, { issue: { update: defineMutator( z.object({ id: z.string(), title: z.string() }), async (tx, {id, title}) => { await tx.mutate.issue.update({id, title}) asyncTasks.push(() => sendEmailToSubscribers(id)) } ) } }) } Then in your mutate handler: export const Route = createFileRoute('/api/zero/mutate')({ server: { handlers: { POST: async ({request}) => { const asyncTasks: Array<() => Promise<void>> = [] const mutators = createMutators(asyncTasks) const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({ tx, ctx: {userId: 'anon'}, args }) }), request ) // Run all async tasks // If any fail, do not block the response, since the // mutation result has already been written to the database. await Promise.allSettled( asyncTasks.map(task => task()) ) return json(result) } } } })export async function POST(req: Request) { const asyncTasks: Array<() => Promise<void>> = [] const mutators = createMutators(asyncTasks) const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({tx, ctx: {userId: 'anon'}, args}) }), req ) // Run all async tasks // If any fail, do not block the response, since the // mutation result has already been written to the database. await Promise.allSettled(asyncTasks.map(task => task())) return Response.json(result) }export async function POST(event: APIEvent) { const asyncTasks: Array<() => Promise<void>> = [] const mutators = createMutators(asyncTasks) const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({tx, ctx: {userId: 'anon'}, args}) }), event.request ) // Run all async tasks // If any fail, do not block the response, since the // mutation result has already been written to the database. await Promise.allSettled(asyncTasks.map(task => task())) return new Response.json(result) }app.post('/api/zero/mutate', async c => { const asyncTasks: Array<() => Promise<void>> = [] const mutators = createMutators(asyncTasks) const result = await handleMutateRequest( dbProvider, transact => transact((tx, name, args) => { const mutator = mustGetMutator(mutators, name) return mutator.fn({ tx, ctx: {userId: 'anon'}, args }) }), c.req.raw ) // Run all async tasks // If any fail, do not block the response, since the // mutation result has already been written to the database. await Promise.allSettled(asyncTasks.map(task => task())) return c.json(result) })",
    "kind": "section"
  },
  {
    "id": "197-mutators#custom-mutate-implementation",
    "title": "Mutators",
    "searchTitle": "Custom Mutate Implementation",
    "sectionTitle": "Custom Mutate Implementation",
    "sectionId": "custom-mutate-implementation",
    "url": "/docs/mutators",
    "content": "You can manually implement the mutate endpoint in any programming language. This will be documented in the future, but you can refer to the handleMutateRequest source code for an example for now.",
    "kind": "section"
  },
  {
    "id": "18-offline",
    "title": "Offline",
    "searchTitle": "Offline",
    "url": "/docs/offline",
    "content": "Zero currently supports offline reads, but not writes. When the client is disconnected, error, or closed, mutations are short-circuited and return an error. The lack of offline writes is often surprising to people familiar with sync engines, because offline is usually touted as something that comes for free with these tools. This page explains why Zero doesn't currently support offline writes, how we recommend you handle connectivity loss, and our future plans in this area. Offline Writes are a UX Problem While sync engines can technically queue offline writes and replay them when reconnected, that fact doesn't make supporting offline writes much easier. That's because a really hard part of offline writes is in handling conflicts, and no software tool can make that problem go away. For example, imagine two users are editing an article about cats. One goes offline and does a bunch of work on the article, while the other decides that the article should actually be about dogs and rewrites it. When the offline user reconnects, there is no way that any software algorithm can automatically resolve their conflict. One or the other of them is going to be upset. And while the above example may sound extreme, you can construct similar situations with the majority of common applications. Just take your own application and ask yourself what should really happen if one user takes their device offline for a week and makes arbitrarily complex changes while other users are working online. People who work on sync engines and related tools often say that offline is just extreme lag, but that's only true at a technical level. At a human level, being \"offline\" for a few seconds is very different from being offline for a few hours. The difference is how much knowledge you have about what your collaborators are doing, and how much of your work can be lost. The only way to support offline writes in general is to either: Make the logical datamodel append-only (i.e., users can create and mark tasks done, but cannot edit or delete them). Support custom UX to allow users to fork and merge conflicts when they occur. Only support editing from a single device. None of these is free. Building a good offline UX is a lot of work, and most of that work is borne by application developers. ‚Ä¶ And a Schema Problem But it's not just users that can diverge from each other. The server software and database schema can also diverge arbitrarily far from the client while the client is disconnected. When the client comes back online, the changes made may no longer be processable by the application, or may have a different effect than the user intended. So to support long offline periods, the server must also maintain backward compatibility with clients indefinitely. Similarly, the server can never reject an offline write (i.e., due to validation) because that could lead to a user losing huge amounts of work. ‚Ä¶ And a Sync Engine Problem Supporting offline writes also requires work in the sync engine. In Zero, there are a few specific impacts: The Zero client itself can get out of date while offline. On reconnect, the app might reload with a new version of the client. This new version must be able to read and process old data from arbitrarily long ago. An arbitrarily large number of pending mutations can be built up. These mutations must be replayed on reconnect, which can take a long time. When processing mutations on server we must consider what should happen if the database or application server are temporarily unavailable. We need to treat that kind of error differently from a validation error. These problems are surmountable, but significant effort. Their solutions might also be in tension with other goals of the sync engine, like online performance and scalability. These tradeoffs will take time to work through. Zero's Position For all of the above reasons, offline writes are disabled in Zero for beta. If zero-cache cannot reach your API server, the client transitions the connection to needs-auth or error rather than a special offline mode. Mutations are rejected in disconnected, needs-auth, error, and closed. While we recognize that offline writes would be useful for some applications, the reality is that for most of the apps we want to support, the user is online the vast majority of the time and the cost to support offline is extremely high. There is simply more value in making the online experience great first. We would like to revisit this in the future and really think through how to design APIs and patterns that allow developers to make successful offline-enabled apps. But it's not our priority right now. Dealing with Offline Today Use the Connection Status API to detect connection loss and visually disable writes in your UI: const state = useConnectionState() return ( <button disabled={state.name !== 'connected'}> Add to cart </button> ) Even More Information Lies I was Told About Collaborative Editing: a detailed overview of the challenges around offline writes in any collaborative editing system. This Zero Discord thread covers some challenges specifically in the context of Zero. Patchwork by Ink & Switch is new and interesting research around how to support offline writes well in collaborative systems.",
    "headings": [
      {
        "text": "Offline Writes are a UX Problem",
        "id": "offline-writes-are-a-ux-problem"
      },
      {
        "text": "‚Ä¶ And a Schema Problem",
        "id": "-and-a-schema-problem"
      },
      {
        "text": "‚Ä¶ And a Sync Engine Problem",
        "id": "-and-a-sync-engine-problem"
      },
      {
        "text": "Zero's Position",
        "id": "zeros-position"
      },
      {
        "text": "Dealing with Offline Today",
        "id": "dealing-with-offline-today"
      },
      {
        "text": "Even More Information",
        "id": "even-more-information"
      }
    ],
    "kind": "page"
  },
  {
    "id": "198-offline#offline-writes-are-a-ux-problem",
    "title": "Offline",
    "searchTitle": "Offline Writes are a UX Problem",
    "sectionTitle": "Offline Writes are a UX Problem",
    "sectionId": "offline-writes-are-a-ux-problem",
    "url": "/docs/offline",
    "content": "While sync engines can technically queue offline writes and replay them when reconnected, that fact doesn't make supporting offline writes much easier. That's because a really hard part of offline writes is in handling conflicts, and no software tool can make that problem go away. For example, imagine two users are editing an article about cats. One goes offline and does a bunch of work on the article, while the other decides that the article should actually be about dogs and rewrites it. When the offline user reconnects, there is no way that any software algorithm can automatically resolve their conflict. One or the other of them is going to be upset. And while the above example may sound extreme, you can construct similar situations with the majority of common applications. Just take your own application and ask yourself what should really happen if one user takes their device offline for a week and makes arbitrarily complex changes while other users are working online. People who work on sync engines and related tools often say that offline is just extreme lag, but that's only true at a technical level. At a human level, being \"offline\" for a few seconds is very different from being offline for a few hours. The difference is how much knowledge you have about what your collaborators are doing, and how much of your work can be lost. The only way to support offline writes in general is to either: Make the logical datamodel append-only (i.e., users can create and mark tasks done, but cannot edit or delete them). Support custom UX to allow users to fork and merge conflicts when they occur. Only support editing from a single device. None of these is free. Building a good offline UX is a lot of work, and most of that work is borne by application developers.",
    "kind": "section"
  },
  {
    "id": "199-offline#-and-a-schema-problem",
    "title": "Offline",
    "searchTitle": "‚Ä¶ And a Schema Problem",
    "sectionTitle": "‚Ä¶ And a Schema Problem",
    "sectionId": "-and-a-schema-problem",
    "url": "/docs/offline",
    "content": "But it's not just users that can diverge from each other. The server software and database schema can also diverge arbitrarily far from the client while the client is disconnected. When the client comes back online, the changes made may no longer be processable by the application, or may have a different effect than the user intended. So to support long offline periods, the server must also maintain backward compatibility with clients indefinitely. Similarly, the server can never reject an offline write (i.e., due to validation) because that could lead to a user losing huge amounts of work.",
    "kind": "section"
  },
  {
    "id": "200-offline#-and-a-sync-engine-problem",
    "title": "Offline",
    "searchTitle": "‚Ä¶ And a Sync Engine Problem",
    "sectionTitle": "‚Ä¶ And a Sync Engine Problem",
    "sectionId": "-and-a-sync-engine-problem",
    "url": "/docs/offline",
    "content": "Supporting offline writes also requires work in the sync engine. In Zero, there are a few specific impacts: The Zero client itself can get out of date while offline. On reconnect, the app might reload with a new version of the client. This new version must be able to read and process old data from arbitrarily long ago. An arbitrarily large number of pending mutations can be built up. These mutations must be replayed on reconnect, which can take a long time. When processing mutations on server we must consider what should happen if the database or application server are temporarily unavailable. We need to treat that kind of error differently from a validation error. These problems are surmountable, but significant effort. Their solutions might also be in tension with other goals of the sync engine, like online performance and scalability. These tradeoffs will take time to work through.",
    "kind": "section"
  },
  {
    "id": "201-offline#zeros-position",
    "title": "Offline",
    "searchTitle": "Zero's Position",
    "sectionTitle": "Zero's Position",
    "sectionId": "zeros-position",
    "url": "/docs/offline",
    "content": "For all of the above reasons, offline writes are disabled in Zero for beta. If zero-cache cannot reach your API server, the client transitions the connection to needs-auth or error rather than a special offline mode. Mutations are rejected in disconnected, needs-auth, error, and closed. While we recognize that offline writes would be useful for some applications, the reality is that for most of the apps we want to support, the user is online the vast majority of the time and the cost to support offline is extremely high. There is simply more value in making the online experience great first. We would like to revisit this in the future and really think through how to design APIs and patterns that allow developers to make successful offline-enabled apps. But it's not our priority right now.",
    "kind": "section"
  },
  {
    "id": "202-offline#dealing-with-offline-today",
    "title": "Offline",
    "searchTitle": "Dealing with Offline Today",
    "sectionTitle": "Dealing with Offline Today",
    "sectionId": "dealing-with-offline-today",
    "url": "/docs/offline",
    "content": "Use the Connection Status API to detect connection loss and visually disable writes in your UI: const state = useConnectionState() return ( <button disabled={state.name !== 'connected'}> Add to cart </button> )",
    "kind": "section"
  },
  {
    "id": "203-offline#even-more-information",
    "title": "Offline",
    "searchTitle": "Even More Information",
    "sectionTitle": "Even More Information",
    "sectionId": "even-more-information",
    "url": "/docs/offline",
    "content": "Lies I was Told About Collaborative Editing: a detailed overview of the challenges around offline writes in any collaborative editing system. This Zero Discord thread covers some challenges specifically in the context of Zero. Patchwork by Ink & Switch is new and interesting research around how to support offline writes well in collaborative systems.",
    "kind": "section"
  },
  {
    "id": "19-open-source",
    "title": "Zero is Open Source Software",
    "searchTitle": "Zero is Open Source Software",
    "url": "/docs/open-source",
    "content": "Specifically, the Zero client and server are Apache-2 licensed. You can use, modify, host, and distribute them freely: https://github.com/rocicorp/mono/blob/main/LICENSE Business Model We plan to commercialize Zero in the future by offering a hosted zero-cache service for people who do not want to run it themselves. We expect to charge prices for this roughly comparable to today's database hosting services. We'll also offer white-glove service to help enterprises run zero-cache within their own infrastructure. These plans may change as we develop Zero further. For example, we may also build closed-source companion software ‚Äì similar to how Docker, Inc. charges for team access to Docker Desktop. But we have no plans to ever change the licensing of the core product: We're building a general-purpose sync engine for the entire web, and we can only do that if the core remains completely open.",
    "headings": [
      {
        "text": "Business Model",
        "id": "business-model"
      }
    ],
    "kind": "page"
  },
  {
    "id": "204-open-source#business-model",
    "title": "Zero is Open Source Software",
    "searchTitle": "Business Model",
    "sectionTitle": "Business Model",
    "sectionId": "business-model",
    "url": "/docs/open-source",
    "content": "We plan to commercialize Zero in the future by offering a hosted zero-cache service for people who do not want to run it themselves. We expect to charge prices for this roughly comparable to today's database hosting services. We'll also offer white-glove service to help enterprises run zero-cache within their own infrastructure. These plans may change as we develop Zero further. For example, we may also build closed-source companion software ‚Äì similar to how Docker, Inc. charges for team access to Docker Desktop. But we have no plans to ever change the licensing of the core product: We're building a general-purpose sync engine for the entire web, and we can only do that if the core remains completely open.",
    "kind": "section"
  },
  {
    "id": "20-postgres-support",
    "title": "Supported Postgres Features",
    "searchTitle": "Supported Postgres Features",
    "url": "/docs/postgres-support",
    "content": "Postgres has a massive feature set, and Zero supports a growing subset of it. Object Names Table and column names must begin with a letter or underscore This can be followed by letters, numbers, underscores, and hyphens Regex: /^[A-Za-z_]+[A-Za-z0-9_-]*$/ The column name _0_version is reserved for internal use Object Types Tables are synced. Views are not synced. generated as identity columns are synced. In Postgres 18+, generated stored columns are synced. In lower Postgres versions they aren't. Indexes aren't synced per-se, but we do implicitly add indexes to the replica that match the upstream indexes. In the future, this will be customizable. Column Types Postgres Type Type to put in schema.ts Resulting JS/TS Type All numeric types number number char, varchar, text, uuid string string bool boolean boolean date, timestamp, timestampz number number json, jsonb json JSONValue enum enumeration string T[] where T is a supported Postgres type (but please see ‚ö†Ô∏è below) json<U[]> where U is the schema.ts type for T V[] where V is the JS/TS type for T Zero will sync arrays to the client, but there is no support for filtering or joining on array elements yet in ZQL. Other Postgres column types aren‚Äôt supported. They will be ignored when replicating (the synced data will be missing that column) and you will get a warning when zero-cache starts up. If your schema has a pg type not listed here, you can support it in Zero by using a trigger to map it to some type that Zero can support. For example if you have a GIS polygon type in the column my_poly polygon, you can use a trigger to map it to a my_poly_json json column. You could either use another trigger to map in the reverse direction to support changes for writes, or you could use a mutator to write to the polygon type directly on the server. Let us know if the lack of a particular column type is hindering your use of Zero. It can likely be added. Column Defaults Default values are allowed in the Postgres schema, but there currently is no way to use them from a Zero app. An insert() mutation requires all columns to be specified, except when columns are nullable (in which case, they default to null). Since there is no way to leave non-nullable columns off the insert on the client, there is no way for PG to apply the default. This is a known issue and will be fixed in the future. IDs It is strongly recommended to use client-generated random strings like uuid, ulid, nanoid, etc for primary keys. This makes optimistic creation and updates much easier. Imagine that the PK of your table is an auto-incrementing integer. If you optimistically create an entity of this type, you will have to give it some ID ‚Äì the type will require it locally, but also if you want to optimistically create relationships to this row you‚Äôll need an ID. You could sync the highest value seen for that table, but there are race conditions and it is possible for that ID to be taken by the time the creation makes it to the server. Your database can resolve this and assign the next ID, but now the relationships you created optimistically will be against the wrong row. Blech. GUIDs makes a lot more sense in synced applications. If your table has a natural key you can use that and it has less problems. But there is still the chance for a conflict. Imagine you are modeling orgs and you choose domainName as the natural key. It is possible for a race to happen and when the creation gets to the server, somebody has already chosen that domain name. In that case, the best thing to do is reject the write and show the user an error. If you want to have a short auto-incrementing numeric ID for UX reasons (i.e., a bug number), that is possible - see this video. Primary Keys Each table synced with Zero must have either a primary key or at least one unique index. This is needed so that Zero can identify rows during sync, to distinguish between an edit and a remove/add. Multi-column primary and foreign keys are supported. Limiting Replication There are two levels of replication to consider with Zero: replicating from Postgres to zero-cache, and from zero-cache to the Zero browser client. zero-cache replication By default, Zero creates a Postgres publication that publishes all tables in the public schema to zero-cache. To limit which tables or columns are replicated to zero-cache, you can create a Postgres publication with the tables and columns you want: CREATE PUBLICATION zero_data FOR TABLE users (col1, col2, col3, ...), issues, comments; Then, specify this publication in the App Publications zero-cache option. Browser client replication You can use Read Permissions to control which rows are synced from the zero-cache replica to actual clients (e.g., web browsers). Currently, Permissions can limit which tables and rows can be replicated to the client. In the near future, you'll also be able to use Permissions to limit syncing individual columns. Until then, you will need to create a publication to control which columns are synced to zero-cache. Schema changes Most Postgres schema changes are supported as is. Two cases require special handling: Adding columns Adding a column with a non-constant DEFAULT value is not supported. This includes any expression with parentheses, as well as the special functions CURRENT_TIME, CURRENT_DATE, and CURRENT_TIMESTAMP (due to a constraint of SQLite). However, the DEFAULT value of an existing column can be changed to any value, including non-constant expressions. To achieve the desired column default: Add the column with no DEFAULT value Backfill the column with desired values Set the column's DEFAULT value BEGIN; ALTER TABLE foo ADD bar ...; -- without a DEFAULT value UPDATE foo SET bar = ...; ALTER TABLE foo ALTER bar SET DEFAULT ...; COMMIT; Changing publications Postgres allows you to change published tables/columns with an ALTER PUBLICATION statement. Zero automatically adjusts the table schemas on the replica, but it does not receive the pre-existing data. To stream the pre-existing data to Zero, make an innocuous UPDATE after adding the tables/columns to the publication: BEGIN; ALTER PUBLICATION zero_data ADD TABLE foo; ALTER TABLE foo REPLICA IDENTITY FULL; UPDATE foo SET id = id; -- For some column \"id\" in \"foo\" ALTER TABLE foo REPLICA IDENTITY DEFAULT; COMMIT; Self-Referential Relationships See schema",
    "headings": [
      {
        "text": "Object Names",
        "id": "object-names"
      },
      {
        "text": "Object Types",
        "id": "object-types"
      },
      {
        "text": "Column Types",
        "id": "column-types"
      },
      {
        "text": "Column Defaults",
        "id": "column-defaults"
      },
      {
        "text": "IDs",
        "id": "ids"
      },
      {
        "text": "Primary Keys",
        "id": "primary-keys"
      },
      {
        "text": "Limiting Replication",
        "id": "limiting-replication"
      },
      {
        "text": "zero-cache replication",
        "id": "zero-cache-replication"
      },
      {
        "text": "Browser client replication",
        "id": "browser-client-replication"
      },
      {
        "text": "Schema changes",
        "id": "schema-changes"
      },
      {
        "text": "Adding columns",
        "id": "adding-columns"
      },
      {
        "text": "Changing publications",
        "id": "changing-publications"
      },
      {
        "text": "Self-Referential Relationships",
        "id": "self-referential-relationships"
      }
    ],
    "kind": "page"
  },
  {
    "id": "205-postgres-support#object-names",
    "title": "Supported Postgres Features",
    "searchTitle": "Object Names",
    "sectionTitle": "Object Names",
    "sectionId": "object-names",
    "url": "/docs/postgres-support",
    "content": "Table and column names must begin with a letter or underscore This can be followed by letters, numbers, underscores, and hyphens Regex: /^[A-Za-z_]+[A-Za-z0-9_-]*$/ The column name _0_version is reserved for internal use",
    "kind": "section"
  },
  {
    "id": "206-postgres-support#object-types",
    "title": "Supported Postgres Features",
    "searchTitle": "Object Types",
    "sectionTitle": "Object Types",
    "sectionId": "object-types",
    "url": "/docs/postgres-support",
    "content": "Tables are synced. Views are not synced. generated as identity columns are synced. In Postgres 18+, generated stored columns are synced. In lower Postgres versions they aren't. Indexes aren't synced per-se, but we do implicitly add indexes to the replica that match the upstream indexes. In the future, this will be customizable.",
    "kind": "section"
  },
  {
    "id": "207-postgres-support#column-types",
    "title": "Supported Postgres Features",
    "searchTitle": "Column Types",
    "sectionTitle": "Column Types",
    "sectionId": "column-types",
    "url": "/docs/postgres-support",
    "content": "Postgres Type Type to put in schema.ts Resulting JS/TS Type All numeric types number number char, varchar, text, uuid string string bool boolean boolean date, timestamp, timestampz number number json, jsonb json JSONValue enum enumeration string T[] where T is a supported Postgres type (but please see ‚ö†Ô∏è below) json<U[]> where U is the schema.ts type for T V[] where V is the JS/TS type for T Zero will sync arrays to the client, but there is no support for filtering or joining on array elements yet in ZQL. Other Postgres column types aren‚Äôt supported. They will be ignored when replicating (the synced data will be missing that column) and you will get a warning when zero-cache starts up. If your schema has a pg type not listed here, you can support it in Zero by using a trigger to map it to some type that Zero can support. For example if you have a GIS polygon type in the column my_poly polygon, you can use a trigger to map it to a my_poly_json json column. You could either use another trigger to map in the reverse direction to support changes for writes, or you could use a mutator to write to the polygon type directly on the server. Let us know if the lack of a particular column type is hindering your use of Zero. It can likely be added.",
    "kind": "section"
  },
  {
    "id": "208-postgres-support#column-defaults",
    "title": "Supported Postgres Features",
    "searchTitle": "Column Defaults",
    "sectionTitle": "Column Defaults",
    "sectionId": "column-defaults",
    "url": "/docs/postgres-support",
    "content": "Default values are allowed in the Postgres schema, but there currently is no way to use them from a Zero app. An insert() mutation requires all columns to be specified, except when columns are nullable (in which case, they default to null). Since there is no way to leave non-nullable columns off the insert on the client, there is no way for PG to apply the default. This is a known issue and will be fixed in the future.",
    "kind": "section"
  },
  {
    "id": "209-postgres-support#ids",
    "title": "Supported Postgres Features",
    "searchTitle": "IDs",
    "sectionTitle": "IDs",
    "sectionId": "ids",
    "url": "/docs/postgres-support",
    "content": "It is strongly recommended to use client-generated random strings like uuid, ulid, nanoid, etc for primary keys. This makes optimistic creation and updates much easier. Imagine that the PK of your table is an auto-incrementing integer. If you optimistically create an entity of this type, you will have to give it some ID ‚Äì the type will require it locally, but also if you want to optimistically create relationships to this row you‚Äôll need an ID. You could sync the highest value seen for that table, but there are race conditions and it is possible for that ID to be taken by the time the creation makes it to the server. Your database can resolve this and assign the next ID, but now the relationships you created optimistically will be against the wrong row. Blech. GUIDs makes a lot more sense in synced applications. If your table has a natural key you can use that and it has less problems. But there is still the chance for a conflict. Imagine you are modeling orgs and you choose domainName as the natural key. It is possible for a race to happen and when the creation gets to the server, somebody has already chosen that domain name. In that case, the best thing to do is reject the write and show the user an error. If you want to have a short auto-incrementing numeric ID for UX reasons (i.e., a bug number), that is possible - see this video.",
    "kind": "section"
  },
  {
    "id": "210-postgres-support#primary-keys",
    "title": "Supported Postgres Features",
    "searchTitle": "Primary Keys",
    "sectionTitle": "Primary Keys",
    "sectionId": "primary-keys",
    "url": "/docs/postgres-support",
    "content": "Each table synced with Zero must have either a primary key or at least one unique index. This is needed so that Zero can identify rows during sync, to distinguish between an edit and a remove/add. Multi-column primary and foreign keys are supported.",
    "kind": "section"
  },
  {
    "id": "211-postgres-support#limiting-replication",
    "title": "Supported Postgres Features",
    "searchTitle": "Limiting Replication",
    "sectionTitle": "Limiting Replication",
    "sectionId": "limiting-replication",
    "url": "/docs/postgres-support",
    "content": "There are two levels of replication to consider with Zero: replicating from Postgres to zero-cache, and from zero-cache to the Zero browser client. zero-cache replication By default, Zero creates a Postgres publication that publishes all tables in the public schema to zero-cache. To limit which tables or columns are replicated to zero-cache, you can create a Postgres publication with the tables and columns you want: CREATE PUBLICATION zero_data FOR TABLE users (col1, col2, col3, ...), issues, comments; Then, specify this publication in the App Publications zero-cache option. Browser client replication You can use Read Permissions to control which rows are synced from the zero-cache replica to actual clients (e.g., web browsers). Currently, Permissions can limit which tables and rows can be replicated to the client. In the near future, you'll also be able to use Permissions to limit syncing individual columns. Until then, you will need to create a publication to control which columns are synced to zero-cache.",
    "kind": "section"
  },
  {
    "id": "212-postgres-support#zero-cache-replication",
    "title": "Supported Postgres Features",
    "searchTitle": "zero-cache replication",
    "sectionTitle": "zero-cache replication",
    "sectionId": "zero-cache-replication",
    "url": "/docs/postgres-support",
    "content": "By default, Zero creates a Postgres publication that publishes all tables in the public schema to zero-cache. To limit which tables or columns are replicated to zero-cache, you can create a Postgres publication with the tables and columns you want: CREATE PUBLICATION zero_data FOR TABLE users (col1, col2, col3, ...), issues, comments; Then, specify this publication in the App Publications zero-cache option.",
    "kind": "section"
  },
  {
    "id": "213-postgres-support#browser-client-replication",
    "title": "Supported Postgres Features",
    "searchTitle": "Browser client replication",
    "sectionTitle": "Browser client replication",
    "sectionId": "browser-client-replication",
    "url": "/docs/postgres-support",
    "content": "You can use Read Permissions to control which rows are synced from the zero-cache replica to actual clients (e.g., web browsers). Currently, Permissions can limit which tables and rows can be replicated to the client. In the near future, you'll also be able to use Permissions to limit syncing individual columns. Until then, you will need to create a publication to control which columns are synced to zero-cache.",
    "kind": "section"
  },
  {
    "id": "214-postgres-support#schema-changes",
    "title": "Supported Postgres Features",
    "searchTitle": "Schema changes",
    "sectionTitle": "Schema changes",
    "sectionId": "schema-changes",
    "url": "/docs/postgres-support",
    "content": "Most Postgres schema changes are supported as is. Two cases require special handling: Adding columns Adding a column with a non-constant DEFAULT value is not supported. This includes any expression with parentheses, as well as the special functions CURRENT_TIME, CURRENT_DATE, and CURRENT_TIMESTAMP (due to a constraint of SQLite). However, the DEFAULT value of an existing column can be changed to any value, including non-constant expressions. To achieve the desired column default: Add the column with no DEFAULT value Backfill the column with desired values Set the column's DEFAULT value BEGIN; ALTER TABLE foo ADD bar ...; -- without a DEFAULT value UPDATE foo SET bar = ...; ALTER TABLE foo ALTER bar SET DEFAULT ...; COMMIT; Changing publications Postgres allows you to change published tables/columns with an ALTER PUBLICATION statement. Zero automatically adjusts the table schemas on the replica, but it does not receive the pre-existing data. To stream the pre-existing data to Zero, make an innocuous UPDATE after adding the tables/columns to the publication: BEGIN; ALTER PUBLICATION zero_data ADD TABLE foo; ALTER TABLE foo REPLICA IDENTITY FULL; UPDATE foo SET id = id; -- For some column \"id\" in \"foo\" ALTER TABLE foo REPLICA IDENTITY DEFAULT; COMMIT;",
    "kind": "section"
  },
  {
    "id": "215-postgres-support#adding-columns",
    "title": "Supported Postgres Features",
    "searchTitle": "Adding columns",
    "sectionTitle": "Adding columns",
    "sectionId": "adding-columns",
    "url": "/docs/postgres-support",
    "content": "Adding a column with a non-constant DEFAULT value is not supported. This includes any expression with parentheses, as well as the special functions CURRENT_TIME, CURRENT_DATE, and CURRENT_TIMESTAMP (due to a constraint of SQLite). However, the DEFAULT value of an existing column can be changed to any value, including non-constant expressions. To achieve the desired column default: Add the column with no DEFAULT value Backfill the column with desired values Set the column's DEFAULT value BEGIN; ALTER TABLE foo ADD bar ...; -- without a DEFAULT value UPDATE foo SET bar = ...; ALTER TABLE foo ALTER bar SET DEFAULT ...; COMMIT;",
    "kind": "section"
  },
  {
    "id": "216-postgres-support#changing-publications",
    "title": "Supported Postgres Features",
    "searchTitle": "Changing publications",
    "sectionTitle": "Changing publications",
    "sectionId": "changing-publications",
    "url": "/docs/postgres-support",
    "content": "Postgres allows you to change published tables/columns with an ALTER PUBLICATION statement. Zero automatically adjusts the table schemas on the replica, but it does not receive the pre-existing data. To stream the pre-existing data to Zero, make an innocuous UPDATE after adding the tables/columns to the publication: BEGIN; ALTER PUBLICATION zero_data ADD TABLE foo; ALTER TABLE foo REPLICA IDENTITY FULL; UPDATE foo SET id = id; -- For some column \"id\" in \"foo\" ALTER TABLE foo REPLICA IDENTITY DEFAULT; COMMIT;",
    "kind": "section"
  },
  {
    "id": "217-postgres-support#self-referential-relationships",
    "title": "Supported Postgres Features",
    "searchTitle": "Self-Referential Relationships",
    "sectionTitle": "Self-Referential Relationships",
    "sectionId": "self-referential-relationships",
    "url": "/docs/postgres-support",
    "content": "See schema",
    "kind": "section"
  },
  {
    "id": "21-queries",
    "title": "Queries",
    "searchTitle": "Queries",
    "url": "/docs/queries",
    "content": "Queries are how you read and sync data with Zero. Here's a simple example: import {defineQueries, defineQuery} from '@rocicorp/zero' import {z} from 'zod' import {zql} from 'schema.ts' export const queries = defineQueries({ postsByAuthor: defineQuery( z.object({authorID: z.string()}), ({args: {authorID}}) => zql.post.where('authorID', authorID) ) }) Architecture A copy of each query exists on both the client and on your server: Often the implementations will be the same, and you can just share their code. This is easy with full-stack frameworks like TanStack Start or Next.js. But the implementations don't have to be the same, or even compute the same result. For example, the server can add extra filters to enforce permissions that the client query does not. Life of a Query When a query is invoked, it initially runs on the client, against the client-side datastore. Any matching data is returned immediately and the user sees instant results. In the background, the name and arguments for the query are sent to zero-cache. Zero-cache calls the queries endpoint on your server to get the ZQL for the query. Your server looks up its implementation of the query, invokes it, and returns the resulting ZQL expression to zero-cache. Zero-cache then runs this ZQL against the server-side data. The initial server result is sent back to the client and the client query updates in response. zero-cache receives updates from Postgres via logical replication. It updates affected queries and sends row changes back to the client, which updates the client query, and the user sees the changes. Defining Queries Basics Create a query using defineQuery. The only required argument is a QueryFn, which must return a ZQL expression: import {zql} from 'schema.ts' const allPostsQueryDef = defineQuery(() => zql.post) Arguments The QueryFn can take a single args parameter. To enable this, pass a validator to defineQuery: import {zql} from 'schema.ts' const postsByAuthor = defineQuery( z.object({authorID: z.string().optional()}), ({args: {authorID}}) => { let q = zql.post if (authorID !== undefined) { q = q.where('authorID', authorID) } return q } ) We use Zod in these examples, but you can use any validation library that implements Standard Schema. Zero queries run on both the client and on your server. In the server case, the parameters come from the client and are untrusted. The validator ensures the data passed to your query is of the expected type. Query Registries The result of defineQuery is a QueryDefinition. By itself this isn't super useful. You need to register it using defineQueries: export const queries = defineQueries({ posts: { all: allPostsQueryDef } }) Typically these are done together in one step: export const queries = defineQueries({ posts: { all: defineQuery(() => zql.post) } }) The result of defineQueries is called a QueryRegistry. Each field in the registry is a callable Query that you can use to read data: import {zero} from 'zero.ts' import {queries} from 'queries.ts' const allPosts = await zero.run(queries.posts.all()) Query Names Each Query has a queryName which is computed by defineQueries. This name is later sent to your server to identify the query to run: console.log(queries.posts.all.queryName) // \"posts.all\" Context Query parameters are supplied by the client application and passed to the server automatically by Zero. This makes them unsuitable for credentials, since the user could modify them. For this reason, Zero queries also support the concept of a context object. Access your context with the ctx parameter to your query: const myPostsQuery = defineQuery(({ctx: {userID}}) => { // User cannot control context.userID, so this safely // restricts the query to the user's own posts. return zql.post.where('authorID', userID) }) queries.ts By convention, all queries for an application are listed in a central queries.ts file. This allows them to be easily used on both the client and server: import {defineQueries, defineQuery} from '@rocicorp/zero' import {z} from 'zod' import {zql} from './schema.ts' export const queries = defineQueries({ posts: { get: defineQuery(z.string(), id => zql.post.where('id', id) ), byAuthor: defineQuery( z.object({ authorID: z.string(), includeDrafts: z.boolean().optional() }), ({args: {authorID, includeDrafts}}) => { let q = zql.post.where('authorID', authorID) if (!includeDrafts) { q = q.where('isDraft', false) } return q } ) } }) You can use as many levels of nesting as you want to organize your queries. As your application grows, you can move queries to different files to keep them organized: // posts.ts export const postQueries = { get: defineQuery(z.string(), id => zql.post.where('id', id) ) // ... } // users.ts export const userQueries = { byRole: defineQuery(z.string(), role => zql.user.where('role', role) ) // ... } // queries.ts import {postQueries} from './posts.ts' import {userQueries} from './users.ts' export const queries = defineQueries({ posts: postQueries, users: userQueries }) Because defineQueries establishes the full name for each query (i.e., posts.get, users.byRole), it should only be used once at the top level of your queries.ts file. Server Setup In order for queries to sync, you must provide an implementation of the query endpoint on your server. zero-cache calls this endpoint to resolve each query to ZQL that it can run. Registering the Endpoint Use ZERO_QUERY_URL to tell zero-cache where to find your query implementation: export ZERO_QUERY_URL=\"http://localhost:3000/api/zero/query\" # run zero-cache, e.g. `npx zero-cache-dev` Implementing the Endpoint You can use the handleQueryRequest and mustGetQuery functions to implement the endpoint. // src/routes/api/zero/query.ts import {createFileRoute} from '@tanstack/react-router' import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from 'queries.ts' import {schema} from 'schema.ts' export const Route = createFileRoute('/api/zero/query')({ server: { handlers: { POST: async ({request}) => { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, request ) return Response.json(result) } } } })// app/api/zero/query/route.ts import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from 'queries.ts' import {schema} from 'schema.ts' export async function POST(req: Request) { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, req ) return Response.json(result) }// src/routes/api/zero/query.ts import type {APIEvent} from '@solidjs/start/server' import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from 'queries.ts' import {schema} from 'schema.ts' export async function POST(event: APIEvent) { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, event.request ) return new Response.json(result) }// api/app.ts import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from 'queries.ts' import {schema} from 'schema.ts' app.post('/api/zero/query', async c => { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, c.req.raw ) return c.json(result) }) handleQueryRequest accepts a standard Request and returns a JSON object which can be serialized and returned by your server framework of choice. mustGetQuery looks up the query in the registry and throws an error if not found. The query.fn function is your query implementation wrapped in the validator you provided. Custom Query URL By default, Zero sends queries to the URL specified in the ZERO_QUERY_URL parameter in the zero-cache config. However you can customize this on a per-client basis. To do so, list multiple comma-separated URLs in ZERO_QUERY_URL: ZERO_QUERY_URL='https://api.example.com/query,https://api.staging.example.com/query' Then choose one of those URLs by passing it to queryURL on the Zero constructor: const zero = new Zero({ schema, queries, queryURL: 'https://api.staging.example.com/query' }) URL Patterns The strings listed in ZERO_QUERY_URL can also be URLPatterns: ZERO_QUERY_URL=\"https://mybranch-*.preview.myapp.com/query\" This queries URL will allow clients to choose URLs like: https://mybranch-aaa.preview.myapp.com/query ‚úÖ https://mybranch-bbb.preview.myapp.com/query ‚úÖ But rejects URLs like: https://preview.myapp.com/query ‚ùå (missing subdomain) https://malicious.com/query ‚ùå (different domain) https://mybranch-123.preview.myapp.com/query/extra ‚ùå (extra path) https://mybranch-123.preview.myapp.com/other ‚ùå (different path) Because URLPattern is a web standard, you can test them right in your browser: For more information, see the URLPattern docs. Running Queries Reactively The most common way to use queries is with the useQuery reactive hooks from the React or SolidJS bindings (or the equivalent low-level API): import {useQuery} from '@rocicorp/zero/react' import {queries} from 'zero/queries.ts' function App() { const [posts] = useQuery(queries.posts.get('user123')) return posts.map(post => ( <div key={post.id}>{post.title}</div> )) }import {useQuery} from '@rocicorp/zero/solid' import {queries} from 'zero/queries.ts' function App() { const [posts] = useQuery(() => queries.posts.get('user123') ) return ( <For each={posts()}> {post => <div key={post.id}>{post.title}</div>} </For> ) }import {queries} from 'zero/queries.ts' import {zero} from 'zero.ts' const postsView = zero.materialize( queries.posts.byAuthorID('user123') ) for (let post of postsView.data) { console.log(post.title) } // updates as the underlying data changes postsView.addListener(posts => { console.log('posts', posts) }) These functions allow you to automatically re-render UI when a query changes. Once You usually want to subscribe to a query in a reactive UI, but every so often you'll need to run a query just once. To do this, use zero.run(): const results = await zero.run( queries.issues.byPriority('high') ) By default, run() only returns results that are currently available on the client. That is, it returns the data that would be given for result.type === 'unknown'. If you want to wait for the server to return results, pass {type: 'complete'} to run: const results = await zero.run( queries.issues.byPriority('high'), {type: 'complete'} ) For Preloading Almost all Zero apps will want to preload some data in order to maximize the feel of instantaneous UI transitions. Because preload queries are often much larger than a screenful of UI, Zero provides a special zero.preload() method to avoid the overhead of materializing the result into JS objects: // Preload a large number of the inbox query results. zero.preload( queries.issues.inbox({ sort: 'created', sortDirection: 'desc', limit: 1000 }) ) Missing Data Because Zero returns local results immediately and server results asynchronously, displaying \"not found\" / 404 UI can be slightly tricky. If you just use a simple existence check, you will often see the 404 UI flicker while the server result loads: const [issue] = useQuery(queries.issues.get('some-id')) // ‚ùå This causes flickering of the UI if (!issue) { return <div>404 Not Found</div> } else { return <div>{issue.title}</div> }const [issue] = useQuery(() => queries.issues.get('some-id') ) return ( <Show when={issue()}> {resolved => ( <Show when={resolved} fallback={<div>404 Not Found</div>} > <div>{resolved.title}</div> </Show> )} </Show> )const postsView = zero.materialize( queries.posts.byAuthorID('user123') ) postsView.addListener(posts => { // ‚ùå This is updated as data comes in console.log('posts', posts) }) To do this correctly, only display the \"not found\" UI when the result type is complete. This way the 404 page is slow but pages with data are still just as fast: const [issue, issueResult] = useQuery( queries.issues.get('some-id') ) if (!issue && issueResult.type === 'complete') { return <div>404 Not Found</div> } if (!issue) { return null } return <div>{issue.title}</div>const [issue, issueResult] = useQuery(() => queries.issues.get('some-id') ) return ( <Switch fallback={null}> <Match when={issue()}> {resolved => <div>{resolved.title}</div>} </Match> <Match when={issueResult().type === 'complete'}> <div>404 Not Found</div> </Match> </Switch> )const postsView = zero.materialize( queries.posts.byAuthorID('user123') ) postsView.addListener((posts, resultType) => { if (resultType === 'complete') { console.log('posts', posts) } }) Partial Data Zero immediately returns the data for a query it has on the client, then falls back to the server for any missing data. Sometimes it's useful to know the difference between these two types of results. To do so, use the result from useQuery: const [issues, issuesResult] = useQuery( queries.issues.inbox() ) if (issuesResult.type === 'complete') { console.log('All data is present') } else { console.log('Some data is missing') }const [issues, issuesResult] = useQuery(() => queries.issues.inbox() ) if (issuesResult().type === 'complete') { console.log('All data is present') } else { console.log('Some data is missing') }const view = zero.materialize(queries.issues.inbox()) view.addListener((issues, resultType) => { if (resultType === 'complete') { console.log('All data is present') } else { console.log('Some data is missing') } }) The possible values of result.type are currently complete and unknown. The complete value is currently only returned when Zero has received the server result. In the future, Zero will be able to return this result type when it knows that all possible data for this query is already available locally. Additionally, we plan to add a prefix result for when the data is known to be a prefix of the complete result. See Consistency for more information. Handling Errors If the queries endpoint throws an application or parse error, zero-cache will report it to the client using the type and error fields on the query details object: const [posts, postsResult] = useQuery( queries.posts.byAuthorID('user123') ) if (postsResult.type === 'error') { return ( <div> Error loading posts: {postsResult.error.message} </div> ) }const [posts, postsResult] = useQuery(() => queries.posts.byAuthorID('user123') ) return ( <Switch> <Match when={postsResult().type === 'error'}> <div> Error loading posts: {postsResult().error.message} </div> </Match> </Switch> )// Materialize a view of a query const postsView = queries.posts .byAuthorID('user123') .materialize() postsView.addListener((posts, resultType, error) => { if (resultType === 'error') { console.error('Error loading posts', error) } }) See Connection Status for how HTTP or network errors from the queries endpoint are handled. Granular Updates You can use the materialize() method to create a view that you can listen to for changes. However, this will only tell you when the view has changed and give you the complete new result. It won't tell you what changed. To know what changed, you can create your own custom View implementation: // Inside the View class // Instead of storing the change, we invoke some callback push(change: Change): void { switch (change.type) { case 'add': this.#onAdd?.(change) break case 'remove': this.#onRemove?.(change) break case 'edit': this.#onEdit?.(change) break case 'child': this.#onChild?.(change) break default: throw new Error(`Unknown change type: ${change['type']}`) } } For examples, see the View implementations in zero-vue or zero-solid. Query Caching Queries can be either active or cached. An active query is one that is currently being used by the application. Cached queries are not currently in use, but continue syncing in case they are needed again soon. Queries are deactivated according to how they were created: For useQuery(), the UI unmounts the component (which calls destroy() under the covers). For preload(), the UI calls cleanup() on the return value of preload(). For run(), queries are automatically deactivated immediately after the result is returned. For materialize() queries, the UI calls destroy() on the view. Additionally when a Zero instance closes, all active queries are automatically deactivated. This also happens when the containing page or script is unloaded. TTLs Each query has a ttl that controls how long it stays cached. If the user closes all tabs for your app, Zero stops running and the time that elapses doesn't count toward any TTLs. You do not need to account for such time when choosing a TTL ‚Äì you only need to account for time your app is running without a query. TTL Defaults In most cases, the default TTL should work well: preload() queries default to ttl:'none', meaning they are not cached at all, and will stop syncing immediately when deactivated. But because preload() queries are typically registered at app startup and never shutdown, and because the ttl clock only ticks while Zero is running, this means that preload queries never get unregistered. Other queries have a default ttl of 5m (five minutes). Setting Different TTLs You can override the default TTL with the ttl parameter: const [user] = useQuery( queries.posts.byAuthorID('user123'), {ttl: '5m'} ) // preload() zero.preload(queries.posts.byAuthorID('user123'), { ttl: '5m' })const [user] = useQuery( () => queries.posts.byAuthorID('user123'), {ttl: '5m'} ) // preload() zero().preload(queries.posts.byAuthorID('user123'), { ttl: '5m' })// run() const user = await zero.run( queries.posts.byAuthorID('user123'), {ttl: '5m'} ) // materialize() const view = zero.materialize( queries.posts.byAuthorID('user123'), {ttl: '5m'} ) // preload() zero.preload(queries.posts.byAuthorID('user123'), { ttl: '5m' }) TTLs up to 10m (ten minutes) are currently supported. The following formats are allowed: Why Zero TTLs are Short Zero queries are not free. Just as in any database, queries consume resources on both the client and server. Memory is used to keep metadata about the query, and disk storage is used to keep the query's current state. We do drop this state after we haven't heard from a client for awhile, but this is only a partial improvement. If the client returns, we have to re-run the query to get the latest data. This means that we do not actually want to keep queries active unless there is a good chance they will be needed again soon. The default Zero TTL values might initially seem too short, but they are designed to work well with the way Zero's TTL clock works and strike a good balance between keeping queries alive long enough to be useful, while not keeping them alive so long that they consume resources unnecessarily. Local-Only Queries It can sometimes be useful to run queries only on the client. For example, to implement typeahead search, it really doesn't make sense to register a query with the server for every single keystroke. Zero doesn't yet have a way to run named queries local-only, but you can run ZQL expressions locally by passing them anywhere a query is supported. For example, to subscribe to a local-only query: // Queries the already synced data for issues, // without syncing more data. const [issues] = useQuery( zql.issue.orderBy('created', 'desc').limit(10) )// Queries the already synced data for issues, // without syncing more data. const [issues] = useQuery(() => zql.issue.orderBy('created', 'desc').limit(10) )// Queries the already synced data for issues, // without syncing more data. const view = z.materialize( zql.issue.orderBy('created', 'desc').limit(10) ) view.addListener(issues => { console.log('issues', issues) }) Custom Server Implementation It is possible to implement the ZERO_QUERY_URL endpoint without using Zero's TypeScript libraries, or even in a different language entirely. The endpoint receives a POST request with a JSON body of the form: type QueriesRequestBody = { id: string name: string args: readonly ReadonlyJSONValue[] }[] And responds with: type QueriesResponseBody = ( | { id: string name: string // See https://github.com/rocicorp/mono/blob/main/packages/zero-protocol/src/ast.ts ast: AST } | { error: 'app' id: string name: string details: ReadonlyJSONValue } | { error: 'zero' id: string name: string details: ReadonlyJSONValue } | { error: 'http' id: string name: string status: number details: ReadonlyJSONValue } )[] Consistency Zero always syncs a consistent partial replica of the backend database to the client. This avoids many common consistency issues that come up in classic web applications. But there are still some consistency issues to be aware of when using Zero. For example, imagine that you have a bug database w/ 10k issues. You preload the first 1k issues sorted by created. The user then does a query of issues assigned to themselves, sorted by created. Among the 1k issues that were preloaded imagine 100 are found that match the query. Since the data we preloaded is in the same order as this query, we are guaranteed that any local results found will be a prefix of the server results. The UX that result is nice: the user will see initial results to the query instantly. If more results are found server-side, those results are guaranteed to sort below the local results. There's no shuffling of results when the server response comes in. Now imagine that the user switches the sort to ‚Äòsort by modified‚Äô. This new query will run locally, and will again find some local matches. But it is now unlikely that the local results found are a prefix of the server results. When the server result comes in, the user will probably see the results shuffle around. To avoid this annoying effect, what you should do in this example is also preload the first 1k issues sorted by modified desc. In general for any query shape you intend to do, you should preload the first n results for that query shape with no filters, in each sort you intend to use. Zero syncs the union of all active queries' results. You don't have to worry about syncing many sorts of the same query when it's likely the results will overlap heavily. In the future, we will be implementing a consistency model that fixes these issues automatically. We will prevent Zero from returning local data when that data is not known to be a prefix of the server result. Once the consistency model is implemented, preloading can be thought of as purely a performance thing, and not required to avoid unsightly flickering.",
    "headings": [
      {
        "text": "Architecture",
        "id": "architecture"
      },
      {
        "text": "Life of a Query",
        "id": "life-of-a-query"
      },
      {
        "text": "Defining Queries",
        "id": "defining-queries"
      },
      {
        "text": "Basics",
        "id": "basics"
      },
      {
        "text": "Arguments",
        "id": "arguments"
      },
      {
        "text": "Query Registries",
        "id": "query-registries"
      },
      {
        "text": "Query Names",
        "id": "query-names"
      },
      {
        "text": "Context",
        "id": "context"
      },
      {
        "text": "queries.ts",
        "id": "queriests"
      },
      {
        "text": "Server Setup",
        "id": "server-setup"
      },
      {
        "text": "Registering the Endpoint",
        "id": "registering-the-endpoint"
      },
      {
        "text": "Implementing the Endpoint",
        "id": "implementing-the-endpoint"
      },
      {
        "text": "Custom Query URL",
        "id": "custom-query-url"
      },
      {
        "text": "URL Patterns",
        "id": "url-patterns"
      },
      {
        "text": "Running Queries",
        "id": "running-queries"
      },
      {
        "text": "Reactively",
        "id": "reactively"
      },
      {
        "text": "Once",
        "id": "once"
      },
      {
        "text": "For Preloading",
        "id": "for-preloading"
      },
      {
        "text": "Missing Data",
        "id": "missing-data"
      },
      {
        "text": "Partial Data",
        "id": "partial-data"
      },
      {
        "text": "Handling Errors",
        "id": "handling-errors"
      },
      {
        "text": "Granular Updates",
        "id": "granular-updates"
      },
      {
        "text": "Query Caching",
        "id": "query-caching"
      },
      {
        "text": "TTLs",
        "id": "ttls"
      },
      {
        "text": "TTL Defaults",
        "id": "ttl-defaults"
      },
      {
        "text": "Setting Different TTLs",
        "id": "setting-different-ttls"
      },
      {
        "text": "Why Zero TTLs are Short",
        "id": "why-zero-ttls-are-short"
      },
      {
        "text": "Local-Only Queries",
        "id": "local-only-queries"
      },
      {
        "text": "Custom Server Implementation",
        "id": "custom-server-implementation"
      },
      {
        "text": "Consistency",
        "id": "consistency"
      }
    ],
    "kind": "page"
  },
  {
    "id": "218-queries#architecture",
    "title": "Queries",
    "searchTitle": "Architecture",
    "sectionTitle": "Architecture",
    "sectionId": "architecture",
    "url": "/docs/queries",
    "content": "A copy of each query exists on both the client and on your server: Often the implementations will be the same, and you can just share their code. This is easy with full-stack frameworks like TanStack Start or Next.js. But the implementations don't have to be the same, or even compute the same result. For example, the server can add extra filters to enforce permissions that the client query does not. Life of a Query When a query is invoked, it initially runs on the client, against the client-side datastore. Any matching data is returned immediately and the user sees instant results. In the background, the name and arguments for the query are sent to zero-cache. Zero-cache calls the queries endpoint on your server to get the ZQL for the query. Your server looks up its implementation of the query, invokes it, and returns the resulting ZQL expression to zero-cache. Zero-cache then runs this ZQL against the server-side data. The initial server result is sent back to the client and the client query updates in response. zero-cache receives updates from Postgres via logical replication. It updates affected queries and sends row changes back to the client, which updates the client query, and the user sees the changes.",
    "kind": "section"
  },
  {
    "id": "219-queries#life-of-a-query",
    "title": "Queries",
    "searchTitle": "Life of a Query",
    "sectionTitle": "Life of a Query",
    "sectionId": "life-of-a-query",
    "url": "/docs/queries",
    "content": "When a query is invoked, it initially runs on the client, against the client-side datastore. Any matching data is returned immediately and the user sees instant results. In the background, the name and arguments for the query are sent to zero-cache. Zero-cache calls the queries endpoint on your server to get the ZQL for the query. Your server looks up its implementation of the query, invokes it, and returns the resulting ZQL expression to zero-cache. Zero-cache then runs this ZQL against the server-side data. The initial server result is sent back to the client and the client query updates in response. zero-cache receives updates from Postgres via logical replication. It updates affected queries and sends row changes back to the client, which updates the client query, and the user sees the changes.",
    "kind": "section"
  },
  {
    "id": "220-queries#defining-queries",
    "title": "Queries",
    "searchTitle": "Defining Queries",
    "sectionTitle": "Defining Queries",
    "sectionId": "defining-queries",
    "url": "/docs/queries",
    "content": "Basics Create a query using defineQuery. The only required argument is a QueryFn, which must return a ZQL expression: import {zql} from 'schema.ts' const allPostsQueryDef = defineQuery(() => zql.post) Arguments The QueryFn can take a single args parameter. To enable this, pass a validator to defineQuery: import {zql} from 'schema.ts' const postsByAuthor = defineQuery( z.object({authorID: z.string().optional()}), ({args: {authorID}}) => { let q = zql.post if (authorID !== undefined) { q = q.where('authorID', authorID) } return q } ) We use Zod in these examples, but you can use any validation library that implements Standard Schema. Zero queries run on both the client and on your server. In the server case, the parameters come from the client and are untrusted. The validator ensures the data passed to your query is of the expected type. Query Registries The result of defineQuery is a QueryDefinition. By itself this isn't super useful. You need to register it using defineQueries: export const queries = defineQueries({ posts: { all: allPostsQueryDef } }) Typically these are done together in one step: export const queries = defineQueries({ posts: { all: defineQuery(() => zql.post) } }) The result of defineQueries is called a QueryRegistry. Each field in the registry is a callable Query that you can use to read data: import {zero} from 'zero.ts' import {queries} from 'queries.ts' const allPosts = await zero.run(queries.posts.all()) Query Names Each Query has a queryName which is computed by defineQueries. This name is later sent to your server to identify the query to run: console.log(queries.posts.all.queryName) // \"posts.all\" Context Query parameters are supplied by the client application and passed to the server automatically by Zero. This makes them unsuitable for credentials, since the user could modify them. For this reason, Zero queries also support the concept of a context object. Access your context with the ctx parameter to your query: const myPostsQuery = defineQuery(({ctx: {userID}}) => { // User cannot control context.userID, so this safely // restricts the query to the user's own posts. return zql.post.where('authorID', userID) }) queries.ts By convention, all queries for an application are listed in a central queries.ts file. This allows them to be easily used on both the client and server: import {defineQueries, defineQuery} from '@rocicorp/zero' import {z} from 'zod' import {zql} from './schema.ts' export const queries = defineQueries({ posts: { get: defineQuery(z.string(), id => zql.post.where('id', id) ), byAuthor: defineQuery( z.object({ authorID: z.string(), includeDrafts: z.boolean().optional() }), ({args: {authorID, includeDrafts}}) => { let q = zql.post.where('authorID', authorID) if (!includeDrafts) { q = q.where('isDraft', false) } return q } ) } }) You can use as many levels of nesting as you want to organize your queries. As your application grows, you can move queries to different files to keep them organized: // posts.ts export const postQueries = { get: defineQuery(z.string(), id => zql.post.where('id', id) ) // ... } // users.ts export const userQueries = { byRole: defineQuery(z.string(), role => zql.user.where('role', role) ) // ... } // queries.ts import {postQueries} from './posts.ts' import {userQueries} from './users.ts' export const queries = defineQueries({ posts: postQueries, users: userQueries }) Because defineQueries establishes the full name for each query (i.e., posts.get, users.byRole), it should only be used once at the top level of your queries.ts file.",
    "kind": "section"
  },
  {
    "id": "221-queries#basics",
    "title": "Queries",
    "searchTitle": "Basics",
    "sectionTitle": "Basics",
    "sectionId": "basics",
    "url": "/docs/queries",
    "content": "Create a query using defineQuery. The only required argument is a QueryFn, which must return a ZQL expression: import {zql} from 'schema.ts' const allPostsQueryDef = defineQuery(() => zql.post)",
    "kind": "section"
  },
  {
    "id": "222-queries#arguments",
    "title": "Queries",
    "searchTitle": "Arguments",
    "sectionTitle": "Arguments",
    "sectionId": "arguments",
    "url": "/docs/queries",
    "content": "The QueryFn can take a single args parameter. To enable this, pass a validator to defineQuery: import {zql} from 'schema.ts' const postsByAuthor = defineQuery( z.object({authorID: z.string().optional()}), ({args: {authorID}}) => { let q = zql.post if (authorID !== undefined) { q = q.where('authorID', authorID) } return q } ) We use Zod in these examples, but you can use any validation library that implements Standard Schema. Zero queries run on both the client and on your server. In the server case, the parameters come from the client and are untrusted. The validator ensures the data passed to your query is of the expected type.",
    "kind": "section"
  },
  {
    "id": "223-queries#query-registries",
    "title": "Queries",
    "searchTitle": "Query Registries",
    "sectionTitle": "Query Registries",
    "sectionId": "query-registries",
    "url": "/docs/queries",
    "content": "The result of defineQuery is a QueryDefinition. By itself this isn't super useful. You need to register it using defineQueries: export const queries = defineQueries({ posts: { all: allPostsQueryDef } }) Typically these are done together in one step: export const queries = defineQueries({ posts: { all: defineQuery(() => zql.post) } }) The result of defineQueries is called a QueryRegistry. Each field in the registry is a callable Query that you can use to read data: import {zero} from 'zero.ts' import {queries} from 'queries.ts' const allPosts = await zero.run(queries.posts.all())",
    "kind": "section"
  },
  {
    "id": "224-queries#query-names",
    "title": "Queries",
    "searchTitle": "Query Names",
    "sectionTitle": "Query Names",
    "sectionId": "query-names",
    "url": "/docs/queries",
    "content": "Each Query has a queryName which is computed by defineQueries. This name is later sent to your server to identify the query to run: console.log(queries.posts.all.queryName) // \"posts.all\"",
    "kind": "section"
  },
  {
    "id": "225-queries#context",
    "title": "Queries",
    "searchTitle": "Context",
    "sectionTitle": "Context",
    "sectionId": "context",
    "url": "/docs/queries",
    "content": "Query parameters are supplied by the client application and passed to the server automatically by Zero. This makes them unsuitable for credentials, since the user could modify them. For this reason, Zero queries also support the concept of a context object. Access your context with the ctx parameter to your query: const myPostsQuery = defineQuery(({ctx: {userID}}) => { // User cannot control context.userID, so this safely // restricts the query to the user's own posts. return zql.post.where('authorID', userID) })",
    "kind": "section"
  },
  {
    "id": "226-queries#queriests",
    "title": "Queries",
    "searchTitle": "queries.ts",
    "sectionTitle": "queries.ts",
    "sectionId": "queriests",
    "url": "/docs/queries",
    "content": "By convention, all queries for an application are listed in a central queries.ts file. This allows them to be easily used on both the client and server: import {defineQueries, defineQuery} from '@rocicorp/zero' import {z} from 'zod' import {zql} from './schema.ts' export const queries = defineQueries({ posts: { get: defineQuery(z.string(), id => zql.post.where('id', id) ), byAuthor: defineQuery( z.object({ authorID: z.string(), includeDrafts: z.boolean().optional() }), ({args: {authorID, includeDrafts}}) => { let q = zql.post.where('authorID', authorID) if (!includeDrafts) { q = q.where('isDraft', false) } return q } ) } }) You can use as many levels of nesting as you want to organize your queries. As your application grows, you can move queries to different files to keep them organized: // posts.ts export const postQueries = { get: defineQuery(z.string(), id => zql.post.where('id', id) ) // ... } // users.ts export const userQueries = { byRole: defineQuery(z.string(), role => zql.user.where('role', role) ) // ... } // queries.ts import {postQueries} from './posts.ts' import {userQueries} from './users.ts' export const queries = defineQueries({ posts: postQueries, users: userQueries }) Because defineQueries establishes the full name for each query (i.e., posts.get, users.byRole), it should only be used once at the top level of your queries.ts file.",
    "kind": "section"
  },
  {
    "id": "227-queries#server-setup",
    "title": "Queries",
    "searchTitle": "Server Setup",
    "sectionTitle": "Server Setup",
    "sectionId": "server-setup",
    "url": "/docs/queries",
    "content": "In order for queries to sync, you must provide an implementation of the query endpoint on your server. zero-cache calls this endpoint to resolve each query to ZQL that it can run. Registering the Endpoint Use ZERO_QUERY_URL to tell zero-cache where to find your query implementation: export ZERO_QUERY_URL=\"http://localhost:3000/api/zero/query\" # run zero-cache, e.g. `npx zero-cache-dev` Implementing the Endpoint You can use the handleQueryRequest and mustGetQuery functions to implement the endpoint. // src/routes/api/zero/query.ts import {createFileRoute} from '@tanstack/react-router' import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from 'queries.ts' import {schema} from 'schema.ts' export const Route = createFileRoute('/api/zero/query')({ server: { handlers: { POST: async ({request}) => { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, request ) return Response.json(result) } } } })// app/api/zero/query/route.ts import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from 'queries.ts' import {schema} from 'schema.ts' export async function POST(req: Request) { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, req ) return Response.json(result) }// src/routes/api/zero/query.ts import type {APIEvent} from '@solidjs/start/server' import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from 'queries.ts' import {schema} from 'schema.ts' export async function POST(event: APIEvent) { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, event.request ) return new Response.json(result) }// api/app.ts import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from 'queries.ts' import {schema} from 'schema.ts' app.post('/api/zero/query', async c => { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, c.req.raw ) return c.json(result) }) handleQueryRequest accepts a standard Request and returns a JSON object which can be serialized and returned by your server framework of choice. mustGetQuery looks up the query in the registry and throws an error if not found. The query.fn function is your query implementation wrapped in the validator you provided. Custom Query URL By default, Zero sends queries to the URL specified in the ZERO_QUERY_URL parameter in the zero-cache config. However you can customize this on a per-client basis. To do so, list multiple comma-separated URLs in ZERO_QUERY_URL: ZERO_QUERY_URL='https://api.example.com/query,https://api.staging.example.com/query' Then choose one of those URLs by passing it to queryURL on the Zero constructor: const zero = new Zero({ schema, queries, queryURL: 'https://api.staging.example.com/query' }) URL Patterns The strings listed in ZERO_QUERY_URL can also be URLPatterns: ZERO_QUERY_URL=\"https://mybranch-*.preview.myapp.com/query\" This queries URL will allow clients to choose URLs like: https://mybranch-aaa.preview.myapp.com/query ‚úÖ https://mybranch-bbb.preview.myapp.com/query ‚úÖ But rejects URLs like: https://preview.myapp.com/query ‚ùå (missing subdomain) https://malicious.com/query ‚ùå (different domain) https://mybranch-123.preview.myapp.com/query/extra ‚ùå (extra path) https://mybranch-123.preview.myapp.com/other ‚ùå (different path) Because URLPattern is a web standard, you can test them right in your browser: For more information, see the URLPattern docs.",
    "kind": "section"
  },
  {
    "id": "228-queries#registering-the-endpoint",
    "title": "Queries",
    "searchTitle": "Registering the Endpoint",
    "sectionTitle": "Registering the Endpoint",
    "sectionId": "registering-the-endpoint",
    "url": "/docs/queries",
    "content": "Use ZERO_QUERY_URL to tell zero-cache where to find your query implementation: export ZERO_QUERY_URL=\"http://localhost:3000/api/zero/query\" # run zero-cache, e.g. `npx zero-cache-dev`",
    "kind": "section"
  },
  {
    "id": "229-queries#implementing-the-endpoint",
    "title": "Queries",
    "searchTitle": "Implementing the Endpoint",
    "sectionTitle": "Implementing the Endpoint",
    "sectionId": "implementing-the-endpoint",
    "url": "/docs/queries",
    "content": "You can use the handleQueryRequest and mustGetQuery functions to implement the endpoint. // src/routes/api/zero/query.ts import {createFileRoute} from '@tanstack/react-router' import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from 'queries.ts' import {schema} from 'schema.ts' export const Route = createFileRoute('/api/zero/query')({ server: { handlers: { POST: async ({request}) => { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, request ) return Response.json(result) } } } })// app/api/zero/query/route.ts import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from 'queries.ts' import {schema} from 'schema.ts' export async function POST(req: Request) { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, req ) return Response.json(result) }// src/routes/api/zero/query.ts import type {APIEvent} from '@solidjs/start/server' import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from 'queries.ts' import {schema} from 'schema.ts' export async function POST(event: APIEvent) { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, event.request ) return new Response.json(result) }// api/app.ts import {handleQueryRequest} from '@rocicorp/zero/server' import {mustGetQuery} from '@rocicorp/zero' import {queries} from 'queries.ts' import {schema} from 'schema.ts' app.post('/api/zero/query', async c => { const result = await handleQueryRequest( (name, args) => { const query = mustGetQuery(queries, name) return query.fn({args, ctx: {userId: 'anon'}}) }, schema, c.req.raw ) return c.json(result) }) handleQueryRequest accepts a standard Request and returns a JSON object which can be serialized and returned by your server framework of choice. mustGetQuery looks up the query in the registry and throws an error if not found. The query.fn function is your query implementation wrapped in the validator you provided.",
    "kind": "section"
  },
  {
    "id": "230-queries#custom-query-url",
    "title": "Queries",
    "searchTitle": "Custom Query URL",
    "sectionTitle": "Custom Query URL",
    "sectionId": "custom-query-url",
    "url": "/docs/queries",
    "content": "By default, Zero sends queries to the URL specified in the ZERO_QUERY_URL parameter in the zero-cache config. However you can customize this on a per-client basis. To do so, list multiple comma-separated URLs in ZERO_QUERY_URL: ZERO_QUERY_URL='https://api.example.com/query,https://api.staging.example.com/query' Then choose one of those URLs by passing it to queryURL on the Zero constructor: const zero = new Zero({ schema, queries, queryURL: 'https://api.staging.example.com/query' })",
    "kind": "section"
  },
  {
    "id": "231-queries#url-patterns",
    "title": "Queries",
    "searchTitle": "URL Patterns",
    "sectionTitle": "URL Patterns",
    "sectionId": "url-patterns",
    "url": "/docs/queries",
    "content": "The strings listed in ZERO_QUERY_URL can also be URLPatterns: ZERO_QUERY_URL=\"https://mybranch-*.preview.myapp.com/query\" This queries URL will allow clients to choose URLs like: https://mybranch-aaa.preview.myapp.com/query ‚úÖ https://mybranch-bbb.preview.myapp.com/query ‚úÖ But rejects URLs like: https://preview.myapp.com/query ‚ùå (missing subdomain) https://malicious.com/query ‚ùå (different domain) https://mybranch-123.preview.myapp.com/query/extra ‚ùå (extra path) https://mybranch-123.preview.myapp.com/other ‚ùå (different path) Because URLPattern is a web standard, you can test them right in your browser: For more information, see the URLPattern docs.",
    "kind": "section"
  },
  {
    "id": "232-queries#running-queries",
    "title": "Queries",
    "searchTitle": "Running Queries",
    "sectionTitle": "Running Queries",
    "sectionId": "running-queries",
    "url": "/docs/queries",
    "content": "Reactively The most common way to use queries is with the useQuery reactive hooks from the React or SolidJS bindings (or the equivalent low-level API): import {useQuery} from '@rocicorp/zero/react' import {queries} from 'zero/queries.ts' function App() { const [posts] = useQuery(queries.posts.get('user123')) return posts.map(post => ( <div key={post.id}>{post.title}</div> )) }import {useQuery} from '@rocicorp/zero/solid' import {queries} from 'zero/queries.ts' function App() { const [posts] = useQuery(() => queries.posts.get('user123') ) return ( <For each={posts()}> {post => <div key={post.id}>{post.title}</div>} </For> ) }import {queries} from 'zero/queries.ts' import {zero} from 'zero.ts' const postsView = zero.materialize( queries.posts.byAuthorID('user123') ) for (let post of postsView.data) { console.log(post.title) } // updates as the underlying data changes postsView.addListener(posts => { console.log('posts', posts) }) These functions allow you to automatically re-render UI when a query changes. Once You usually want to subscribe to a query in a reactive UI, but every so often you'll need to run a query just once. To do this, use zero.run(): const results = await zero.run( queries.issues.byPriority('high') ) By default, run() only returns results that are currently available on the client. That is, it returns the data that would be given for result.type === 'unknown'. If you want to wait for the server to return results, pass {type: 'complete'} to run: const results = await zero.run( queries.issues.byPriority('high'), {type: 'complete'} ) For Preloading Almost all Zero apps will want to preload some data in order to maximize the feel of instantaneous UI transitions. Because preload queries are often much larger than a screenful of UI, Zero provides a special zero.preload() method to avoid the overhead of materializing the result into JS objects: // Preload a large number of the inbox query results. zero.preload( queries.issues.inbox({ sort: 'created', sortDirection: 'desc', limit: 1000 }) )",
    "kind": "section"
  },
  {
    "id": "233-queries#reactively",
    "title": "Queries",
    "searchTitle": "Reactively",
    "sectionTitle": "Reactively",
    "sectionId": "reactively",
    "url": "/docs/queries",
    "content": "The most common way to use queries is with the useQuery reactive hooks from the React or SolidJS bindings (or the equivalent low-level API): import {useQuery} from '@rocicorp/zero/react' import {queries} from 'zero/queries.ts' function App() { const [posts] = useQuery(queries.posts.get('user123')) return posts.map(post => ( <div key={post.id}>{post.title}</div> )) }import {useQuery} from '@rocicorp/zero/solid' import {queries} from 'zero/queries.ts' function App() { const [posts] = useQuery(() => queries.posts.get('user123') ) return ( <For each={posts()}> {post => <div key={post.id}>{post.title}</div>} </For> ) }import {queries} from 'zero/queries.ts' import {zero} from 'zero.ts' const postsView = zero.materialize( queries.posts.byAuthorID('user123') ) for (let post of postsView.data) { console.log(post.title) } // updates as the underlying data changes postsView.addListener(posts => { console.log('posts', posts) }) These functions allow you to automatically re-render UI when a query changes.",
    "kind": "section"
  },
  {
    "id": "234-queries#once",
    "title": "Queries",
    "searchTitle": "Once",
    "sectionTitle": "Once",
    "sectionId": "once",
    "url": "/docs/queries",
    "content": "You usually want to subscribe to a query in a reactive UI, but every so often you'll need to run a query just once. To do this, use zero.run(): const results = await zero.run( queries.issues.byPriority('high') ) By default, run() only returns results that are currently available on the client. That is, it returns the data that would be given for result.type === 'unknown'. If you want to wait for the server to return results, pass {type: 'complete'} to run: const results = await zero.run( queries.issues.byPriority('high'), {type: 'complete'} )",
    "kind": "section"
  },
  {
    "id": "235-queries#for-preloading",
    "title": "Queries",
    "searchTitle": "For Preloading",
    "sectionTitle": "For Preloading",
    "sectionId": "for-preloading",
    "url": "/docs/queries",
    "content": "Almost all Zero apps will want to preload some data in order to maximize the feel of instantaneous UI transitions. Because preload queries are often much larger than a screenful of UI, Zero provides a special zero.preload() method to avoid the overhead of materializing the result into JS objects: // Preload a large number of the inbox query results. zero.preload( queries.issues.inbox({ sort: 'created', sortDirection: 'desc', limit: 1000 }) )",
    "kind": "section"
  },
  {
    "id": "236-queries#missing-data",
    "title": "Queries",
    "searchTitle": "Missing Data",
    "sectionTitle": "Missing Data",
    "sectionId": "missing-data",
    "url": "/docs/queries",
    "content": "Because Zero returns local results immediately and server results asynchronously, displaying \"not found\" / 404 UI can be slightly tricky. If you just use a simple existence check, you will often see the 404 UI flicker while the server result loads: const [issue] = useQuery(queries.issues.get('some-id')) // ‚ùå This causes flickering of the UI if (!issue) { return <div>404 Not Found</div> } else { return <div>{issue.title}</div> }const [issue] = useQuery(() => queries.issues.get('some-id') ) return ( <Show when={issue()}> {resolved => ( <Show when={resolved} fallback={<div>404 Not Found</div>} > <div>{resolved.title}</div> </Show> )} </Show> )const postsView = zero.materialize( queries.posts.byAuthorID('user123') ) postsView.addListener(posts => { // ‚ùå This is updated as data comes in console.log('posts', posts) }) To do this correctly, only display the \"not found\" UI when the result type is complete. This way the 404 page is slow but pages with data are still just as fast: const [issue, issueResult] = useQuery( queries.issues.get('some-id') ) if (!issue && issueResult.type === 'complete') { return <div>404 Not Found</div> } if (!issue) { return null } return <div>{issue.title}</div>const [issue, issueResult] = useQuery(() => queries.issues.get('some-id') ) return ( <Switch fallback={null}> <Match when={issue()}> {resolved => <div>{resolved.title}</div>} </Match> <Match when={issueResult().type === 'complete'}> <div>404 Not Found</div> </Match> </Switch> )const postsView = zero.materialize( queries.posts.byAuthorID('user123') ) postsView.addListener((posts, resultType) => { if (resultType === 'complete') { console.log('posts', posts) } })",
    "kind": "section"
  },
  {
    "id": "237-queries#partial-data",
    "title": "Queries",
    "searchTitle": "Partial Data",
    "sectionTitle": "Partial Data",
    "sectionId": "partial-data",
    "url": "/docs/queries",
    "content": "Zero immediately returns the data for a query it has on the client, then falls back to the server for any missing data. Sometimes it's useful to know the difference between these two types of results. To do so, use the result from useQuery: const [issues, issuesResult] = useQuery( queries.issues.inbox() ) if (issuesResult.type === 'complete') { console.log('All data is present') } else { console.log('Some data is missing') }const [issues, issuesResult] = useQuery(() => queries.issues.inbox() ) if (issuesResult().type === 'complete') { console.log('All data is present') } else { console.log('Some data is missing') }const view = zero.materialize(queries.issues.inbox()) view.addListener((issues, resultType) => { if (resultType === 'complete') { console.log('All data is present') } else { console.log('Some data is missing') } }) The possible values of result.type are currently complete and unknown. The complete value is currently only returned when Zero has received the server result. In the future, Zero will be able to return this result type when it knows that all possible data for this query is already available locally. Additionally, we plan to add a prefix result for when the data is known to be a prefix of the complete result. See Consistency for more information.",
    "kind": "section"
  },
  {
    "id": "238-queries#handling-errors",
    "title": "Queries",
    "searchTitle": "Handling Errors",
    "sectionTitle": "Handling Errors",
    "sectionId": "handling-errors",
    "url": "/docs/queries",
    "content": "If the queries endpoint throws an application or parse error, zero-cache will report it to the client using the type and error fields on the query details object: const [posts, postsResult] = useQuery( queries.posts.byAuthorID('user123') ) if (postsResult.type === 'error') { return ( <div> Error loading posts: {postsResult.error.message} </div> ) }const [posts, postsResult] = useQuery(() => queries.posts.byAuthorID('user123') ) return ( <Switch> <Match when={postsResult().type === 'error'}> <div> Error loading posts: {postsResult().error.message} </div> </Match> </Switch> )// Materialize a view of a query const postsView = queries.posts .byAuthorID('user123') .materialize() postsView.addListener((posts, resultType, error) => { if (resultType === 'error') { console.error('Error loading posts', error) } }) See Connection Status for how HTTP or network errors from the queries endpoint are handled.",
    "kind": "section"
  },
  {
    "id": "239-queries#granular-updates",
    "title": "Queries",
    "searchTitle": "Granular Updates",
    "sectionTitle": "Granular Updates",
    "sectionId": "granular-updates",
    "url": "/docs/queries",
    "content": "You can use the materialize() method to create a view that you can listen to for changes. However, this will only tell you when the view has changed and give you the complete new result. It won't tell you what changed. To know what changed, you can create your own custom View implementation: // Inside the View class // Instead of storing the change, we invoke some callback push(change: Change): void { switch (change.type) { case 'add': this.#onAdd?.(change) break case 'remove': this.#onRemove?.(change) break case 'edit': this.#onEdit?.(change) break case 'child': this.#onChild?.(change) break default: throw new Error(`Unknown change type: ${change['type']}`) } } For examples, see the View implementations in zero-vue or zero-solid.",
    "kind": "section"
  },
  {
    "id": "240-queries#query-caching",
    "title": "Queries",
    "searchTitle": "Query Caching",
    "sectionTitle": "Query Caching",
    "sectionId": "query-caching",
    "url": "/docs/queries",
    "content": "Queries can be either active or cached. An active query is one that is currently being used by the application. Cached queries are not currently in use, but continue syncing in case they are needed again soon. Queries are deactivated according to how they were created: For useQuery(), the UI unmounts the component (which calls destroy() under the covers). For preload(), the UI calls cleanup() on the return value of preload(). For run(), queries are automatically deactivated immediately after the result is returned. For materialize() queries, the UI calls destroy() on the view. Additionally when a Zero instance closes, all active queries are automatically deactivated. This also happens when the containing page or script is unloaded. TTLs Each query has a ttl that controls how long it stays cached. If the user closes all tabs for your app, Zero stops running and the time that elapses doesn't count toward any TTLs. You do not need to account for such time when choosing a TTL ‚Äì you only need to account for time your app is running without a query. TTL Defaults In most cases, the default TTL should work well: preload() queries default to ttl:'none', meaning they are not cached at all, and will stop syncing immediately when deactivated. But because preload() queries are typically registered at app startup and never shutdown, and because the ttl clock only ticks while Zero is running, this means that preload queries never get unregistered. Other queries have a default ttl of 5m (five minutes). Setting Different TTLs You can override the default TTL with the ttl parameter: const [user] = useQuery( queries.posts.byAuthorID('user123'), {ttl: '5m'} ) // preload() zero.preload(queries.posts.byAuthorID('user123'), { ttl: '5m' })const [user] = useQuery( () => queries.posts.byAuthorID('user123'), {ttl: '5m'} ) // preload() zero().preload(queries.posts.byAuthorID('user123'), { ttl: '5m' })// run() const user = await zero.run( queries.posts.byAuthorID('user123'), {ttl: '5m'} ) // materialize() const view = zero.materialize( queries.posts.byAuthorID('user123'), {ttl: '5m'} ) // preload() zero.preload(queries.posts.byAuthorID('user123'), { ttl: '5m' }) TTLs up to 10m (ten minutes) are currently supported. The following formats are allowed: Why Zero TTLs are Short Zero queries are not free. Just as in any database, queries consume resources on both the client and server. Memory is used to keep metadata about the query, and disk storage is used to keep the query's current state. We do drop this state after we haven't heard from a client for awhile, but this is only a partial improvement. If the client returns, we have to re-run the query to get the latest data. This means that we do not actually want to keep queries active unless there is a good chance they will be needed again soon. The default Zero TTL values might initially seem too short, but they are designed to work well with the way Zero's TTL clock works and strike a good balance between keeping queries alive long enough to be useful, while not keeping them alive so long that they consume resources unnecessarily.",
    "kind": "section"
  },
  {
    "id": "241-queries#ttls",
    "title": "Queries",
    "searchTitle": "TTLs",
    "sectionTitle": "TTLs",
    "sectionId": "ttls",
    "url": "/docs/queries",
    "content": "Each query has a ttl that controls how long it stays cached. If the user closes all tabs for your app, Zero stops running and the time that elapses doesn't count toward any TTLs. You do not need to account for such time when choosing a TTL ‚Äì you only need to account for time your app is running without a query.",
    "kind": "section"
  },
  {
    "id": "242-queries#ttl-defaults",
    "title": "Queries",
    "searchTitle": "TTL Defaults",
    "sectionTitle": "TTL Defaults",
    "sectionId": "ttl-defaults",
    "url": "/docs/queries",
    "content": "In most cases, the default TTL should work well: preload() queries default to ttl:'none', meaning they are not cached at all, and will stop syncing immediately when deactivated. But because preload() queries are typically registered at app startup and never shutdown, and because the ttl clock only ticks while Zero is running, this means that preload queries never get unregistered. Other queries have a default ttl of 5m (five minutes).",
    "kind": "section"
  },
  {
    "id": "243-queries#setting-different-ttls",
    "title": "Queries",
    "searchTitle": "Setting Different TTLs",
    "sectionTitle": "Setting Different TTLs",
    "sectionId": "setting-different-ttls",
    "url": "/docs/queries",
    "content": "You can override the default TTL with the ttl parameter: const [user] = useQuery( queries.posts.byAuthorID('user123'), {ttl: '5m'} ) // preload() zero.preload(queries.posts.byAuthorID('user123'), { ttl: '5m' })const [user] = useQuery( () => queries.posts.byAuthorID('user123'), {ttl: '5m'} ) // preload() zero().preload(queries.posts.byAuthorID('user123'), { ttl: '5m' })// run() const user = await zero.run( queries.posts.byAuthorID('user123'), {ttl: '5m'} ) // materialize() const view = zero.materialize( queries.posts.byAuthorID('user123'), {ttl: '5m'} ) // preload() zero.preload(queries.posts.byAuthorID('user123'), { ttl: '5m' }) TTLs up to 10m (ten minutes) are currently supported. The following formats are allowed:",
    "kind": "section"
  },
  {
    "id": "244-queries#why-zero-ttls-are-short",
    "title": "Queries",
    "searchTitle": "Why Zero TTLs are Short",
    "sectionTitle": "Why Zero TTLs are Short",
    "sectionId": "why-zero-ttls-are-short",
    "url": "/docs/queries",
    "content": "Zero queries are not free. Just as in any database, queries consume resources on both the client and server. Memory is used to keep metadata about the query, and disk storage is used to keep the query's current state. We do drop this state after we haven't heard from a client for awhile, but this is only a partial improvement. If the client returns, we have to re-run the query to get the latest data. This means that we do not actually want to keep queries active unless there is a good chance they will be needed again soon. The default Zero TTL values might initially seem too short, but they are designed to work well with the way Zero's TTL clock works and strike a good balance between keeping queries alive long enough to be useful, while not keeping them alive so long that they consume resources unnecessarily.",
    "kind": "section"
  },
  {
    "id": "245-queries#local-only-queries",
    "title": "Queries",
    "searchTitle": "Local-Only Queries",
    "sectionTitle": "Local-Only Queries",
    "sectionId": "local-only-queries",
    "url": "/docs/queries",
    "content": "It can sometimes be useful to run queries only on the client. For example, to implement typeahead search, it really doesn't make sense to register a query with the server for every single keystroke. Zero doesn't yet have a way to run named queries local-only, but you can run ZQL expressions locally by passing them anywhere a query is supported. For example, to subscribe to a local-only query: // Queries the already synced data for issues, // without syncing more data. const [issues] = useQuery( zql.issue.orderBy('created', 'desc').limit(10) )// Queries the already synced data for issues, // without syncing more data. const [issues] = useQuery(() => zql.issue.orderBy('created', 'desc').limit(10) )// Queries the already synced data for issues, // without syncing more data. const view = z.materialize( zql.issue.orderBy('created', 'desc').limit(10) ) view.addListener(issues => { console.log('issues', issues) })",
    "kind": "section"
  },
  {
    "id": "246-queries#custom-server-implementation",
    "title": "Queries",
    "searchTitle": "Custom Server Implementation",
    "sectionTitle": "Custom Server Implementation",
    "sectionId": "custom-server-implementation",
    "url": "/docs/queries",
    "content": "It is possible to implement the ZERO_QUERY_URL endpoint without using Zero's TypeScript libraries, or even in a different language entirely. The endpoint receives a POST request with a JSON body of the form: type QueriesRequestBody = { id: string name: string args: readonly ReadonlyJSONValue[] }[] And responds with: type QueriesResponseBody = ( | { id: string name: string // See https://github.com/rocicorp/mono/blob/main/packages/zero-protocol/src/ast.ts ast: AST } | { error: 'app' id: string name: string details: ReadonlyJSONValue } | { error: 'zero' id: string name: string details: ReadonlyJSONValue } | { error: 'http' id: string name: string status: number details: ReadonlyJSONValue } )[]",
    "kind": "section"
  },
  {
    "id": "247-queries#consistency",
    "title": "Queries",
    "searchTitle": "Consistency",
    "sectionTitle": "Consistency",
    "sectionId": "consistency",
    "url": "/docs/queries",
    "content": "Zero always syncs a consistent partial replica of the backend database to the client. This avoids many common consistency issues that come up in classic web applications. But there are still some consistency issues to be aware of when using Zero. For example, imagine that you have a bug database w/ 10k issues. You preload the first 1k issues sorted by created. The user then does a query of issues assigned to themselves, sorted by created. Among the 1k issues that were preloaded imagine 100 are found that match the query. Since the data we preloaded is in the same order as this query, we are guaranteed that any local results found will be a prefix of the server results. The UX that result is nice: the user will see initial results to the query instantly. If more results are found server-side, those results are guaranteed to sort below the local results. There's no shuffling of results when the server response comes in. Now imagine that the user switches the sort to ‚Äòsort by modified‚Äô. This new query will run locally, and will again find some local matches. But it is now unlikely that the local results found are a prefix of the server results. When the server result comes in, the user will probably see the results shuffle around. To avoid this annoying effect, what you should do in this example is also preload the first 1k issues sorted by modified desc. In general for any query shape you intend to do, you should preload the first n results for that query shape with no filters, in each sort you intend to use. Zero syncs the union of all active queries' results. You don't have to worry about syncing many sorts of the same query when it's likely the results will overlap heavily. In the future, we will be implementing a consistency model that fixes these issues automatically. We will prevent Zero from returning local data when that data is not known to be a prefix of the server result. Once the consistency model is implemented, preloading can be thought of as purely a performance thing, and not required to avoid unsightly flickering.",
    "kind": "section"
  },
  {
    "id": "22-quickstart",
    "title": "Quickstarts",
    "searchTitle": "Quickstarts",
    "url": "/docs/quickstart",
    "content": "Minimal starter apps for Zero with a variety of stacks. hello-zero-solid Simple quickstart for Zero/SolidJS. Stack: Vite/Hono/SolidJS Source: https://github.com/rocicorp/hello-zero-solid Features: Instant reads and writes, realtime updates hello-zero-cf Shows how to use the Zero in a Cloudflare Worker environment. This sample runs Zero in a React/Hono app, within the Cloudflare worker environment. It uses Hono to implement authentication and Zero's API endpoints. It also runs zero-client within a Durable Object and monitors changes to a Zero query. This can be used to do things like send notifications, update external services, etc. Stack: pnpm/Vite/Hono/React/Cloudflare Workers Source: https://github.com/rocicorp/hello-zero-cf hello-zero Quickstart for Zero/React. Stack: Vite/Hono/React Source: https://github.com/rocicorp/hello-zero Docs: Quickstart Features: Instant reads and writes, realtime updates.",
    "headings": [
      {
        "text": "hello-zero-solid",
        "id": "hello-zero-solid"
      },
      {
        "text": "hello-zero-cf",
        "id": "hello-zero-cf"
      },
      {
        "text": "hello-zero",
        "id": "hello-zero"
      }
    ],
    "kind": "page"
  },
  {
    "id": "248-quickstart#hello-zero-solid",
    "title": "Quickstarts",
    "searchTitle": "hello-zero-solid",
    "sectionTitle": "hello-zero-solid",
    "sectionId": "hello-zero-solid",
    "url": "/docs/quickstart",
    "content": "Simple quickstart for Zero/SolidJS. Stack: Vite/Hono/SolidJS Source: https://github.com/rocicorp/hello-zero-solid Features: Instant reads and writes, realtime updates",
    "kind": "section"
  },
  {
    "id": "249-quickstart#hello-zero-cf",
    "title": "Quickstarts",
    "searchTitle": "hello-zero-cf",
    "sectionTitle": "hello-zero-cf",
    "sectionId": "hello-zero-cf",
    "url": "/docs/quickstart",
    "content": "Shows how to use the Zero in a Cloudflare Worker environment. This sample runs Zero in a React/Hono app, within the Cloudflare worker environment. It uses Hono to implement authentication and Zero's API endpoints. It also runs zero-client within a Durable Object and monitors changes to a Zero query. This can be used to do things like send notifications, update external services, etc. Stack: pnpm/Vite/Hono/React/Cloudflare Workers Source: https://github.com/rocicorp/hello-zero-cf",
    "kind": "section"
  },
  {
    "id": "250-quickstart#hello-zero",
    "title": "Quickstarts",
    "searchTitle": "hello-zero",
    "sectionTitle": "hello-zero",
    "sectionId": "hello-zero",
    "url": "/docs/quickstart",
    "content": "Quickstart for Zero/React. Stack: Vite/Hono/React Source: https://github.com/rocicorp/hello-zero Docs: Quickstart Features: Instant reads and writes, realtime updates.",
    "kind": "section"
  },
  {
    "id": "23-react-native",
    "title": "React Native",
    "searchTitle": "React Native",
    "url": "/docs/react-native",
    "content": "Zero has built-in support for React Native and Expo. Usage is identical to React on the web, except you must provide a kvStore implementation. Choose the storage adapter you prefer: import {ZeroProvider} from '@rocicorp/zero/react' import {expoSQLiteStoreProvider} from '@rocicorp/zero/expo-sqlite' export function RootLayout() { return ( <ZeroProvider // ... kvStore={ // On native, use expo-sqlite; on web, use IndexedDB Platform.OS !== 'web' ? expoSQLiteStoreProvider() : 'idb' } > <App /> </ZeroProvider> ) }import {ZeroProvider} from '@rocicorp/zero/react' import {opSQLiteStoreProvider} from '@rocicorp/zero/op-sqlite' export default function RootLayout() { return ( <ZeroProvider // ... kvStore={opSQLiteStoreProvider()} > <App /> </ZeroProvider> ) } For a complete example, see zslack. op-sqlite is much faster than expo-sqlite but does not work with Expo Go. However, it is supported with expo prebuild and development builds.",
    "headings": [],
    "kind": "page"
  },
  {
    "id": "24-react",
    "title": "React",
    "searchTitle": "React",
    "url": "/docs/react",
    "content": "Zero has built-in support for React. Here's what basic usage looks like. Setup Use the ZeroProvider component to setup Zero. It takes care of creating and destroying Zero instances reactively: import {createRoot} from 'react-dom/client' import {ZeroProvider} from '@rocicorp/zero/react' import {useSession} from 'my-session-provider' import App from './App.tsx' import {schema} from 'schema.ts' import {mutators} from 'mutators.ts' const cacheURL = import.meta.env.VITE_PUBLIC_ZERO_CACHE_URL! export default function Root() { const session = useSession() const {userID} = session const context = {userID} return ( <ZeroProvider {...{userID, context, cacheURL, schema, mutators}} > <App /> </ZeroProvider> ) } You can also pass a Zero instance to the ZeroProvider if you want to control the lifecycle of the Zero instance yourself: // ZeroProvider just sets up the context, it doesn't manage // the lifecycle of the Zero instance. <ZeroProvider zero={zero}> <App /> </ZeroProvider> Usage Use useQuery to run queries: import {useQuery} from '@rocicorp/zero/react' import {queries} from 'queries.ts' function Posts() { const [posts] = useQuery( queries.posts.byStatus({status: 'draft'}) ) return ( <> {posts.map(p => ( <div key={p.id}> {p.title} ({p.comments.length} comments) </div> ))} </> ) } Use useZero to get access to the Zero instance, for example to run mutators: import {useZero} from '@rocicorp/zero/react' import {mutators} from 'mutators.ts' function CompleteButton({issueID}: {issueID: string}) { const zero = useZero() const onClick = () => { zero.mutate(mutators.issues.complete({id: issueID})) } return <button onClick={onClick}>Complete Issue</button> } Suspense The useSuspenseQuery hook is exactly like useQuery, except it supports React Suspense. const [issues] = useSuspenseQuery(issueQuery, { suspendUntil: 'complete' // 'partial' or 'complete' }) Use the suspendUntil parameter to control how long to suspend for. The value complete suspends until authoritative results from the server are received. The partial value suspends until any non-empty data is received, or for a empty result that is complete. Examples See the sample directory for more complete React examples.",
    "headings": [
      {
        "text": "Setup",
        "id": "setup"
      },
      {
        "text": "Usage",
        "id": "usage"
      },
      {
        "text": "Suspense",
        "id": "suspense"
      },
      {
        "text": "Examples",
        "id": "examples"
      }
    ],
    "kind": "page"
  },
  {
    "id": "251-react#setup",
    "title": "React",
    "searchTitle": "Setup",
    "sectionTitle": "Setup",
    "sectionId": "setup",
    "url": "/docs/react",
    "content": "Use the ZeroProvider component to setup Zero. It takes care of creating and destroying Zero instances reactively: import {createRoot} from 'react-dom/client' import {ZeroProvider} from '@rocicorp/zero/react' import {useSession} from 'my-session-provider' import App from './App.tsx' import {schema} from 'schema.ts' import {mutators} from 'mutators.ts' const cacheURL = import.meta.env.VITE_PUBLIC_ZERO_CACHE_URL! export default function Root() { const session = useSession() const {userID} = session const context = {userID} return ( <ZeroProvider {...{userID, context, cacheURL, schema, mutators}} > <App /> </ZeroProvider> ) } You can also pass a Zero instance to the ZeroProvider if you want to control the lifecycle of the Zero instance yourself: // ZeroProvider just sets up the context, it doesn't manage // the lifecycle of the Zero instance. <ZeroProvider zero={zero}> <App /> </ZeroProvider>",
    "kind": "section"
  },
  {
    "id": "252-react#usage",
    "title": "React",
    "searchTitle": "Usage",
    "sectionTitle": "Usage",
    "sectionId": "usage",
    "url": "/docs/react",
    "content": "Use useQuery to run queries: import {useQuery} from '@rocicorp/zero/react' import {queries} from 'queries.ts' function Posts() { const [posts] = useQuery( queries.posts.byStatus({status: 'draft'}) ) return ( <> {posts.map(p => ( <div key={p.id}> {p.title} ({p.comments.length} comments) </div> ))} </> ) } Use useZero to get access to the Zero instance, for example to run mutators: import {useZero} from '@rocicorp/zero/react' import {mutators} from 'mutators.ts' function CompleteButton({issueID}: {issueID: string}) { const zero = useZero() const onClick = () => { zero.mutate(mutators.issues.complete({id: issueID})) } return <button onClick={onClick}>Complete Issue</button> }",
    "kind": "section"
  },
  {
    "id": "253-react#suspense",
    "title": "React",
    "searchTitle": "Suspense",
    "sectionTitle": "Suspense",
    "sectionId": "suspense",
    "url": "/docs/react",
    "content": "The useSuspenseQuery hook is exactly like useQuery, except it supports React Suspense. const [issues] = useSuspenseQuery(issueQuery, { suspendUntil: 'complete' // 'partial' or 'complete' }) Use the suspendUntil parameter to control how long to suspend for. The value complete suspends until authoritative results from the server are received. The partial value suspends until any non-empty data is received, or for a empty result that is complete.",
    "kind": "section"
  },
  {
    "id": "254-react#examples",
    "title": "React",
    "searchTitle": "Examples",
    "sectionTitle": "Examples",
    "sectionId": "examples",
    "url": "/docs/react",
    "content": "See the sample directory for more complete React examples.",
    "kind": "section"
  },
  {
    "id": "25-release-notes/0.1",
    "title": "Zero 0.1",
    "searchTitle": "Zero 0.1",
    "url": "/docs/release-notes/0.1",
    "content": "Breaking changes The name of some config keys in zero.config.json changed: upstreamUri ‚Üí upstreamDBConnStr cvrDbUri ‚Üí cvrDBConnStr changeDbUri ‚Üí changeDBConnStr replicaDbFile ‚Üí replicaDBFile Changed default port of zero-cache to 4848 . So your app startup should look like VITE_PUBLIC_SERVER=\"http://localhost:4848\". Features Print a warning to js console when Zero constructor server param is null or undefined zero-cache should now correctly bind to both ipv4 and ipv6 loopback addresses. This should fix the issue where using localhost to connect to zero-cache on some systems did not work. Check for presence of WebSocket early in startup of Zero. Print a clear error to catch people accidentally running Zero under SSR. Fix annoying error in js console in React strict mode from constructing and closing Replicache in quick succession. Source tree fixes These only apply if you were working in the Rocicorp monorepo. Fixed issue where zbugs didn‚Äôt rebuild when zero dependency changed - generally zbugs build normally again The zero binary has the right permissions bit so you don‚Äôt have to chmod u+x after build Remove overloaded name snapshot in use-query.tsx (thanks Scott üôÉ)",
    "headings": [
      {
        "text": "Breaking changes",
        "id": "breaking-changes"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Source tree fixes",
        "id": "source-tree-fixes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "255-release-notes/0.1#breaking-changes",
    "title": "Zero 0.1",
    "searchTitle": "Breaking changes",
    "sectionTitle": "Breaking changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.1",
    "content": "The name of some config keys in zero.config.json changed: upstreamUri ‚Üí upstreamDBConnStr cvrDbUri ‚Üí cvrDBConnStr changeDbUri ‚Üí changeDBConnStr replicaDbFile ‚Üí replicaDBFile Changed default port of zero-cache to 4848 . So your app startup should look like VITE_PUBLIC_SERVER=\"http://localhost:4848\".",
    "kind": "section"
  },
  {
    "id": "256-release-notes/0.1#features",
    "title": "Zero 0.1",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.1",
    "content": "Print a warning to js console when Zero constructor server param is null or undefined zero-cache should now correctly bind to both ipv4 and ipv6 loopback addresses. This should fix the issue where using localhost to connect to zero-cache on some systems did not work. Check for presence of WebSocket early in startup of Zero. Print a clear error to catch people accidentally running Zero under SSR. Fix annoying error in js console in React strict mode from constructing and closing Replicache in quick succession.",
    "kind": "section"
  },
  {
    "id": "257-release-notes/0.1#source-tree-fixes",
    "title": "Zero 0.1",
    "searchTitle": "Source tree fixes",
    "sectionTitle": "Source tree fixes",
    "sectionId": "source-tree-fixes",
    "url": "/docs/release-notes/0.1",
    "content": "These only apply if you were working in the Rocicorp monorepo. Fixed issue where zbugs didn‚Äôt rebuild when zero dependency changed - generally zbugs build normally again The zero binary has the right permissions bit so you don‚Äôt have to chmod u+x after build Remove overloaded name snapshot in use-query.tsx (thanks Scott üôÉ)",
    "kind": "section"
  },
  {
    "id": "26-release-notes/0.10",
    "title": "Zero 0.10",
    "searchTitle": "Zero 0.10",
    "url": "/docs/release-notes/0.10",
    "content": "Install npm install @rocicorp/zero@0.10 Features None. Fixes Remove top-level await from zero-client. Various logging improvements. Don't throw error when WebSocket unavailable on server. Support building on Windows (running on Windows still doesn't work) Breaking Changes None.",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "258-release-notes/0.10#install",
    "title": "Zero 0.10",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.10",
    "content": "npm install @rocicorp/zero@0.10",
    "kind": "section"
  },
  {
    "id": "259-release-notes/0.10#features",
    "title": "Zero 0.10",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.10",
    "content": "None.",
    "kind": "section"
  },
  {
    "id": "260-release-notes/0.10#fixes",
    "title": "Zero 0.10",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.10",
    "content": "Remove top-level await from zero-client. Various logging improvements. Don't throw error when WebSocket unavailable on server. Support building on Windows (running on Windows still doesn't work)",
    "kind": "section"
  },
  {
    "id": "261-release-notes/0.10#breaking-changes",
    "title": "Zero 0.10",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.10",
    "content": "None.",
    "kind": "section"
  },
  {
    "id": "27-release-notes/0.11",
    "title": "Zero 0.11",
    "searchTitle": "Zero 0.11",
    "url": "/docs/release-notes/0.11",
    "content": "Install npm install @rocicorp/zero@0.11 Features Windows should work a lot better now. Thank you very much to aexylus and Sergio Leon for the testing and contributions here. Support nested property access in JWT auth tokens (docs). Make initial sync configurable (docs). Add query result type to SolidJS (docs) Docker image now contains native amd64 and arm64 binaries. Add storageKey constructor parameter to enable multiple Zero instances for same userID. Fixes Many, many fixes, including: Fix downstream replication of primitive values Fix replication of TRUNCATE messages Fix large storage use for idle pg instances Add runtime sanity checks for when a table is referenced but not synced Fix zero-cache-dev for multitenant Breaking Changes The addition of result types to SolidJS is a breaking API change on SolidJS only. See the changes to hello-zero-solid for upgrade example.",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "262-release-notes/0.11#install",
    "title": "Zero 0.11",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.11",
    "content": "npm install @rocicorp/zero@0.11",
    "kind": "section"
  },
  {
    "id": "263-release-notes/0.11#features",
    "title": "Zero 0.11",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.11",
    "content": "Windows should work a lot better now. Thank you very much to aexylus and Sergio Leon for the testing and contributions here. Support nested property access in JWT auth tokens (docs). Make initial sync configurable (docs). Add query result type to SolidJS (docs) Docker image now contains native amd64 and arm64 binaries. Add storageKey constructor parameter to enable multiple Zero instances for same userID.",
    "kind": "section"
  },
  {
    "id": "264-release-notes/0.11#fixes",
    "title": "Zero 0.11",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.11",
    "content": "Many, many fixes, including: Fix downstream replication of primitive values Fix replication of TRUNCATE messages Fix large storage use for idle pg instances Add runtime sanity checks for when a table is referenced but not synced Fix zero-cache-dev for multitenant",
    "kind": "section"
  },
  {
    "id": "265-release-notes/0.11#breaking-changes",
    "title": "Zero 0.11",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.11",
    "content": "The addition of result types to SolidJS is a breaking API change on SolidJS only. See the changes to hello-zero-solid for upgrade example.",
    "kind": "section"
  },
  {
    "id": "28-release-notes/0.12",
    "title": "Zero 0.12",
    "searchTitle": "Zero 0.12",
    "url": "/docs/release-notes/0.12",
    "content": "Install npm install @rocicorp/zero@0.12 Features Schemas now support circular relationships (docs). Added one() and many() schema helpers to default relationship type (docs). Support for syncing tables without a primary key as long as there is a unique index. This enables Prisma's implicit many-to-many relations (docs). Zero has been confirmed to work with Aurora and Google Cloud SQL (docs) Client bundle size reduced from 55kb to 47kb (-15%). Fixes Windows: zero-cache was spawning emptying terminals and leaving listeners connected on exit. Incorrect warning in zero-cache about enums not being supported. Failure to handle the primary key of Postgres tables changing. Incorrect results when whereExists() is before where() in query (bug). Error: The inferred type of '...' cannot be named without a reference to .... Error: insufficient upstream connections. Several causes of flicker in React. Incorrect values for ResultType when unloading and loading a query quickly (bug). Error: Postgres is missing the column '...' but that column was part of a row. Pointless initial empty render in React when data is already available in memory. Error: Expected string at ... Got array during auth. where() incorrectly allows comparing to null with the = operator (bug). SolidJS: Only call setState once per transaction. Breaking Changes The schema definition syntax has changed to support circular relationships. See the changes to hello-zero and hello-zero-solid for upgrade examples.",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "266-release-notes/0.12#install",
    "title": "Zero 0.12",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.12",
    "content": "npm install @rocicorp/zero@0.12",
    "kind": "section"
  },
  {
    "id": "267-release-notes/0.12#features",
    "title": "Zero 0.12",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.12",
    "content": "Schemas now support circular relationships (docs). Added one() and many() schema helpers to default relationship type (docs). Support for syncing tables without a primary key as long as there is a unique index. This enables Prisma's implicit many-to-many relations (docs). Zero has been confirmed to work with Aurora and Google Cloud SQL (docs) Client bundle size reduced from 55kb to 47kb (-15%).",
    "kind": "section"
  },
  {
    "id": "268-release-notes/0.12#fixes",
    "title": "Zero 0.12",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.12",
    "content": "Windows: zero-cache was spawning emptying terminals and leaving listeners connected on exit. Incorrect warning in zero-cache about enums not being supported. Failure to handle the primary key of Postgres tables changing. Incorrect results when whereExists() is before where() in query (bug). Error: The inferred type of '...' cannot be named without a reference to .... Error: insufficient upstream connections. Several causes of flicker in React. Incorrect values for ResultType when unloading and loading a query quickly (bug). Error: Postgres is missing the column '...' but that column was part of a row. Pointless initial empty render in React when data is already available in memory. Error: Expected string at ... Got array during auth. where() incorrectly allows comparing to null with the = operator (bug). SolidJS: Only call setState once per transaction.",
    "kind": "section"
  },
  {
    "id": "269-release-notes/0.12#breaking-changes",
    "title": "Zero 0.12",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.12",
    "content": "The schema definition syntax has changed to support circular relationships. See the changes to hello-zero and hello-zero-solid for upgrade examples.",
    "kind": "section"
  },
  {
    "id": "29-release-notes/0.13",
    "title": "Zero 0.13",
    "searchTitle": "Zero 0.13",
    "url": "/docs/release-notes/0.13",
    "content": "Install npm install @rocicorp/zero@0.13 Features Multinode deployment for horizontal scalability and zero-downtime deploys (docs). SST Deployment Guide (docs). Plain AWS Deployment Guide (docs). Various exports for external libraries Remove build hash from docker version for consistency with npm (discussion) Fixes Move heartbeat monitoring to separate path, not port Type instantiation is excessively deep and possibly infinite (bug). 20x improvement to whereExists performance (discussion) Breaking Changes Removing the hash from the version is a breaking change if you had scripts relying on that. Moving the heartbeat monitor to a path is a breaking change for deployments that were using that.",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "270-release-notes/0.13#install",
    "title": "Zero 0.13",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.13",
    "content": "npm install @rocicorp/zero@0.13",
    "kind": "section"
  },
  {
    "id": "271-release-notes/0.13#features",
    "title": "Zero 0.13",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.13",
    "content": "Multinode deployment for horizontal scalability and zero-downtime deploys (docs). SST Deployment Guide (docs). Plain AWS Deployment Guide (docs). Various exports for external libraries Remove build hash from docker version for consistency with npm (discussion)",
    "kind": "section"
  },
  {
    "id": "272-release-notes/0.13#fixes",
    "title": "Zero 0.13",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.13",
    "content": "Move heartbeat monitoring to separate path, not port Type instantiation is excessively deep and possibly infinite (bug). 20x improvement to whereExists performance (discussion)",
    "kind": "section"
  },
  {
    "id": "273-release-notes/0.13#breaking-changes",
    "title": "Zero 0.13",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.13",
    "content": "Removing the hash from the version is a breaking change if you had scripts relying on that. Moving the heartbeat monitor to a path is a breaking change for deployments that were using that.",
    "kind": "section"
  },
  {
    "id": "30-release-notes/0.14",
    "title": "Zero 0.14",
    "searchTitle": "Zero 0.14",
    "url": "/docs/release-notes/0.14",
    "content": "Install npm install @rocicorp/zero@0.14 Features Use from() to map column or tables to a different name (docs). Sync from muliple Postgres schemas (docs) Fixes useQuery not working when server unset (bug) Error: \"single output already exists\" in hello-zero-solid (bug) Row helper doesn't work with query having one() (bug) Partitioned Postgres tables not replicating Breaking Changes None.",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "274-release-notes/0.14#install",
    "title": "Zero 0.14",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.14",
    "content": "npm install @rocicorp/zero@0.14",
    "kind": "section"
  },
  {
    "id": "275-release-notes/0.14#features",
    "title": "Zero 0.14",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.14",
    "content": "Use from() to map column or tables to a different name (docs). Sync from muliple Postgres schemas (docs)",
    "kind": "section"
  },
  {
    "id": "276-release-notes/0.14#fixes",
    "title": "Zero 0.14",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.14",
    "content": "useQuery not working when server unset (bug) Error: \"single output already exists\" in hello-zero-solid (bug) Row helper doesn't work with query having one() (bug) Partitioned Postgres tables not replicating",
    "kind": "section"
  },
  {
    "id": "277-release-notes/0.14#breaking-changes",
    "title": "Zero 0.14",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.14",
    "content": "None.",
    "kind": "section"
  },
  {
    "id": "31-release-notes/0.15",
    "title": "Zero 0.15",
    "searchTitle": "Zero 0.15",
    "url": "/docs/release-notes/0.15",
    "content": "Install npm install @rocicorp/zero@0.15 Upgrade Guide This release changes the way that permissions are sent to the server. Before, permissions were sent to the server by setting the ZERO_SCHEMA_JSON or ZERO_SCHEMA_FILE environment variables, which include the permissions. In 0.15, these variables go away and are replaced by a new command: npx zero-deploy-permissions. This command writes the permissions to a new table in the upstream database. This design allows live permission updates, without restarting the server. It also solves problems with max env var size that users were seeing. This release also flips the default permission from allow to deny for all rules. To upgrade your app: See the changes to hello-zero or hello-zero-solid for how to update your permissions. Remove the ZERO_SCHEMA_JSON and ZERO_SCHEMA_FILE environment variables from your setup. They aren't used anymore. Use npx zero-deploy-permissions to deploy permissions when necessary. You can hook this up to your CI to automate it. See the zbugs implementation as an example. Features Live-updating permissions (docs). Permissions now default to deny rather than allow (docs). Fixes Multiple whereExists in same query not working (PR) Allow overlapped mutators (bug) \"Immutable type too deep\" error (PR) Log server version at startup (PR) Eliminate quadratic CVR writes (PR) Handle numeric in the replication stream (PR) Make the auto-reset required error more prominent (PR) Add \"type\":\"module\" recommendation when schema load fails (PR) Throw error if multiple auth options set (PR) Handle NULL characters in JSON columns (PR) Breaking Changes Making permissions deny by default breaks existing apps. To fix add ANYONE_CAN or other appropriate permissions for your tables. See docs. The ZERO_SCHEMA_JSON and ZERO_SCHEMA_FILE environment variables are no longer used. Remove them from your setup and use npx zero-deploy-permissions instead.",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Upgrade Guide",
        "id": "upgrade-guide"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "278-release-notes/0.15#install",
    "title": "Zero 0.15",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.15",
    "content": "npm install @rocicorp/zero@0.15",
    "kind": "section"
  },
  {
    "id": "279-release-notes/0.15#upgrade-guide",
    "title": "Zero 0.15",
    "searchTitle": "Upgrade Guide",
    "sectionTitle": "Upgrade Guide",
    "sectionId": "upgrade-guide",
    "url": "/docs/release-notes/0.15",
    "content": "This release changes the way that permissions are sent to the server. Before, permissions were sent to the server by setting the ZERO_SCHEMA_JSON or ZERO_SCHEMA_FILE environment variables, which include the permissions. In 0.15, these variables go away and are replaced by a new command: npx zero-deploy-permissions. This command writes the permissions to a new table in the upstream database. This design allows live permission updates, without restarting the server. It also solves problems with max env var size that users were seeing. This release also flips the default permission from allow to deny for all rules. To upgrade your app: See the changes to hello-zero or hello-zero-solid for how to update your permissions. Remove the ZERO_SCHEMA_JSON and ZERO_SCHEMA_FILE environment variables from your setup. They aren't used anymore. Use npx zero-deploy-permissions to deploy permissions when necessary. You can hook this up to your CI to automate it. See the zbugs implementation as an example.",
    "kind": "section"
  },
  {
    "id": "280-release-notes/0.15#features",
    "title": "Zero 0.15",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.15",
    "content": "Live-updating permissions (docs). Permissions now default to deny rather than allow (docs).",
    "kind": "section"
  },
  {
    "id": "281-release-notes/0.15#fixes",
    "title": "Zero 0.15",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.15",
    "content": "Multiple whereExists in same query not working (PR) Allow overlapped mutators (bug) \"Immutable type too deep\" error (PR) Log server version at startup (PR) Eliminate quadratic CVR writes (PR) Handle numeric in the replication stream (PR) Make the auto-reset required error more prominent (PR) Add \"type\":\"module\" recommendation when schema load fails (PR) Throw error if multiple auth options set (PR) Handle NULL characters in JSON columns (PR)",
    "kind": "section"
  },
  {
    "id": "282-release-notes/0.15#breaking-changes",
    "title": "Zero 0.15",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.15",
    "content": "Making permissions deny by default breaks existing apps. To fix add ANYONE_CAN or other appropriate permissions for your tables. See docs. The ZERO_SCHEMA_JSON and ZERO_SCHEMA_FILE environment variables are no longer used. Remove them from your setup and use npx zero-deploy-permissions instead.",
    "kind": "section"
  },
  {
    "id": "32-release-notes/0.16",
    "title": "Zero 0.16",
    "searchTitle": "Zero 0.16",
    "url": "/docs/release-notes/0.16",
    "content": "Install npm install @rocicorp/zero@0.16 Upgrading See the upgrade from hello-zero or hello-zero-solid for an example. Features Documented how to use lambdas to deploy permissions in SST, rather than needing CI/CD to have access to Postgres. (doc ‚Äì search for \"permissionsDeployer\"). Added simple debugging logs for read and write permissions (doc). Fixes Improve performance of initial sync about 2x (PR 1, PR 2). IN should allow readonly array arguments (Report, PR). Export ANYONE_CAN_DO_ANYTHING (Report). Fix false-positive in schema change detection (Report, PR). Fix writes of numeric types (Report, PR) Fix bug where litestream was creating way too many files in s3 (PR) Fix memory leak in change-streamer noticeable under high write load (PR) Fix query already registered error (PR) Correctly handle optional booleans (PR) Ignore indexes with unpublished columns (PR) Breaking Changes None.",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Upgrading",
        "id": "upgrading"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "283-release-notes/0.16#install",
    "title": "Zero 0.16",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.16",
    "content": "npm install @rocicorp/zero@0.16",
    "kind": "section"
  },
  {
    "id": "284-release-notes/0.16#upgrading",
    "title": "Zero 0.16",
    "searchTitle": "Upgrading",
    "sectionTitle": "Upgrading",
    "sectionId": "upgrading",
    "url": "/docs/release-notes/0.16",
    "content": "See the upgrade from hello-zero or hello-zero-solid for an example.",
    "kind": "section"
  },
  {
    "id": "285-release-notes/0.16#features",
    "title": "Zero 0.16",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.16",
    "content": "Documented how to use lambdas to deploy permissions in SST, rather than needing CI/CD to have access to Postgres. (doc ‚Äì search for \"permissionsDeployer\"). Added simple debugging logs for read and write permissions (doc).",
    "kind": "section"
  },
  {
    "id": "286-release-notes/0.16#fixes",
    "title": "Zero 0.16",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.16",
    "content": "Improve performance of initial sync about 2x (PR 1, PR 2). IN should allow readonly array arguments (Report, PR). Export ANYONE_CAN_DO_ANYTHING (Report). Fix false-positive in schema change detection (Report, PR). Fix writes of numeric types (Report, PR) Fix bug where litestream was creating way too many files in s3 (PR) Fix memory leak in change-streamer noticeable under high write load (PR) Fix query already registered error (PR) Correctly handle optional booleans (PR) Ignore indexes with unpublished columns (PR)",
    "kind": "section"
  },
  {
    "id": "287-release-notes/0.16#breaking-changes",
    "title": "Zero 0.16",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.16",
    "content": "None.",
    "kind": "section"
  },
  {
    "id": "33-release-notes/0.17",
    "title": "Zero 0.17",
    "searchTitle": "Zero 0.17",
    "url": "/docs/release-notes/0.17",
    "content": "Install npm install @rocicorp/zero@0.17 Upgrading See the upgrade from hello-zero or hello-zero-solid for an example. Features Queries now take an optional ttl argument. This argument backgrounds queries for some time after the app stops using them. Background queries continue syncing so they are instantly ready if the UI re-requests them. The data from background queries is also available to be used by new queries where possible (doc). Structural schema versioning. This is TypeScript, why are we versioning with numbers like cave-people?? We got rid of schemaVersion concept entirely and now determine schema compatibility completely automatically, TS-stylie (doc). Permissions now scoped to \"apps\". You can now have different Zero \"apps\" talking to the same upstream database. Each app gets completely separate configuration and permissions. This should also enable previewing zero-cache (each preview would be its own app). Apps replace the existing \"shard\" concept (doc). Initial replication is over 5x faster, up to about 50MB/second or 15k row/second in our tests. Added warnings for slow hydration in both client and server (doc). auto-reset is now enabled by default for databases that don't support event triggers (doc). Default cvr and change databases to upstream, so that you don't have to specify them in the common case where they are the same as upstream. This docs site now has search! Fixes Certain kinds of many:many joins were causing node already exists assertions Certain kinds of or queries were causing consistency issues Support replica identity full for PostgreSQL tables We now print a stack trace during close at debug level to enable debugging errors where Zero is accessed after close. We now print a warning when IndexedDB is missing rather than throwing. This makes it a little easier to use Zero in SSR setups. We now reset zero-cache implicitly in a few edge cases rather than halting replication. Fixed a deadlock in change-streamer. Breaking Changes query.run() now returns its result via promise. This is required for compatibility with upcoming custom mutators, but also will allow us to wait for server results in the future (though that (still üò¢) doesn't exist yet).",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Upgrading",
        "id": "upgrading"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "288-release-notes/0.17#install",
    "title": "Zero 0.17",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.17",
    "content": "npm install @rocicorp/zero@0.17",
    "kind": "section"
  },
  {
    "id": "289-release-notes/0.17#upgrading",
    "title": "Zero 0.17",
    "searchTitle": "Upgrading",
    "sectionTitle": "Upgrading",
    "sectionId": "upgrading",
    "url": "/docs/release-notes/0.17",
    "content": "See the upgrade from hello-zero or hello-zero-solid for an example.",
    "kind": "section"
  },
  {
    "id": "290-release-notes/0.17#features",
    "title": "Zero 0.17",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.17",
    "content": "Queries now take an optional ttl argument. This argument backgrounds queries for some time after the app stops using them. Background queries continue syncing so they are instantly ready if the UI re-requests them. The data from background queries is also available to be used by new queries where possible (doc). Structural schema versioning. This is TypeScript, why are we versioning with numbers like cave-people?? We got rid of schemaVersion concept entirely and now determine schema compatibility completely automatically, TS-stylie (doc). Permissions now scoped to \"apps\". You can now have different Zero \"apps\" talking to the same upstream database. Each app gets completely separate configuration and permissions. This should also enable previewing zero-cache (each preview would be its own app). Apps replace the existing \"shard\" concept (doc). Initial replication is over 5x faster, up to about 50MB/second or 15k row/second in our tests. Added warnings for slow hydration in both client and server (doc). auto-reset is now enabled by default for databases that don't support event triggers (doc). Default cvr and change databases to upstream, so that you don't have to specify them in the common case where they are the same as upstream. This docs site now has search!",
    "kind": "section"
  },
  {
    "id": "291-release-notes/0.17#fixes",
    "title": "Zero 0.17",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.17",
    "content": "Certain kinds of many:many joins were causing node already exists assertions Certain kinds of or queries were causing consistency issues Support replica identity full for PostgreSQL tables We now print a stack trace during close at debug level to enable debugging errors where Zero is accessed after close. We now print a warning when IndexedDB is missing rather than throwing. This makes it a little easier to use Zero in SSR setups. We now reset zero-cache implicitly in a few edge cases rather than halting replication. Fixed a deadlock in change-streamer.",
    "kind": "section"
  },
  {
    "id": "292-release-notes/0.17#breaking-changes",
    "title": "Zero 0.17",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.17",
    "content": "query.run() now returns its result via promise. This is required for compatibility with upcoming custom mutators, but also will allow us to wait for server results in the future (though that (still üò¢) doesn't exist yet).",
    "kind": "section"
  },
  {
    "id": "34-release-notes/0.18",
    "title": "Zero 0.18",
    "searchTitle": "Zero 0.18",
    "url": "/docs/release-notes/0.18",
    "content": "Install npm install @rocicorp/zero@0.18 Upgrading To try out custom mutators, see the changes to hello-zero-solid. Features Custom Mutators! Finally! Define arbitrary write operations in code (doc). Added inspector API for debugging sync, queries, and client storage (doc). Added analyze-query tool to debug query performance (doc). Added transform-query tool to debug permissions (doc). Added ast-to-zql script to prettify Zero's internal AST format (doc). Fixes Added backpressure to replication-manager to protect against Postgres moving faster than we can push to clients (PR). @rocicorp/zero/advanced has been deprecated. AdvancedQuery got folded into Query and ZeroAdvancedOptions got folded into ZeroOptions (PR). Support ALTER SCHEMA DDL changes (PR) Allow replication-manager to continue running while a new one re-replicates. (PR). Improve replication performance for some schema changes (PR). Make the log level of zero-deploy-permissions configurable (PR) Bind exists to the expression builder (PR) Fix single output already exists error (PR) Fix getBrowserGlobal('window')?.addEventListener not a function in Expo (thanks @andrewcoelho!) (PR). Fix Vue bindings ref counting bug. Bindings no longer need to pass RefCountMap (PR). Fix CVR ownership takeover race conditions (PR). Support REPLICA IDENTITY FULL in degraded-mode pg providers (PR). Handle corrupt sqlite db by re-replicating (PR). Don't send useless pokes to clients that are unchanged (PR). Add limit(1) to queries using a relation that is marked one() (PR). Export UseQueryOptions Breaking Changes None.",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Upgrading",
        "id": "upgrading"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "293-release-notes/0.18#install",
    "title": "Zero 0.18",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.18",
    "content": "npm install @rocicorp/zero@0.18",
    "kind": "section"
  },
  {
    "id": "294-release-notes/0.18#upgrading",
    "title": "Zero 0.18",
    "searchTitle": "Upgrading",
    "sectionTitle": "Upgrading",
    "sectionId": "upgrading",
    "url": "/docs/release-notes/0.18",
    "content": "To try out custom mutators, see the changes to hello-zero-solid.",
    "kind": "section"
  },
  {
    "id": "295-release-notes/0.18#features",
    "title": "Zero 0.18",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.18",
    "content": "Custom Mutators! Finally! Define arbitrary write operations in code (doc). Added inspector API for debugging sync, queries, and client storage (doc). Added analyze-query tool to debug query performance (doc). Added transform-query tool to debug permissions (doc). Added ast-to-zql script to prettify Zero's internal AST format (doc).",
    "kind": "section"
  },
  {
    "id": "296-release-notes/0.18#fixes",
    "title": "Zero 0.18",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.18",
    "content": "Added backpressure to replication-manager to protect against Postgres moving faster than we can push to clients (PR). @rocicorp/zero/advanced has been deprecated. AdvancedQuery got folded into Query and ZeroAdvancedOptions got folded into ZeroOptions (PR). Support ALTER SCHEMA DDL changes (PR) Allow replication-manager to continue running while a new one re-replicates. (PR). Improve replication performance for some schema changes (PR). Make the log level of zero-deploy-permissions configurable (PR) Bind exists to the expression builder (PR) Fix single output already exists error (PR) Fix getBrowserGlobal('window')?.addEventListener not a function in Expo (thanks @andrewcoelho!) (PR). Fix Vue bindings ref counting bug. Bindings no longer need to pass RefCountMap (PR). Fix CVR ownership takeover race conditions (PR). Support REPLICA IDENTITY FULL in degraded-mode pg providers (PR). Handle corrupt sqlite db by re-replicating (PR). Don't send useless pokes to clients that are unchanged (PR). Add limit(1) to queries using a relation that is marked one() (PR). Export UseQueryOptions",
    "kind": "section"
  },
  {
    "id": "297-release-notes/0.18#breaking-changes",
    "title": "Zero 0.18",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.18",
    "content": "None.",
    "kind": "section"
  },
  {
    "id": "35-release-notes/0.19",
    "title": "Zero 0.19",
    "searchTitle": "Zero 0.19",
    "url": "/docs/release-notes/0.19",
    "content": "Install npm install @rocicorp/zero@0.19 Upgrading If you use custom mutators, please see hello-zero-solid for how to update your push endpoint. If you use SolidJS, please switch to createQuery. If you are awaiting z.mutate.foo.bar(), you should switch to await z.mutate.foo.bar().client to be consistent with .server. If you were using a 0.19 canary, the .server property returns error by rejection again (like 0.18 did). Sorry about the thrash here. Features Add a type param to query.run() so it can wait for server results (doc, bug) await z.mutate.foo.bar() is now await z.mutate.foo.bar().client for consistency with .server, old API still works but deprecated (doc) Improve speed of litestream restore by about 7x Increase replication speed when using JSON by about 25% Add options to analyze-query to apply permissions and auth data (doc). Add option to --lazy-startup to zero-cache to delay connecting to upstram until first connection (doc) Add /statz endpoint for getting some health statistics from a running Zero instance (doc) Fixes Support passing Request to PushProccesor.process() (PR) Fix layering in PushProcessor to better support custom db implementations (thanks Erik Munson!) (PR) Fix socket disconnects in GCP (PR) Quote Postgres enum types to preserve casing (report) z2s: Return undefined for empty result set when using query.one() z2s: Allow accessing tables in non-public schemas z2s: Allow tx.foo.update({bar: undefined}) where bar is optional to match client behavior Fix broken replication when updating a key that is part of a unique (but non-PK) index solid: Rename useQuery to createQuery to fit Solid naming conventions (old name deprecated) Resync when publications are missing (PR) Fix missing NOT LIKE in query.where() (PR) Fix timezone shift when writing to timestamp/timestamptz and server is non-UTC timezone (thanks Tom Jenkinson!) (PR) Bound time spent in incremental updates to 1/2 hydration time Fix ttl being off by 1000 in some cases üò¨ (PR) z2s: Relationships nested in a junction relationship were not working correctly (PR) Custom mutators: Due to multitab, client can receive multiple responses for same mutation Fix deadlock that could happen when pushing on a closed websocket (PR) Fix incorrect shutdown under heavy CPU load (thanks Erik Munson!) (PR) Fix case where deletes were getting reverted (thanks for reproduction Marc MacLeod!) (PR) z2s: Incorrect handling of self-join, and not exists not(exists()) is not supported on the client re-auth on 401s returned by push endpoint Added push.queryParams constructor parameter to allow passing query params to the push endpoint (doc) Breaking Changes The structure of setting up a PushProcesor has changed slightly. See push endpoint setup or upgrade guide. Not technically a breaking change from 0.18, but if you were using 0.19 canaries, the .server property returns error by rejection again (like 0.18 did) (doc).",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Upgrading",
        "id": "upgrading"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "298-release-notes/0.19#install",
    "title": "Zero 0.19",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.19",
    "content": "npm install @rocicorp/zero@0.19",
    "kind": "section"
  },
  {
    "id": "299-release-notes/0.19#upgrading",
    "title": "Zero 0.19",
    "searchTitle": "Upgrading",
    "sectionTitle": "Upgrading",
    "sectionId": "upgrading",
    "url": "/docs/release-notes/0.19",
    "content": "If you use custom mutators, please see hello-zero-solid for how to update your push endpoint. If you use SolidJS, please switch to createQuery. If you are awaiting z.mutate.foo.bar(), you should switch to await z.mutate.foo.bar().client to be consistent with .server. If you were using a 0.19 canary, the .server property returns error by rejection again (like 0.18 did). Sorry about the thrash here.",
    "kind": "section"
  },
  {
    "id": "300-release-notes/0.19#features",
    "title": "Zero 0.19",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.19",
    "content": "Add a type param to query.run() so it can wait for server results (doc, bug) await z.mutate.foo.bar() is now await z.mutate.foo.bar().client for consistency with .server, old API still works but deprecated (doc) Improve speed of litestream restore by about 7x Increase replication speed when using JSON by about 25% Add options to analyze-query to apply permissions and auth data (doc). Add option to --lazy-startup to zero-cache to delay connecting to upstram until first connection (doc) Add /statz endpoint for getting some health statistics from a running Zero instance (doc)",
    "kind": "section"
  },
  {
    "id": "301-release-notes/0.19#fixes",
    "title": "Zero 0.19",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.19",
    "content": "Support passing Request to PushProccesor.process() (PR) Fix layering in PushProcessor to better support custom db implementations (thanks Erik Munson!) (PR) Fix socket disconnects in GCP (PR) Quote Postgres enum types to preserve casing (report) z2s: Return undefined for empty result set when using query.one() z2s: Allow accessing tables in non-public schemas z2s: Allow tx.foo.update({bar: undefined}) where bar is optional to match client behavior Fix broken replication when updating a key that is part of a unique (but non-PK) index solid: Rename useQuery to createQuery to fit Solid naming conventions (old name deprecated) Resync when publications are missing (PR) Fix missing NOT LIKE in query.where() (PR) Fix timezone shift when writing to timestamp/timestamptz and server is non-UTC timezone (thanks Tom Jenkinson!) (PR) Bound time spent in incremental updates to 1/2 hydration time Fix ttl being off by 1000 in some cases üò¨ (PR) z2s: Relationships nested in a junction relationship were not working correctly (PR) Custom mutators: Due to multitab, client can receive multiple responses for same mutation Fix deadlock that could happen when pushing on a closed websocket (PR) Fix incorrect shutdown under heavy CPU load (thanks Erik Munson!) (PR) Fix case where deletes were getting reverted (thanks for reproduction Marc MacLeod!) (PR) z2s: Incorrect handling of self-join, and not exists not(exists()) is not supported on the client re-auth on 401s returned by push endpoint Added push.queryParams constructor parameter to allow passing query params to the push endpoint (doc)",
    "kind": "section"
  },
  {
    "id": "302-release-notes/0.19#breaking-changes",
    "title": "Zero 0.19",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.19",
    "content": "The structure of setting up a PushProcesor has changed slightly. See push endpoint setup or upgrade guide. Not technically a breaking change from 0.18, but if you were using 0.19 canaries, the .server property returns error by rejection again (like 0.18 did) (doc).",
    "kind": "section"
  },
  {
    "id": "36-release-notes/0.2",
    "title": "Zero 0.2",
    "searchTitle": "Zero 0.2",
    "url": "/docs/release-notes/0.2",
    "content": "Breaking changes None Features ‚ÄúSkip mode‚Äù: zero-cache now skips columns with unsupported datatypes. A warning is printed out when this happens: This makes it easy to use zero-cache with existing schemas that have columns Zero can‚Äôt handle. You can pair this with Postgres triggers to easily translate unsupported types into something Zero can sync. Zero now supports compound primary keys. You no longer need to include an extraneous id column on the junction tables. Fixes Change the way Zero detects unsupported environments to work in One (and any other supported env). Before, Zero was looking for WebSocket and indexedDB early on, but indexedDB won‚Äôt be present on RN as SQLite will be used. Instead look for indexedDB only at use. Require Node v20 explicitly in package.json to prevent accidentally compiling better-sqlite3 with different Node version than running with. Ensure error messages early in startup get printed out before shutting down in multiprocess mode. Docs Factored out the sample app from the docs into its own Github repo so you can just download it and poke around if you prefer that. Source tree fixes Run zero-cache from source. You no longer have to build zero before running zbugs, it picks up the changes automatically. zbugs Numerous polish/styling fixes Change default to ‚Äòopen‚Äô bugs Add ‚Äòassignee‚Äô field",
    "headings": [
      {
        "text": "Breaking changes",
        "id": "breaking-changes"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Docs",
        "id": "docs"
      },
      {
        "text": "Source tree fixes",
        "id": "source-tree-fixes"
      },
      {
        "text": "zbugs",
        "id": "zbugs"
      }
    ],
    "kind": "page"
  },
  {
    "id": "303-release-notes/0.2#breaking-changes",
    "title": "Zero 0.2",
    "searchTitle": "Breaking changes",
    "sectionTitle": "Breaking changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.2",
    "content": "None",
    "kind": "section"
  },
  {
    "id": "304-release-notes/0.2#features",
    "title": "Zero 0.2",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.2",
    "content": "‚ÄúSkip mode‚Äù: zero-cache now skips columns with unsupported datatypes. A warning is printed out when this happens: This makes it easy to use zero-cache with existing schemas that have columns Zero can‚Äôt handle. You can pair this with Postgres triggers to easily translate unsupported types into something Zero can sync. Zero now supports compound primary keys. You no longer need to include an extraneous id column on the junction tables.",
    "kind": "section"
  },
  {
    "id": "305-release-notes/0.2#fixes",
    "title": "Zero 0.2",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.2",
    "content": "Change the way Zero detects unsupported environments to work in One (and any other supported env). Before, Zero was looking for WebSocket and indexedDB early on, but indexedDB won‚Äôt be present on RN as SQLite will be used. Instead look for indexedDB only at use. Require Node v20 explicitly in package.json to prevent accidentally compiling better-sqlite3 with different Node version than running with. Ensure error messages early in startup get printed out before shutting down in multiprocess mode.",
    "kind": "section"
  },
  {
    "id": "306-release-notes/0.2#docs",
    "title": "Zero 0.2",
    "searchTitle": "Docs",
    "sectionTitle": "Docs",
    "sectionId": "docs",
    "url": "/docs/release-notes/0.2",
    "content": "Factored out the sample app from the docs into its own Github repo so you can just download it and poke around if you prefer that.",
    "kind": "section"
  },
  {
    "id": "307-release-notes/0.2#source-tree-fixes",
    "title": "Zero 0.2",
    "searchTitle": "Source tree fixes",
    "sectionTitle": "Source tree fixes",
    "sectionId": "source-tree-fixes",
    "url": "/docs/release-notes/0.2",
    "content": "Run zero-cache from source. You no longer have to build zero before running zbugs, it picks up the changes automatically.",
    "kind": "section"
  },
  {
    "id": "308-release-notes/0.2#zbugs",
    "title": "Zero 0.2",
    "searchTitle": "zbugs",
    "sectionTitle": "zbugs",
    "sectionId": "zbugs",
    "url": "/docs/release-notes/0.2",
    "content": "Numerous polish/styling fixes Change default to ‚Äòopen‚Äô bugs Add ‚Äòassignee‚Äô field",
    "kind": "section"
  },
  {
    "id": "37-release-notes/0.20",
    "title": "Zero 0.20",
    "searchTitle": "Zero 0.20",
    "url": "/docs/release-notes/0.20",
    "content": "Install npm install @rocicorp/zero@0.20 Upgrading There are two config changes for multinode deployments: Required: Remove view-syncer's ZERO_CHANGE_STREAMER_URI env var and replace it with ZERO_CHANGE_STREAMER_MODE: \"discover\". Optional: Change the ZERO_LITESTREAM_BACKUP_URL env var from being passed to both replication-manager and view-syncer nodes to being passed only to replication-manager. This config is no longer needed by view-syncer (and is ignored by it). See hello-zero for an upgrade example using SST. Additionally, the ZERO_TENANTS_JSON, feature was removed. We do not think anyone was using it, but if you were please reach out to us for options. Features Supabase is now fully supported. After upgrading, you should see that schema changes are incremental and don't reset the replica (docs). Improve performance of single-key reads on client. Scale depends on size of data but 100x improvement is common (PR). Implement short-circuiting for or queries. Because of permissions, one or more branches of or would often be empty, turning the entire or into a full-table scan. 100x improvement on chinook test dataset (PR). Remove DNF conversion. This was intended to make consistency easier in the future, but was resulting in some queries exploding in size (PR, bug). Autodiscovery for replication-manager. view-syncer nodes now find replication-manager using the Postgres changedb database, and no longer need an internal load balancer. See the new ZERO_CHANGE_STREAMER_MODE: \"discover\" config in the deployment docs (PR). Make ZERO_LITESTREAM_BACKUP_URL specific to replication-manager. view-syncer nodes now ignore this config and learn it from replication-manager instead. This makes restarting replication less error-prone (PR, discussion). OpenTelemetry support (docs). Fixes Allow dots in column names (only works with custom mutators) (PR). Fix websocket liveness check to avoid false negatives when busy (PR). Fix unhandled exception in zero-cache when processing query eviction (PR). Keep microsecond precision across timezones (PR). Fix unhandled exception in zero-cache during handleClose (PR). Fix NOT IN in z2s (PR). Mutators: assert provided columns actually exist (PR). Fix ordering of columns in replicated index (PR). Use a shorter keepalive for replication stream for compat with Neon (PR). Allow destructuring where in query.related (PR). Add flow control for large change DB transactions (PR). Fix z2s handling of pg types with params (char, varchar, numeric, etc) (PR). Support from and --schema-path in analyze-query (PR). Breaking Changes The autodiscovery feature for replication-manager is a breaking change for multinode deployments. See the upgrade instructions for details. The ZERO_TENANTS_JSON config was removed ü´ó. The ZERO_INITIAL_SYNC_ROW_BATCH_SIZE config was removed. It is no longer needed because initial sync now adapts to available memory automatically.",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Upgrading",
        "id": "upgrading"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "309-release-notes/0.20#install",
    "title": "Zero 0.20",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.20",
    "content": "npm install @rocicorp/zero@0.20",
    "kind": "section"
  },
  {
    "id": "310-release-notes/0.20#upgrading",
    "title": "Zero 0.20",
    "searchTitle": "Upgrading",
    "sectionTitle": "Upgrading",
    "sectionId": "upgrading",
    "url": "/docs/release-notes/0.20",
    "content": "There are two config changes for multinode deployments: Required: Remove view-syncer's ZERO_CHANGE_STREAMER_URI env var and replace it with ZERO_CHANGE_STREAMER_MODE: \"discover\". Optional: Change the ZERO_LITESTREAM_BACKUP_URL env var from being passed to both replication-manager and view-syncer nodes to being passed only to replication-manager. This config is no longer needed by view-syncer (and is ignored by it). See hello-zero for an upgrade example using SST. Additionally, the ZERO_TENANTS_JSON, feature was removed. We do not think anyone was using it, but if you were please reach out to us for options.",
    "kind": "section"
  },
  {
    "id": "311-release-notes/0.20#features",
    "title": "Zero 0.20",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.20",
    "content": "Supabase is now fully supported. After upgrading, you should see that schema changes are incremental and don't reset the replica (docs). Improve performance of single-key reads on client. Scale depends on size of data but 100x improvement is common (PR). Implement short-circuiting for or queries. Because of permissions, one or more branches of or would often be empty, turning the entire or into a full-table scan. 100x improvement on chinook test dataset (PR). Remove DNF conversion. This was intended to make consistency easier in the future, but was resulting in some queries exploding in size (PR, bug). Autodiscovery for replication-manager. view-syncer nodes now find replication-manager using the Postgres changedb database, and no longer need an internal load balancer. See the new ZERO_CHANGE_STREAMER_MODE: \"discover\" config in the deployment docs (PR). Make ZERO_LITESTREAM_BACKUP_URL specific to replication-manager. view-syncer nodes now ignore this config and learn it from replication-manager instead. This makes restarting replication less error-prone (PR, discussion). OpenTelemetry support (docs).",
    "kind": "section"
  },
  {
    "id": "312-release-notes/0.20#fixes",
    "title": "Zero 0.20",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.20",
    "content": "Allow dots in column names (only works with custom mutators) (PR). Fix websocket liveness check to avoid false negatives when busy (PR). Fix unhandled exception in zero-cache when processing query eviction (PR). Keep microsecond precision across timezones (PR). Fix unhandled exception in zero-cache during handleClose (PR). Fix NOT IN in z2s (PR). Mutators: assert provided columns actually exist (PR). Fix ordering of columns in replicated index (PR). Use a shorter keepalive for replication stream for compat with Neon (PR). Allow destructuring where in query.related (PR). Add flow control for large change DB transactions (PR). Fix z2s handling of pg types with params (char, varchar, numeric, etc) (PR). Support from and --schema-path in analyze-query (PR).",
    "kind": "section"
  },
  {
    "id": "313-release-notes/0.20#breaking-changes",
    "title": "Zero 0.20",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.20",
    "content": "The autodiscovery feature for replication-manager is a breaking change for multinode deployments. See the upgrade instructions for details. The ZERO_TENANTS_JSON config was removed ü´ó. The ZERO_INITIAL_SYNC_ROW_BATCH_SIZE config was removed. It is no longer needed because initial sync now adapts to available memory automatically.",
    "kind": "section"
  },
  {
    "id": "38-release-notes/0.21",
    "title": "Zero 0.21",
    "searchTitle": "Zero 0.21",
    "url": "/docs/release-notes/0.21",
    "content": "Install npm install @rocicorp/zero@0.21 Upgrading There is one breaking change in this release, but we think it is unlikely to affect anyone since the results were wrong already ‚Äì the change just makes the error explicit. See hello-zero for an example of using arrays and the new ZeroProvider features. Features New \"ztunes\" sample using TanStack, Drizzle, Better Auth, and Fly.io (docs). Add initial support for Postgres arrays (docs, bug). Improved React lifecycle management with ZeroProvider (docs, PR). Expose Zero instances automatically at __zero (docs, PR). Add --output-{synced|vended}-rows to analyze-query (PR). Technically a bug fix, but this was so annoying I'm calling it a feature: zero-sqlite3 now correctly supports the up/down arrow keys (commit). Another super annoying fix: logs from zero-cache are now level-colored (PR). Fixes Lazy-load otel. This was causing problems with pnpm (PR). Initial replication is now memory-bounded (PR). Change the way otel starts up in zero-cache-dev to not rely on npx (PR). Use existing --log-slow-hydrate-threshold as the threshold for --query-hydration-stats rather than hardcoded 200ms. Fix race condition starting up in multinode deployments (PR). Avoid site-local IPv6 addresses in auto-discovery (PR). Many z2s fixes found by fuzzing (PRs: 4415, 4416, 4417, 4421, 4422, 4423). Don't load prettier in analyze-query. This was causing problems when prettier config was cjs. (PR). Don't hydrate system relationships in analyze-query. This was causing incorrect results. (PR). Fix memory leaks from not cleaning up pusher and mutagen (PR). Fix handling of invalid websocket requests that were crashing server. (PR). Remove red error text when .env missing (PR). Allow zero-cache to startup without schema file, but print a warning (PR). Log a warning when auth token exceeds max allowed header size (PR). Breaking Changes Using order by and limit in many-to-many relationships now throws an error. It didn't work before but did the wrong thing silently. Now it throws a runtime error. See docs, bug.",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Upgrading",
        "id": "upgrading"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "314-release-notes/0.21#install",
    "title": "Zero 0.21",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.21",
    "content": "npm install @rocicorp/zero@0.21",
    "kind": "section"
  },
  {
    "id": "315-release-notes/0.21#upgrading",
    "title": "Zero 0.21",
    "searchTitle": "Upgrading",
    "sectionTitle": "Upgrading",
    "sectionId": "upgrading",
    "url": "/docs/release-notes/0.21",
    "content": "There is one breaking change in this release, but we think it is unlikely to affect anyone since the results were wrong already ‚Äì the change just makes the error explicit. See hello-zero for an example of using arrays and the new ZeroProvider features.",
    "kind": "section"
  },
  {
    "id": "316-release-notes/0.21#features",
    "title": "Zero 0.21",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.21",
    "content": "New \"ztunes\" sample using TanStack, Drizzle, Better Auth, and Fly.io (docs). Add initial support for Postgres arrays (docs, bug). Improved React lifecycle management with ZeroProvider (docs, PR). Expose Zero instances automatically at __zero (docs, PR). Add --output-{synced|vended}-rows to analyze-query (PR). Technically a bug fix, but this was so annoying I'm calling it a feature: zero-sqlite3 now correctly supports the up/down arrow keys (commit). Another super annoying fix: logs from zero-cache are now level-colored (PR).",
    "kind": "section"
  },
  {
    "id": "317-release-notes/0.21#fixes",
    "title": "Zero 0.21",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.21",
    "content": "Lazy-load otel. This was causing problems with pnpm (PR). Initial replication is now memory-bounded (PR). Change the way otel starts up in zero-cache-dev to not rely on npx (PR). Use existing --log-slow-hydrate-threshold as the threshold for --query-hydration-stats rather than hardcoded 200ms. Fix race condition starting up in multinode deployments (PR). Avoid site-local IPv6 addresses in auto-discovery (PR). Many z2s fixes found by fuzzing (PRs: 4415, 4416, 4417, 4421, 4422, 4423). Don't load prettier in analyze-query. This was causing problems when prettier config was cjs. (PR). Don't hydrate system relationships in analyze-query. This was causing incorrect results. (PR). Fix memory leaks from not cleaning up pusher and mutagen (PR). Fix handling of invalid websocket requests that were crashing server. (PR). Remove red error text when .env missing (PR). Allow zero-cache to startup without schema file, but print a warning (PR). Log a warning when auth token exceeds max allowed header size (PR).",
    "kind": "section"
  },
  {
    "id": "318-release-notes/0.21#breaking-changes",
    "title": "Zero 0.21",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.21",
    "content": "Using order by and limit in many-to-many relationships now throws an error. It didn't work before but did the wrong thing silently. Now it throws a runtime error. See docs, bug.",
    "kind": "section"
  },
  {
    "id": "39-release-notes/0.22",
    "title": "Zero 0.22",
    "searchTitle": "Zero 0.22",
    "url": "/docs/release-notes/0.22",
    "content": "Install npm install @rocicorp/zero@0.22 Upgrading This release simplifies the concept of TTLs in Zero. See hello-zero, hello-zero-solid, or ztunes for example upgrades. This release also adds anonymous telemetry collection. You can opt-out with the DO_NOT_TRACK=1 environment variable. Though we hope you will not, as it helps us improve Zero. How TTLs Used to Work Previously, the TTL of a query simply measured normal \"wall clock time\" from when a query inactivated, including any time the app wasn't running at all. With experience, we realized this was a misfeature because it encouraged the use of very long or infinite TTLs, especially for preload queries. Developers always want preload queries registered, but since they do not know how long the app will be offline, they had no real choice but to use the TTL forever. These infinite TTLs would remain registered even after the app's code changed such that it no longer wanted them, slowing connections and incremental updates to process queries that the app was not even using. Worse, the prevalence of infinite TTLs in turns meant we needed some way to limit the size of the client cache. We introduced the \"client row limit\" for this. But developers would frequently accidentally blow through this limit, causing cache thrash. How TTLs Work Now Now, query TTLs measure \"TTL time\" which only elapses while Zero is running. This means that preload queries usually don't need a TTL at all, since they run the entire time Zero is active. This in turn means we can clamp TTLs to low values, which means queries evict naturally, which means we no longer need the client row limit either. Using New TTLs You don't need to do anything specific to upgrade. Zero will clamp your TTLs at 10m and print a warning. But for best results, please review the new TTL documentation. In particular, see how to set your TTLs (TL;DR: You can often just remove them ‚Äì the defaults usually just work). Features Rework and simplify query TTLs (docs, upgrading). SolidJS bindings are now fine-grained, improving performance (PR). Restore ZERO_CHANGE_STREAMER_URI option (doc, PR). Add useZeroOnline to React and SolidJS bindings (doc). Add ttl to run() (PR). Allow client to specify push.url in the Zero constructor (doc). Add anonymous telemetry collection to zero-cache (doc). Fixes Handle public in aliases, like .from('public.table') (PR). Sorting by columns with null values was incorrect in some cases (PR). Fix fencepost issue editing queries with limits (PR). Fix copy runner to more reliably reuse connections (PR). Queries early in startup could not contain non-latin chars (PR). SolidJS: Export createUseZero (PR). Support parallel rollouts of replication-manager and view-syncer (PR). Fix upgrade path for already replicated array types (PR). Breaking Changes Require Node v22+ (see discussion)",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Upgrading",
        "id": "upgrading"
      },
      {
        "text": "How TTLs Used to Work",
        "id": "how-ttls-used-to-work"
      },
      {
        "text": "How TTLs Work Now",
        "id": "how-ttls-work-now"
      },
      {
        "text": "Using New TTLs",
        "id": "using-new-ttls"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "319-release-notes/0.22#install",
    "title": "Zero 0.22",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.22",
    "content": "npm install @rocicorp/zero@0.22",
    "kind": "section"
  },
  {
    "id": "320-release-notes/0.22#upgrading",
    "title": "Zero 0.22",
    "searchTitle": "Upgrading",
    "sectionTitle": "Upgrading",
    "sectionId": "upgrading",
    "url": "/docs/release-notes/0.22",
    "content": "This release simplifies the concept of TTLs in Zero. See hello-zero, hello-zero-solid, or ztunes for example upgrades. This release also adds anonymous telemetry collection. You can opt-out with the DO_NOT_TRACK=1 environment variable. Though we hope you will not, as it helps us improve Zero. How TTLs Used to Work Previously, the TTL of a query simply measured normal \"wall clock time\" from when a query inactivated, including any time the app wasn't running at all. With experience, we realized this was a misfeature because it encouraged the use of very long or infinite TTLs, especially for preload queries. Developers always want preload queries registered, but since they do not know how long the app will be offline, they had no real choice but to use the TTL forever. These infinite TTLs would remain registered even after the app's code changed such that it no longer wanted them, slowing connections and incremental updates to process queries that the app was not even using. Worse, the prevalence of infinite TTLs in turns meant we needed some way to limit the size of the client cache. We introduced the \"client row limit\" for this. But developers would frequently accidentally blow through this limit, causing cache thrash. How TTLs Work Now Now, query TTLs measure \"TTL time\" which only elapses while Zero is running. This means that preload queries usually don't need a TTL at all, since they run the entire time Zero is active. This in turn means we can clamp TTLs to low values, which means queries evict naturally, which means we no longer need the client row limit either. Using New TTLs You don't need to do anything specific to upgrade. Zero will clamp your TTLs at 10m and print a warning. But for best results, please review the new TTL documentation. In particular, see how to set your TTLs (TL;DR: You can often just remove them ‚Äì the defaults usually just work).",
    "kind": "section"
  },
  {
    "id": "321-release-notes/0.22#how-ttls-used-to-work",
    "title": "Zero 0.22",
    "searchTitle": "How TTLs Used to Work",
    "sectionTitle": "How TTLs Used to Work",
    "sectionId": "how-ttls-used-to-work",
    "url": "/docs/release-notes/0.22",
    "content": "Previously, the TTL of a query simply measured normal \"wall clock time\" from when a query inactivated, including any time the app wasn't running at all. With experience, we realized this was a misfeature because it encouraged the use of very long or infinite TTLs, especially for preload queries. Developers always want preload queries registered, but since they do not know how long the app will be offline, they had no real choice but to use the TTL forever. These infinite TTLs would remain registered even after the app's code changed such that it no longer wanted them, slowing connections and incremental updates to process queries that the app was not even using. Worse, the prevalence of infinite TTLs in turns meant we needed some way to limit the size of the client cache. We introduced the \"client row limit\" for this. But developers would frequently accidentally blow through this limit, causing cache thrash.",
    "kind": "section"
  },
  {
    "id": "322-release-notes/0.22#how-ttls-work-now",
    "title": "Zero 0.22",
    "searchTitle": "How TTLs Work Now",
    "sectionTitle": "How TTLs Work Now",
    "sectionId": "how-ttls-work-now",
    "url": "/docs/release-notes/0.22",
    "content": "Now, query TTLs measure \"TTL time\" which only elapses while Zero is running. This means that preload queries usually don't need a TTL at all, since they run the entire time Zero is active. This in turn means we can clamp TTLs to low values, which means queries evict naturally, which means we no longer need the client row limit either.",
    "kind": "section"
  },
  {
    "id": "323-release-notes/0.22#using-new-ttls",
    "title": "Zero 0.22",
    "searchTitle": "Using New TTLs",
    "sectionTitle": "Using New TTLs",
    "sectionId": "using-new-ttls",
    "url": "/docs/release-notes/0.22",
    "content": "You don't need to do anything specific to upgrade. Zero will clamp your TTLs at 10m and print a warning. But for best results, please review the new TTL documentation. In particular, see how to set your TTLs (TL;DR: You can often just remove them ‚Äì the defaults usually just work).",
    "kind": "section"
  },
  {
    "id": "324-release-notes/0.22#features",
    "title": "Zero 0.22",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.22",
    "content": "Rework and simplify query TTLs (docs, upgrading). SolidJS bindings are now fine-grained, improving performance (PR). Restore ZERO_CHANGE_STREAMER_URI option (doc, PR). Add useZeroOnline to React and SolidJS bindings (doc). Add ttl to run() (PR). Allow client to specify push.url in the Zero constructor (doc). Add anonymous telemetry collection to zero-cache (doc).",
    "kind": "section"
  },
  {
    "id": "325-release-notes/0.22#fixes",
    "title": "Zero 0.22",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.22",
    "content": "Handle public in aliases, like .from('public.table') (PR). Sorting by columns with null values was incorrect in some cases (PR). Fix fencepost issue editing queries with limits (PR). Fix copy runner to more reliably reuse connections (PR). Queries early in startup could not contain non-latin chars (PR). SolidJS: Export createUseZero (PR). Support parallel rollouts of replication-manager and view-syncer (PR). Fix upgrade path for already replicated array types (PR).",
    "kind": "section"
  },
  {
    "id": "326-release-notes/0.22#breaking-changes",
    "title": "Zero 0.22",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.22",
    "content": "Require Node v22+ (see discussion)",
    "kind": "section"
  },
  {
    "id": "40-release-notes/0.23",
    "title": "Zero 0.23",
    "searchTitle": "Zero 0.23",
    "url": "/docs/release-notes/0.23",
    "content": "Install npm install @rocicorp/zero@0.23 Upgrading For example upgrades, see ztunes or hello-zero-solid. Please note the breaking changes below. They are minor this time. To try synced queries, please refer to the docs, or the zslack / hello-zero-solid repos which have sample implementations. We will update the rest of the samples to synced queries and custom mutators in September. Features Synced Queries (previously known as \"custom queries\") (docs). First-class React Native support (thanks Austin and Chase) (docs). New \"zslack\" sample featuring React Native and Synced Queries (doc). PlanetScale for Postgres is now fully supported (docs). Suspense support for React (thanks Lewis) (docs). Flag to disable CRUD mutators (docs). New query metrics in inspector (docs). Fixes Large connection headers should not throw (PR). Remove verbose stats from /statz that were crashing larger instances (PR). Fix zero-sqlite3 in Docker container (PR). GC inactive CVRs (PR). Refuse or halt replication for unsupported replica identities (PR). Disable DB-level statement_timeout for zero-cache connections (PR). Handle resync after partial state drop (PR). Fix logging color output on light terminals (PR). Use TableSource with analyze-query to better match production (PR). zero-cache now logs its version on startup (PR). \"Go to definition\" works on custom mutators if you set enableLegacyMutators: false (bug). zbugs zbugs now supports image uploads! (PR). Breaking Changes ZeroProvider now required in SolidJS, just like in React (docs, PR). The tx parameter in custom mutators now requires an explicit type (PR). The CustomMutatorDefs type no longer accepts template parameters (PR) The delegate() method on the Query class was removed. Use zero.run, zero.preload, or zero.materialize instead (PR).",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Upgrading",
        "id": "upgrading"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "zbugs",
        "id": "zbugs"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "327-release-notes/0.23#install",
    "title": "Zero 0.23",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.23",
    "content": "npm install @rocicorp/zero@0.23",
    "kind": "section"
  },
  {
    "id": "328-release-notes/0.23#upgrading",
    "title": "Zero 0.23",
    "searchTitle": "Upgrading",
    "sectionTitle": "Upgrading",
    "sectionId": "upgrading",
    "url": "/docs/release-notes/0.23",
    "content": "For example upgrades, see ztunes or hello-zero-solid. Please note the breaking changes below. They are minor this time. To try synced queries, please refer to the docs, or the zslack / hello-zero-solid repos which have sample implementations. We will update the rest of the samples to synced queries and custom mutators in September.",
    "kind": "section"
  },
  {
    "id": "329-release-notes/0.23#features",
    "title": "Zero 0.23",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.23",
    "content": "Synced Queries (previously known as \"custom queries\") (docs). First-class React Native support (thanks Austin and Chase) (docs). New \"zslack\" sample featuring React Native and Synced Queries (doc). PlanetScale for Postgres is now fully supported (docs). Suspense support for React (thanks Lewis) (docs). Flag to disable CRUD mutators (docs). New query metrics in inspector (docs).",
    "kind": "section"
  },
  {
    "id": "330-release-notes/0.23#fixes",
    "title": "Zero 0.23",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.23",
    "content": "Large connection headers should not throw (PR). Remove verbose stats from /statz that were crashing larger instances (PR). Fix zero-sqlite3 in Docker container (PR). GC inactive CVRs (PR). Refuse or halt replication for unsupported replica identities (PR). Disable DB-level statement_timeout for zero-cache connections (PR). Handle resync after partial state drop (PR). Fix logging color output on light terminals (PR). Use TableSource with analyze-query to better match production (PR). zero-cache now logs its version on startup (PR). \"Go to definition\" works on custom mutators if you set enableLegacyMutators: false (bug).",
    "kind": "section"
  },
  {
    "id": "331-release-notes/0.23#zbugs",
    "title": "Zero 0.23",
    "searchTitle": "zbugs",
    "sectionTitle": "zbugs",
    "sectionId": "zbugs",
    "url": "/docs/release-notes/0.23",
    "content": "zbugs now supports image uploads! (PR).",
    "kind": "section"
  },
  {
    "id": "332-release-notes/0.23#breaking-changes",
    "title": "Zero 0.23",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.23",
    "content": "ZeroProvider now required in SolidJS, just like in React (docs, PR). The tx parameter in custom mutators now requires an explicit type (PR). The CustomMutatorDefs type no longer accepts template parameters (PR) The delegate() method on the Query class was removed. Use zero.run, zero.preload, or zero.materialize instead (PR).",
    "kind": "section"
  },
  {
    "id": "41-release-notes/0.24",
    "title": "Zero 0.24",
    "searchTitle": "Zero 0.24",
    "url": "/docs/release-notes/0.24",
    "content": "Installation npm install @rocicorp/zero@0.24 Features Join Flipping: Dramatically improves the performance of whereExists() in many common cases. Join flipping is a form of manual query planning that tells Zero the order to evaluate joins. An automatic planner will follow in a future release. Simplified Auth: If you are using custom mutators and synced queries, you no longer need to use JWT auth. You can use opaque tokens or even better, plain old cookies. This even means you can use HTTP-only cookies. In our demo apps this eliminated hundreds of lines of code. It's a big improvement. Full Preview Support: We tried to get this out in 0.23 but it didn't quite make it. You can now dynamically configure Zero with the location of your mutator and query endpoints. This allows easily using preview URLs on platforms like Vercel. Analyze Query from Inspector: No more npx zero-analyze-query or ssh'ing into production. You can analyze any query right from the dev tools of your browser. On by default in every Zero app. Simplified Metrics Output in Inspector: No more .metrics[\"long-name-i-cant-remember\"].quantile(0.99). The most important metrics ‚Äì hydration and update time ‚Äì are right on the query object. Also, you don't have to (await __zero.inspect()).client.queries anymore. It's just __zero.inspector.client.queries(). New Type helpers: QueryResultType and QueryRowType are handy ways to get the type of a query result or one of its rows. Faster storage for React Native via op-sqlite: The op-sqlite bindings are faster than the expo built-in ones. We now provide this built-in to Zero. Brand New Cloudflare Sample: Turned the old hello-zero-do sample into a complete Cloudflare sample using workers for everything. Improved ZQL server-side APIs: More convenient setup, and added node-postgres and Drizzle support. Fixes Expose server errors from get-queries endpoint to client Correctly map primary keys to server names in some edge cases Reduce some OTEL error logging to warn Fix a case of deadlock related to CVR cleanup Fix hang on shutdown under Bun CLI (thanks David!) Fix \"View already exists\" error, Discord Fix rows sometimes missing from startAt queries Improve performance of CVR cleanup Disable statement-level timeouts that were preventing CVR cleanup Fix WAL growth when no connected clients Fix exception with local-only queries and enableLegacyQueries:false Fix inability to use --flags with zero-cache-dev (thanks Alizain!). Reduce ping timeouts during slow queries with better time-slicing Improve handling of ownership changes in the change-streamer Breaking Changes If you are using the push.url param to the Zero constructor, rename it to mutateURL (more info). If you are using the ZERO_PUSH_URL config param, rename it to ZERO_MUTATE_URL (more info). If you are awaiting queries directly without calling run(), like await z.query.foo.where(...), or await tx.query.foo.where(...), add .run() to the end, like await z.query.foo.where(...).run() or await tx.query.foo.where(...).run().. This is most common in custom mutator implementations (more info) If you are building for React Native, change your import path from @rocicorp/zero/react-native to @rocicorp/zero/expo-sqlite (more info) Add a ZERO_ADMIN_PASSWORD env var to your production config. This is now required (more info) Example Upgrades ztunes zslack hello-zero-solid hello-zero ‚Äì Note that we are keeping hello-zero on the legacy query and mutator APIs until they are deprecated, for testing reasons. So this PR doesn't include custom mutator, synced query, or auth-related changes.",
    "headings": [
      {
        "text": "Installation",
        "id": "installation"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      },
      {
        "text": "Example Upgrades",
        "id": "example-upgrades"
      }
    ],
    "kind": "page"
  },
  {
    "id": "333-release-notes/0.24#installation",
    "title": "Zero 0.24",
    "searchTitle": "Installation",
    "sectionTitle": "Installation",
    "sectionId": "installation",
    "url": "/docs/release-notes/0.24",
    "content": "npm install @rocicorp/zero@0.24",
    "kind": "section"
  },
  {
    "id": "334-release-notes/0.24#features",
    "title": "Zero 0.24",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.24",
    "content": "Join Flipping: Dramatically improves the performance of whereExists() in many common cases. Join flipping is a form of manual query planning that tells Zero the order to evaluate joins. An automatic planner will follow in a future release. Simplified Auth: If you are using custom mutators and synced queries, you no longer need to use JWT auth. You can use opaque tokens or even better, plain old cookies. This even means you can use HTTP-only cookies. In our demo apps this eliminated hundreds of lines of code. It's a big improvement. Full Preview Support: We tried to get this out in 0.23 but it didn't quite make it. You can now dynamically configure Zero with the location of your mutator and query endpoints. This allows easily using preview URLs on platforms like Vercel. Analyze Query from Inspector: No more npx zero-analyze-query or ssh'ing into production. You can analyze any query right from the dev tools of your browser. On by default in every Zero app. Simplified Metrics Output in Inspector: No more .metrics[\"long-name-i-cant-remember\"].quantile(0.99). The most important metrics ‚Äì hydration and update time ‚Äì are right on the query object. Also, you don't have to (await __zero.inspect()).client.queries anymore. It's just __zero.inspector.client.queries(). New Type helpers: QueryResultType and QueryRowType are handy ways to get the type of a query result or one of its rows. Faster storage for React Native via op-sqlite: The op-sqlite bindings are faster than the expo built-in ones. We now provide this built-in to Zero. Brand New Cloudflare Sample: Turned the old hello-zero-do sample into a complete Cloudflare sample using workers for everything. Improved ZQL server-side APIs: More convenient setup, and added node-postgres and Drizzle support.",
    "kind": "section"
  },
  {
    "id": "335-release-notes/0.24#fixes",
    "title": "Zero 0.24",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.24",
    "content": "Expose server errors from get-queries endpoint to client Correctly map primary keys to server names in some edge cases Reduce some OTEL error logging to warn Fix a case of deadlock related to CVR cleanup Fix hang on shutdown under Bun CLI (thanks David!) Fix \"View already exists\" error, Discord Fix rows sometimes missing from startAt queries Improve performance of CVR cleanup Disable statement-level timeouts that were preventing CVR cleanup Fix WAL growth when no connected clients Fix exception with local-only queries and enableLegacyQueries:false Fix inability to use --flags with zero-cache-dev (thanks Alizain!). Reduce ping timeouts during slow queries with better time-slicing Improve handling of ownership changes in the change-streamer",
    "kind": "section"
  },
  {
    "id": "336-release-notes/0.24#breaking-changes",
    "title": "Zero 0.24",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.24",
    "content": "If you are using the push.url param to the Zero constructor, rename it to mutateURL (more info). If you are using the ZERO_PUSH_URL config param, rename it to ZERO_MUTATE_URL (more info). If you are awaiting queries directly without calling run(), like await z.query.foo.where(...), or await tx.query.foo.where(...), add .run() to the end, like await z.query.foo.where(...).run() or await tx.query.foo.where(...).run().. This is most common in custom mutator implementations (more info) If you are building for React Native, change your import path from @rocicorp/zero/react-native to @rocicorp/zero/expo-sqlite (more info) Add a ZERO_ADMIN_PASSWORD env var to your production config. This is now required (more info)",
    "kind": "section"
  },
  {
    "id": "337-release-notes/0.24#example-upgrades",
    "title": "Zero 0.24",
    "searchTitle": "Example Upgrades",
    "sectionTitle": "Example Upgrades",
    "sectionId": "example-upgrades",
    "url": "/docs/release-notes/0.24",
    "content": "ztunes zslack hello-zero-solid hello-zero ‚Äì Note that we are keeping hello-zero on the legacy query and mutator APIs until they are deprecated, for testing reasons. So this PR doesn't include custom mutator, synced query, or auth-related changes.",
    "kind": "section"
  },
  {
    "id": "42-release-notes/0.25",
    "title": "Zero 0.25",
    "searchTitle": "Zero 0.25",
    "url": "/docs/release-notes/0.25",
    "content": "Installation npm install @rocicorp/zero@0.25 Overview This release massively overhauls and simplifies the entire Zero API. Conceptually, Zero is now just schemas, queries, and mutators. There are no longer first-class auth or permission systems. There are no longer \"custom mutators\" and \"crud mutators\", or \"synced queries\" and \"zql queries\". All the old APIs have been deprecated, and the docs have been rewritten and simpilfied. Additionally, this release introduces a raft of big new features. To ease the transition, and allow access to new features without a ton of work, you can still use the old APIs via the enableLegacyQueries and enableLegacyMutators flags. Note that support for these older APIs will be removed for the Zero beta early next year. Upgrading We have implemented both \"minimal\" and \"maximal\" sample upgrades. The minimal upgrades show how to update to 0.25 with very little work, by opting into deprecated APIs: ztunes minimal: Minimal React upgrade. hello-zero-solid minimal: Minimal SolidJS upgrade. The maximal upgrades show how to use all the new features. Refer to the commit log of each PR for step-by-step examples of how to upgrade: ztunes maximal: Maximal React upgrade. hello-zero-solid maximal: Maximal SolidJS upgrade. zslack: Maximal React Native upgrade. hello-zero-cf: Maximal Cloudflare upgrade. Additionally, the old hello-zero sample has been updated from the original CRUD mutators and queries all the way to the latest hotness. So if you were still using those APIs, you can use this as a guide to upgrade. Features Query Planning: Zero now automatically plans joins, similar to other databases. You no longer need to specify flip:true to optimize whereExists() calls. Reworked Query DX: Queries now receive a Context automatically ‚Äì you don't have to pass it at the call site. Also, any Standard Schema conforming library can be used for validation and the API has been generally simplified and streamlined. Reworked Mutator DX: The mutator API has been overhauled to match queries. Mutators now also support validation. drizzle-zero and prisma-zero are now first-class: They have moved into the rocicorp org and are now officially supported. Default Schema Type: The Schema type can now be registered as a default with Zero. You can just say useZero() ‚Äì useZero<Schema> is no longer needed. You can also delete your createUseZero() helper if you have one. This registration happens by default if you use the new drizzle-zero or prisma-zero packages. New Connection Status API: Brand new connection status API with overhauled error-handling and reconnection behavior. Mutator promises now return structured errors: The client and server promises now always resolve. In the case of an error, they now return a structured error object. New Install Guide: Rewritten guide to add Zero to an existing project. New zero-out tool: Removes all trace of Zero from Postgres. Support for Generated Columns: In Postgres 18+, generated stored columns are now synced. This is useful for denormalizing data from JSON columns to support filtering in Zero. Performance Queries should be 0.2x to 4x faster, depending on the query. Some highlights: Only cache for duration of fetch in exists: 60% improvement on sparse exists queries. Do not fetch on null constraint: 33% improvement on some queries. Remove join storage: ~25% improvement in queries with related calls. Fixes Cooperative concurrency for queries ZERO_REPLICA_FILE now defaults to zero.db serverURL renamed to cacheURL GET_QUERIES renamed to QUERY throughout Ping timeouts now configurable in constructor and at runtime Add configurable WebSocket compression tx.upsert should not overwrite w/ null on missing value sourceField in schema wasn't strongly typed Avoid unique violations during push caused by history compression Invalid SQL was being generated for arrays Handle connections to a view-syncer that has shut down properly Process pending messages on closed websocket Do not hydrate queries if no clients initialized Inspector.analyzeQuery() wasn't mapping names Add flow control to subscriber catchup Migrate TTL/inactivatedAt to DOUBLE PRECISION with clamping Continuous change-log cleanup for single-node Count SQLite row scans, not what is returned to us Make schema path optional for zero-cache-dev (thanks @mattkinnersley!) Export context types (thanks @awkweb!) Suppress deprecation warnings for unset options Fixed closing expo-sqlite DB before deleting Catch errors from async/pipelined db calls Expliclty set isolation level in CVR transactions Change when/where we check advance progress to catch slow change Fix desync caused by client/server inconsistency in keys used to identify rows Send name/args for named queries to enable auth context Re-transform queries on every reconnect Defer completing orderBy(s) until pipeline build and use client primary key Pipeline-driver was in invalid state after ResetPipelinesSignal error Fix pipeline storage db collision SolidJS connection state not updating (thanks @fezproof) Corrects the error semantics of the mutate callback Delay resolving ViewSyncer#initialized until cvr.clientSchema is ensured Use memo to ensure minimal updates in SolidJS Sort queries by hydrateServer desc Delay the replication-manager handoff for loadbalancer task registration Use zql_root as root table name instead of root (thanks @tjenkinson!) Breaking Changes Despite the large DX update, we tried to make the upgrade as smooth as possible. However, there are a few breaking changes we could not easily avoid. If you do not want to immediately opt-in to the new query and mutator APIs, set enableLegacyQueries and enableLegacyMutators to true in your schema.ts. If you are using the .client or .server promises with custom mutators, these no longer ever reject. They now always resolve with a result type that has success/error cases Mutators now reject while Zero is offline. You may want to use the new Connection Status API to detect this and display custom UI to users. The pattern for reauthentication has changed. The auth parameter is now always a string ‚Äì the function form is no longer supported. Instead, call zero.connection.connect({auth: token}) to reconnect. This was done to simplify the API and support reauth via cookies consistently. The onError feature of Zero has been removed. It was undocumented and only half-working. Please use the new Connection Status API to detect errors and display custom UI to users.",
    "headings": [
      {
        "text": "Installation",
        "id": "installation"
      },
      {
        "text": "Overview",
        "id": "overview"
      },
      {
        "text": "Upgrading",
        "id": "upgrading"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Performance",
        "id": "performance"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "338-release-notes/0.25#installation",
    "title": "Zero 0.25",
    "searchTitle": "Installation",
    "sectionTitle": "Installation",
    "sectionId": "installation",
    "url": "/docs/release-notes/0.25",
    "content": "npm install @rocicorp/zero@0.25",
    "kind": "section"
  },
  {
    "id": "339-release-notes/0.25#overview",
    "title": "Zero 0.25",
    "searchTitle": "Overview",
    "sectionTitle": "Overview",
    "sectionId": "overview",
    "url": "/docs/release-notes/0.25",
    "content": "This release massively overhauls and simplifies the entire Zero API. Conceptually, Zero is now just schemas, queries, and mutators. There are no longer first-class auth or permission systems. There are no longer \"custom mutators\" and \"crud mutators\", or \"synced queries\" and \"zql queries\". All the old APIs have been deprecated, and the docs have been rewritten and simpilfied. Additionally, this release introduces a raft of big new features. To ease the transition, and allow access to new features without a ton of work, you can still use the old APIs via the enableLegacyQueries and enableLegacyMutators flags. Note that support for these older APIs will be removed for the Zero beta early next year.",
    "kind": "section"
  },
  {
    "id": "340-release-notes/0.25#upgrading",
    "title": "Zero 0.25",
    "searchTitle": "Upgrading",
    "sectionTitle": "Upgrading",
    "sectionId": "upgrading",
    "url": "/docs/release-notes/0.25",
    "content": "We have implemented both \"minimal\" and \"maximal\" sample upgrades. The minimal upgrades show how to update to 0.25 with very little work, by opting into deprecated APIs: ztunes minimal: Minimal React upgrade. hello-zero-solid minimal: Minimal SolidJS upgrade. The maximal upgrades show how to use all the new features. Refer to the commit log of each PR for step-by-step examples of how to upgrade: ztunes maximal: Maximal React upgrade. hello-zero-solid maximal: Maximal SolidJS upgrade. zslack: Maximal React Native upgrade. hello-zero-cf: Maximal Cloudflare upgrade. Additionally, the old hello-zero sample has been updated from the original CRUD mutators and queries all the way to the latest hotness. So if you were still using those APIs, you can use this as a guide to upgrade.",
    "kind": "section"
  },
  {
    "id": "341-release-notes/0.25#features",
    "title": "Zero 0.25",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.25",
    "content": "Query Planning: Zero now automatically plans joins, similar to other databases. You no longer need to specify flip:true to optimize whereExists() calls. Reworked Query DX: Queries now receive a Context automatically ‚Äì you don't have to pass it at the call site. Also, any Standard Schema conforming library can be used for validation and the API has been generally simplified and streamlined. Reworked Mutator DX: The mutator API has been overhauled to match queries. Mutators now also support validation. drizzle-zero and prisma-zero are now first-class: They have moved into the rocicorp org and are now officially supported. Default Schema Type: The Schema type can now be registered as a default with Zero. You can just say useZero() ‚Äì useZero<Schema> is no longer needed. You can also delete your createUseZero() helper if you have one. This registration happens by default if you use the new drizzle-zero or prisma-zero packages. New Connection Status API: Brand new connection status API with overhauled error-handling and reconnection behavior. Mutator promises now return structured errors: The client and server promises now always resolve. In the case of an error, they now return a structured error object. New Install Guide: Rewritten guide to add Zero to an existing project. New zero-out tool: Removes all trace of Zero from Postgres. Support for Generated Columns: In Postgres 18+, generated stored columns are now synced. This is useful for denormalizing data from JSON columns to support filtering in Zero.",
    "kind": "section"
  },
  {
    "id": "342-release-notes/0.25#performance",
    "title": "Zero 0.25",
    "searchTitle": "Performance",
    "sectionTitle": "Performance",
    "sectionId": "performance",
    "url": "/docs/release-notes/0.25",
    "content": "Queries should be 0.2x to 4x faster, depending on the query. Some highlights: Only cache for duration of fetch in exists: 60% improvement on sparse exists queries. Do not fetch on null constraint: 33% improvement on some queries. Remove join storage: ~25% improvement in queries with related calls.",
    "kind": "section"
  },
  {
    "id": "343-release-notes/0.25#fixes",
    "title": "Zero 0.25",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.25",
    "content": "Cooperative concurrency for queries ZERO_REPLICA_FILE now defaults to zero.db serverURL renamed to cacheURL GET_QUERIES renamed to QUERY throughout Ping timeouts now configurable in constructor and at runtime Add configurable WebSocket compression tx.upsert should not overwrite w/ null on missing value sourceField in schema wasn't strongly typed Avoid unique violations during push caused by history compression Invalid SQL was being generated for arrays Handle connections to a view-syncer that has shut down properly Process pending messages on closed websocket Do not hydrate queries if no clients initialized Inspector.analyzeQuery() wasn't mapping names Add flow control to subscriber catchup Migrate TTL/inactivatedAt to DOUBLE PRECISION with clamping Continuous change-log cleanup for single-node Count SQLite row scans, not what is returned to us Make schema path optional for zero-cache-dev (thanks @mattkinnersley!) Export context types (thanks @awkweb!) Suppress deprecation warnings for unset options Fixed closing expo-sqlite DB before deleting Catch errors from async/pipelined db calls Expliclty set isolation level in CVR transactions Change when/where we check advance progress to catch slow change Fix desync caused by client/server inconsistency in keys used to identify rows Send name/args for named queries to enable auth context Re-transform queries on every reconnect Defer completing orderBy(s) until pipeline build and use client primary key Pipeline-driver was in invalid state after ResetPipelinesSignal error Fix pipeline storage db collision SolidJS connection state not updating (thanks @fezproof) Corrects the error semantics of the mutate callback Delay resolving ViewSyncer#initialized until cvr.clientSchema is ensured Use memo to ensure minimal updates in SolidJS Sort queries by hydrateServer desc Delay the replication-manager handoff for loadbalancer task registration Use zql_root as root table name instead of root (thanks @tjenkinson!)",
    "kind": "section"
  },
  {
    "id": "344-release-notes/0.25#breaking-changes",
    "title": "Zero 0.25",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.25",
    "content": "Despite the large DX update, we tried to make the upgrade as smooth as possible. However, there are a few breaking changes we could not easily avoid. If you do not want to immediately opt-in to the new query and mutator APIs, set enableLegacyQueries and enableLegacyMutators to true in your schema.ts. If you are using the .client or .server promises with custom mutators, these no longer ever reject. They now always resolve with a result type that has success/error cases Mutators now reject while Zero is offline. You may want to use the new Connection Status API to detect this and display custom UI to users. The pattern for reauthentication has changed. The auth parameter is now always a string ‚Äì the function form is no longer supported. Instead, call zero.connection.connect({auth: token}) to reconnect. This was done to simplify the API and support reauth via cookies consistently. The onError feature of Zero has been removed. It was undocumented and only half-working. Please use the new Connection Status API to detect errors and display custom UI to users.",
    "kind": "section"
  },
  {
    "id": "43-release-notes/0.3",
    "title": "Zero 0.3",
    "searchTitle": "Zero 0.3",
    "url": "/docs/release-notes/0.3",
    "content": "Install npm install @rocicorp/zero@0.3 Breaking changes zero.config file is now TypeScript, not JSON. See: https://github.com/rocicorp/hello-zero/blob/07c08b1f86b526a96e281ee65af672f52a59bcee/zero.config.ts. Features Schema Migrations: Zero now has first-class support for schema migration (documentation). Write Permissions: First-class write permissions based on ZQL (documentation). Date/Time related types: Zero now natively supports the TIMESTAMP and DATE Postgres types (sample app, documentation). SolidJS: We now have first-class support for SolidJS (documentation). Intellisense for Schema Definition: Introduce createSchema and createTableSchema helper functions to enable intellisense when defining shemas. See Sample App. escapeLike() : Add helper to properly escape strings for use in LIKE filters. See Sample App. New QuickStart App: Entirely rewrote the setup/sample flow to (a) make it much faster to get started playing with Zero, and (b) demonstrate more features. Fixes The @rocicorp/zero package now downloads a prebuilt sqlite instead of compiling it locally. This significantly speeds up install. Support rds.force_ssl=1 RDS configuration. Fixed bug where sibling subqueries could be lost on edit changes. Fixes to error handling to ensure zero-cache prints errors when crashing in multiprocess mode. If zero-cache hears from a client with an unknown CVR/cookie, zero-cache forces that client to reset itself and reload automatically. Useful during development when server-state is frequently getting cleared. Docs Started work to make real docs. Not quite done yet. zbugs https://bugs.rocicorp.dev/ (pw: zql) Improve startup perf: ~3s ‚Üí ~1.5s Hawaii ‚Üî US East. More work to do here but good progress. Responsive design for mobile. ‚ÄúShort IDs‚Äù: Bugs now have a short numeric ID, not a random hash. See Demo Video. First-class label picker. Unread indicators. Finish j/k support for paging through issues. It‚Äôs now ‚Äúsearch-aware‚Äù, it pages through issues in order of search you clicked through to detail page in. Text search (slash to activate ‚Äî needs better discoverability) Emojis on issues and comments Sort controls on list view remove fps meter temporarily numerous other UI polish",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Breaking changes",
        "id": "breaking-changes"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Docs",
        "id": "docs"
      },
      {
        "text": "zbugs",
        "id": "zbugs"
      }
    ],
    "kind": "page"
  },
  {
    "id": "345-release-notes/0.3#install",
    "title": "Zero 0.3",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.3",
    "content": "npm install @rocicorp/zero@0.3",
    "kind": "section"
  },
  {
    "id": "346-release-notes/0.3#breaking-changes",
    "title": "Zero 0.3",
    "searchTitle": "Breaking changes",
    "sectionTitle": "Breaking changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.3",
    "content": "zero.config file is now TypeScript, not JSON. See: https://github.com/rocicorp/hello-zero/blob/07c08b1f86b526a96e281ee65af672f52a59bcee/zero.config.ts.",
    "kind": "section"
  },
  {
    "id": "347-release-notes/0.3#features",
    "title": "Zero 0.3",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.3",
    "content": "Schema Migrations: Zero now has first-class support for schema migration (documentation). Write Permissions: First-class write permissions based on ZQL (documentation). Date/Time related types: Zero now natively supports the TIMESTAMP and DATE Postgres types (sample app, documentation). SolidJS: We now have first-class support for SolidJS (documentation). Intellisense for Schema Definition: Introduce createSchema and createTableSchema helper functions to enable intellisense when defining shemas. See Sample App. escapeLike() : Add helper to properly escape strings for use in LIKE filters. See Sample App. New QuickStart App: Entirely rewrote the setup/sample flow to (a) make it much faster to get started playing with Zero, and (b) demonstrate more features.",
    "kind": "section"
  },
  {
    "id": "348-release-notes/0.3#fixes",
    "title": "Zero 0.3",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.3",
    "content": "The @rocicorp/zero package now downloads a prebuilt sqlite instead of compiling it locally. This significantly speeds up install. Support rds.force_ssl=1 RDS configuration. Fixed bug where sibling subqueries could be lost on edit changes. Fixes to error handling to ensure zero-cache prints errors when crashing in multiprocess mode. If zero-cache hears from a client with an unknown CVR/cookie, zero-cache forces that client to reset itself and reload automatically. Useful during development when server-state is frequently getting cleared.",
    "kind": "section"
  },
  {
    "id": "349-release-notes/0.3#docs",
    "title": "Zero 0.3",
    "searchTitle": "Docs",
    "sectionTitle": "Docs",
    "sectionId": "docs",
    "url": "/docs/release-notes/0.3",
    "content": "Started work to make real docs. Not quite done yet.",
    "kind": "section"
  },
  {
    "id": "350-release-notes/0.3#zbugs",
    "title": "Zero 0.3",
    "searchTitle": "zbugs",
    "sectionTitle": "zbugs",
    "sectionId": "zbugs",
    "url": "/docs/release-notes/0.3",
    "content": "https://bugs.rocicorp.dev/ (pw: zql) Improve startup perf: ~3s ‚Üí ~1.5s Hawaii ‚Üî US East. More work to do here but good progress. Responsive design for mobile. ‚ÄúShort IDs‚Äù: Bugs now have a short numeric ID, not a random hash. See Demo Video. First-class label picker. Unread indicators. Finish j/k support for paging through issues. It‚Äôs now ‚Äúsearch-aware‚Äù, it pages through issues in order of search you clicked through to detail page in. Text search (slash to activate ‚Äî needs better discoverability) Emojis on issues and comments Sort controls on list view remove fps meter temporarily numerous other UI polish",
    "kind": "section"
  },
  {
    "id": "44-release-notes/0.4",
    "title": "Zero 0.4",
    "searchTitle": "Zero 0.4",
    "url": "/docs/release-notes/0.4",
    "content": "Install npm install @rocicorp/zero@0.4 Breaking changes The or changes modified the client/server protocol. You‚Äôll need to restart zero-cache and clear browser data after updating. Added or , and , and not to ZQL (documentation). Added query.run() method (documentation). Fixes Use batch() method in zero-solid to improve performance when multiple updates happen in same frame. To take advantage of this you must use the createZero helper from @rocicorp/zero/solid, instead of instantiating Zero directly. See the solid sample app. Postgres tables that were reserved words in SQLite but not Postgres caused crash during replication. LIKE was not matching correctly in the case of multiline subjects. Upstream database and zero database can now be same Postgres db (don‚Äôt need separate ports). Docs nothing notable zbugs Use or to run text search over both titles and bodies prevent j/k in emoji preload emojis",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Breaking changes",
        "id": "breaking-changes"
      },
      {
        "text": "Added or , and , and not to ZQL (documentation).",
        "id": "added-or--and--and-not-to-zql-documentation"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Docs",
        "id": "docs"
      },
      {
        "text": "zbugs",
        "id": "zbugs"
      }
    ],
    "kind": "page"
  },
  {
    "id": "351-release-notes/0.4#install",
    "title": "Zero 0.4",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.4",
    "content": "npm install @rocicorp/zero@0.4",
    "kind": "section"
  },
  {
    "id": "352-release-notes/0.4#breaking-changes",
    "title": "Zero 0.4",
    "searchTitle": "Breaking changes",
    "sectionTitle": "Breaking changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.4",
    "content": "The or changes modified the client/server protocol. You‚Äôll need to restart zero-cache and clear browser data after updating.",
    "kind": "section"
  },
  {
    "id": "353-release-notes/0.4#added-or--and--and-not-to-zql-documentation",
    "title": "Zero 0.4",
    "searchTitle": "Added or , and , and not to ZQL (documentation).",
    "sectionTitle": "Added or , and , and not to ZQL (documentation).",
    "sectionId": "added-or--and--and-not-to-zql-documentation",
    "url": "/docs/release-notes/0.4",
    "content": "Added query.run() method (documentation).",
    "kind": "section"
  },
  {
    "id": "354-release-notes/0.4#fixes",
    "title": "Zero 0.4",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.4",
    "content": "Use batch() method in zero-solid to improve performance when multiple updates happen in same frame. To take advantage of this you must use the createZero helper from @rocicorp/zero/solid, instead of instantiating Zero directly. See the solid sample app. Postgres tables that were reserved words in SQLite but not Postgres caused crash during replication. LIKE was not matching correctly in the case of multiline subjects. Upstream database and zero database can now be same Postgres db (don‚Äôt need separate ports).",
    "kind": "section"
  },
  {
    "id": "355-release-notes/0.4#docs",
    "title": "Zero 0.4",
    "searchTitle": "Docs",
    "sectionTitle": "Docs",
    "sectionId": "docs",
    "url": "/docs/release-notes/0.4",
    "content": "nothing notable",
    "kind": "section"
  },
  {
    "id": "356-release-notes/0.4#zbugs",
    "title": "Zero 0.4",
    "searchTitle": "zbugs",
    "sectionTitle": "zbugs",
    "sectionId": "zbugs",
    "url": "/docs/release-notes/0.4",
    "content": "Use or to run text search over both titles and bodies prevent j/k in emoji preload emojis",
    "kind": "section"
  },
  {
    "id": "45-release-notes/0.5",
    "title": "Zero 0.5",
    "searchTitle": "Zero 0.5",
    "url": "/docs/release-notes/0.5",
    "content": "Install npm install @rocicorp/zero@0.5 Breaking changes createTableSchema and createSchema moved to @rocicorp/zero/schema subpackage. This is in preparation to moving authorization into the schema file. SchemaToRow helper type was renamed TableSchemaToRow and moved into @rocicorp/zero/schema. Basically: - import { createSchema, createTableSchema, SchemaToRow } from \"@rocicorp/zero\"; + import { createSchema, createTableSchema, TableSchemaToRow } from \"@rocicorp/zero/schema\"; Features Added support for JSON columns in Postgres (documentation). Zero pacakage now includes zero-sqlite3, which can be used to explore our sqlite files (documentation). Fixes We were not correctly replicating the char(n) type, despite documenting that we were. Docs nothing notable zbugs nothing notable",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Breaking changes",
        "id": "breaking-changes"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Docs",
        "id": "docs"
      },
      {
        "text": "zbugs",
        "id": "zbugs"
      }
    ],
    "kind": "page"
  },
  {
    "id": "357-release-notes/0.5#install",
    "title": "Zero 0.5",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.5",
    "content": "npm install @rocicorp/zero@0.5",
    "kind": "section"
  },
  {
    "id": "358-release-notes/0.5#breaking-changes",
    "title": "Zero 0.5",
    "searchTitle": "Breaking changes",
    "sectionTitle": "Breaking changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.5",
    "content": "createTableSchema and createSchema moved to @rocicorp/zero/schema subpackage. This is in preparation to moving authorization into the schema file. SchemaToRow helper type was renamed TableSchemaToRow and moved into @rocicorp/zero/schema. Basically: - import { createSchema, createTableSchema, SchemaToRow } from \"@rocicorp/zero\"; + import { createSchema, createTableSchema, TableSchemaToRow } from \"@rocicorp/zero/schema\";",
    "kind": "section"
  },
  {
    "id": "359-release-notes/0.5#features",
    "title": "Zero 0.5",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.5",
    "content": "Added support for JSON columns in Postgres (documentation). Zero pacakage now includes zero-sqlite3, which can be used to explore our sqlite files (documentation).",
    "kind": "section"
  },
  {
    "id": "360-release-notes/0.5#fixes",
    "title": "Zero 0.5",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.5",
    "content": "We were not correctly replicating the char(n) type, despite documenting that we were.",
    "kind": "section"
  },
  {
    "id": "361-release-notes/0.5#docs",
    "title": "Zero 0.5",
    "searchTitle": "Docs",
    "sectionTitle": "Docs",
    "sectionId": "docs",
    "url": "/docs/release-notes/0.5",
    "content": "nothing notable",
    "kind": "section"
  },
  {
    "id": "362-release-notes/0.5#zbugs",
    "title": "Zero 0.5",
    "searchTitle": "zbugs",
    "sectionTitle": "zbugs",
    "sectionId": "zbugs",
    "url": "/docs/release-notes/0.5",
    "content": "nothing notable",
    "kind": "section"
  },
  {
    "id": "46-release-notes/0.6",
    "title": "Zero 0.6",
    "searchTitle": "Zero 0.6",
    "url": "/docs/release-notes/0.6",
    "content": "Install npm install @rocicorp/zero@0.6 Upgrade Guide This release is a bit harder to upgrade to than previous alphas. For a step-by-step guide, please refer to the commits that upgrade the React and Solid quickstart apps: Upgrading hello-zero from Zero 0.5 to 0.6 Upgrading hello-zero-solid from Zero 0.5 to 0.6 Breaking Changes Totally new configuration system. zero.config.ts is no more ‚Äì config is now via env vars (documentation). Permissions rules moved into schema (documentation). Renamed CRUD mutators to be consistent with SQL naming (bug, documentation). z.mutate.<table>.create -> insert z.mutate.<table>.put -> upsert Removed select from ZQL. It wasn‚Äôt doing anything (documentation) Moved batch mutation to its own mutateBatch method. Before the mutate field also doubled as a method. This made intellisense hard to understand since z.mutate had all the tables as fields but also all the fields of a function. Features Relationship filters. Queries can now include whereExists (bug, documentation). Reworked syntax for compound where filters, including ergonomically building or expressions with dynamic number of clauses (bug, documentation). Support using Postgres databases without superuser access for smaller apps (documentation). Support for running Zero client under Cloudflare Durable Objects (documentation). Reworked support for null / undefined to properly support optional fields (bug, documentation). Added IS / IS NOT to ZQL to support checking for null (bug, documentation). Improved intellisense for mutators. Added --port flag and ZERO_PORT environment variable (bug, documentation). Default max connections of zero-cache more conservatively so that it should fit with even common small Postgres configurations. zero-cache now accepts requests with any base path, not just /api. The server parameter to the Zero client constructor can now be a host (https://myapp-myteam.zero.ms) or a host with a single path component (https://myapp-myteam.zero.ms/zero). These two changes together allow hosting zero-cache on same domain with an app that already uses the /api prefix (bug). Allow Postgres columns with default values, but don‚Äôt sync them (documentation). The npx zero-sqlite utility now accepts all the same flags and arguments that sqlite3 does (documentation). zbugs Added tooltip describing who submitted which emoji reactions Updated implementation of label, assignee, and owner filters to use relationship filters Updated text filter implementation to use or to search description and comments too Docs Added new ZQL reference Added new mutators reference Added new config reference",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Upgrade Guide",
        "id": "upgrade-guide"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "zbugs",
        "id": "zbugs"
      },
      {
        "text": "Docs",
        "id": "docs"
      }
    ],
    "kind": "page"
  },
  {
    "id": "363-release-notes/0.6#install",
    "title": "Zero 0.6",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.6",
    "content": "npm install @rocicorp/zero@0.6",
    "kind": "section"
  },
  {
    "id": "364-release-notes/0.6#upgrade-guide",
    "title": "Zero 0.6",
    "searchTitle": "Upgrade Guide",
    "sectionTitle": "Upgrade Guide",
    "sectionId": "upgrade-guide",
    "url": "/docs/release-notes/0.6",
    "content": "This release is a bit harder to upgrade to than previous alphas. For a step-by-step guide, please refer to the commits that upgrade the React and Solid quickstart apps: Upgrading hello-zero from Zero 0.5 to 0.6 Upgrading hello-zero-solid from Zero 0.5 to 0.6",
    "kind": "section"
  },
  {
    "id": "365-release-notes/0.6#breaking-changes",
    "title": "Zero 0.6",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.6",
    "content": "Totally new configuration system. zero.config.ts is no more ‚Äì config is now via env vars (documentation). Permissions rules moved into schema (documentation). Renamed CRUD mutators to be consistent with SQL naming (bug, documentation). z.mutate.<table>.create -> insert z.mutate.<table>.put -> upsert Removed select from ZQL. It wasn‚Äôt doing anything (documentation) Moved batch mutation to its own mutateBatch method. Before the mutate field also doubled as a method. This made intellisense hard to understand since z.mutate had all the tables as fields but also all the fields of a function.",
    "kind": "section"
  },
  {
    "id": "366-release-notes/0.6#features",
    "title": "Zero 0.6",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.6",
    "content": "Relationship filters. Queries can now include whereExists (bug, documentation). Reworked syntax for compound where filters, including ergonomically building or expressions with dynamic number of clauses (bug, documentation). Support using Postgres databases without superuser access for smaller apps (documentation). Support for running Zero client under Cloudflare Durable Objects (documentation). Reworked support for null / undefined to properly support optional fields (bug, documentation). Added IS / IS NOT to ZQL to support checking for null (bug, documentation). Improved intellisense for mutators. Added --port flag and ZERO_PORT environment variable (bug, documentation). Default max connections of zero-cache more conservatively so that it should fit with even common small Postgres configurations. zero-cache now accepts requests with any base path, not just /api. The server parameter to the Zero client constructor can now be a host (https://myapp-myteam.zero.ms) or a host with a single path component (https://myapp-myteam.zero.ms/zero). These two changes together allow hosting zero-cache on same domain with an app that already uses the /api prefix (bug). Allow Postgres columns with default values, but don‚Äôt sync them (documentation). The npx zero-sqlite utility now accepts all the same flags and arguments that sqlite3 does (documentation).",
    "kind": "section"
  },
  {
    "id": "367-release-notes/0.6#zbugs",
    "title": "Zero 0.6",
    "searchTitle": "zbugs",
    "sectionTitle": "zbugs",
    "sectionId": "zbugs",
    "url": "/docs/release-notes/0.6",
    "content": "Added tooltip describing who submitted which emoji reactions Updated implementation of label, assignee, and owner filters to use relationship filters Updated text filter implementation to use or to search description and comments too",
    "kind": "section"
  },
  {
    "id": "368-release-notes/0.6#docs",
    "title": "Zero 0.6",
    "searchTitle": "Docs",
    "sectionTitle": "Docs",
    "sectionId": "docs",
    "url": "/docs/release-notes/0.6",
    "content": "Added new ZQL reference Added new mutators reference Added new config reference",
    "kind": "section"
  },
  {
    "id": "47-release-notes/0.7",
    "title": "Zero 0.7",
    "searchTitle": "Zero 0.7",
    "url": "/docs/release-notes/0.7",
    "content": "Install npm install @rocicorp/zero@0.7 Features Read permissions. You can now control read access to data using ZQL (docs). Deployment. We now have a single-node Docker container (docs). Future work will add multinode support. Compound FKs. Zero already supported compound primary keys, but now it also supports compound foreign keys (docs). Schema DX: Columns types can use bare strings now if optional is not needed (example). PK can be a single string in the common case where it‚Äôs non-compound (example). Breaking Changes Several changes to schema.ts. See update to hello-zero for overview. Details: defineAuthorization was renamed to definedPermissions to avoid confusion with authentication. The way that many:many relationships are defined has changed to be more general and easy to remember. See example. The signature of definePermissions and the related rule functions have changed: Now rules return an expression instead of full query. This was required to make read permissions work and we did it for write permissions for consitency (see example). The update policy now has two child policies: preMutation and postMutation. The rules we used to have were preMutation. They run before a change and can be used to validate a user has permission to change a row. The postMutation rules run after and can be used to limit the changes a user is allowed to make. The schema.ts file should export an object having two fields: schema and permissions. The way that schema.ts is consumed has also changed. Rather than zero-cache directly reading the typescript source, we compile it to JSON and read that. ZERO_SCHEMA_FILE should now point to a JSON file, not .ts. It defaults to ./zero-schema.json which we‚Äôve found to be pretty useful so you‚Äôll probably just remove this key from your .env entirely. Use npx zero-build-schema to generate the JSON. You must currently do this manually each time you change the schema, we will automate it soon. We compile the schema to JSON so that we can use it on the server without needing a TS toolchain there. Also so that we can run a SaaS in the future without needing to run user code. zbugs Comments now have permalinks. Implementing permalinks in a synced SPA is fun! Private issues. Zbugs now supports private (to team only) issues. I wonder what‚Äôs in them ‚Ä¶ üëÄ. Docs The docs have moved. Please don‚Äôt use Notion anymore, they won‚Äôt be updated.",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      },
      {
        "text": "zbugs",
        "id": "zbugs"
      },
      {
        "text": "Docs",
        "id": "docs"
      }
    ],
    "kind": "page"
  },
  {
    "id": "369-release-notes/0.7#install",
    "title": "Zero 0.7",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.7",
    "content": "npm install @rocicorp/zero@0.7",
    "kind": "section"
  },
  {
    "id": "370-release-notes/0.7#features",
    "title": "Zero 0.7",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.7",
    "content": "Read permissions. You can now control read access to data using ZQL (docs). Deployment. We now have a single-node Docker container (docs). Future work will add multinode support. Compound FKs. Zero already supported compound primary keys, but now it also supports compound foreign keys (docs). Schema DX: Columns types can use bare strings now if optional is not needed (example). PK can be a single string in the common case where it‚Äôs non-compound (example).",
    "kind": "section"
  },
  {
    "id": "371-release-notes/0.7#breaking-changes",
    "title": "Zero 0.7",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.7",
    "content": "Several changes to schema.ts. See update to hello-zero for overview. Details: defineAuthorization was renamed to definedPermissions to avoid confusion with authentication. The way that many:many relationships are defined has changed to be more general and easy to remember. See example. The signature of definePermissions and the related rule functions have changed: Now rules return an expression instead of full query. This was required to make read permissions work and we did it for write permissions for consitency (see example). The update policy now has two child policies: preMutation and postMutation. The rules we used to have were preMutation. They run before a change and can be used to validate a user has permission to change a row. The postMutation rules run after and can be used to limit the changes a user is allowed to make. The schema.ts file should export an object having two fields: schema and permissions. The way that schema.ts is consumed has also changed. Rather than zero-cache directly reading the typescript source, we compile it to JSON and read that. ZERO_SCHEMA_FILE should now point to a JSON file, not .ts. It defaults to ./zero-schema.json which we‚Äôve found to be pretty useful so you‚Äôll probably just remove this key from your .env entirely. Use npx zero-build-schema to generate the JSON. You must currently do this manually each time you change the schema, we will automate it soon. We compile the schema to JSON so that we can use it on the server without needing a TS toolchain there. Also so that we can run a SaaS in the future without needing to run user code.",
    "kind": "section"
  },
  {
    "id": "372-release-notes/0.7#zbugs",
    "title": "Zero 0.7",
    "searchTitle": "zbugs",
    "sectionTitle": "zbugs",
    "sectionId": "zbugs",
    "url": "/docs/release-notes/0.7",
    "content": "Comments now have permalinks. Implementing permalinks in a synced SPA is fun! Private issues. Zbugs now supports private (to team only) issues. I wonder what‚Äôs in them ‚Ä¶ üëÄ.",
    "kind": "section"
  },
  {
    "id": "373-release-notes/0.7#docs",
    "title": "Zero 0.7",
    "searchTitle": "Docs",
    "sectionTitle": "Docs",
    "sectionId": "docs",
    "url": "/docs/release-notes/0.7",
    "content": "The docs have moved. Please don‚Äôt use Notion anymore, they won‚Äôt be updated.",
    "kind": "section"
  },
  {
    "id": "48-release-notes/0.8",
    "title": "Zero 0.8",
    "searchTitle": "Zero 0.8",
    "url": "/docs/release-notes/0.8",
    "content": "Install npm install @rocicorp/zero@0.8 See the changes to hello-zero or hello-zero-solid for example updates. Features Schema Autobuild. There's now a zero-cache-dev script that automatically rebuilds the schema and restarts zero-cache on changes to schema.ts. (docs) Result Type. You can now tell whether a query is complete or partial. (docs) Enums. Enums are now supported in Postgres schemas and on client. (docs) Custom Types. You can define custom JSON types in your schema. (docs) OTEL Tracing. Initial tracing support. (docs) timestampz. Add support for timestampz Postgres column type. (docs) SSLMode. You can disable TLS when zero-cache connects to DB with sslmode=disable. (docs) Permission Helpers. ANYONE_CAN and NOBODY_CAN helpers were added to make these cases more readable. (docs) Multitenant Support. A single zero-cache can now front separate Postgres databases. This is useful for customers that have one \"dev\" database in production per-developer. (docs) Fixes Crash with JSON Columns. Fixed a crash when a JSON column was used in a Zero app with write permissions (bug) Better Connection Error Reporting. Some connection errors would cause zero-cache to exit silently. Now they are returned to client and logged. Breaking Changes useQuery in React now returns a 2-tuple of [rows, result] where result is an object with a type field. postProposedMutation in write permissions for update renamed to postMutation for consistency. TableScheamToRow renamed to Row to not be so silly long.",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "374-release-notes/0.8#install",
    "title": "Zero 0.8",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.8",
    "content": "npm install @rocicorp/zero@0.8 See the changes to hello-zero or hello-zero-solid for example updates.",
    "kind": "section"
  },
  {
    "id": "375-release-notes/0.8#features",
    "title": "Zero 0.8",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.8",
    "content": "Schema Autobuild. There's now a zero-cache-dev script that automatically rebuilds the schema and restarts zero-cache on changes to schema.ts. (docs) Result Type. You can now tell whether a query is complete or partial. (docs) Enums. Enums are now supported in Postgres schemas and on client. (docs) Custom Types. You can define custom JSON types in your schema. (docs) OTEL Tracing. Initial tracing support. (docs) timestampz. Add support for timestampz Postgres column type. (docs) SSLMode. You can disable TLS when zero-cache connects to DB with sslmode=disable. (docs) Permission Helpers. ANYONE_CAN and NOBODY_CAN helpers were added to make these cases more readable. (docs) Multitenant Support. A single zero-cache can now front separate Postgres databases. This is useful for customers that have one \"dev\" database in production per-developer. (docs)",
    "kind": "section"
  },
  {
    "id": "376-release-notes/0.8#fixes",
    "title": "Zero 0.8",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.8",
    "content": "Crash with JSON Columns. Fixed a crash when a JSON column was used in a Zero app with write permissions (bug) Better Connection Error Reporting. Some connection errors would cause zero-cache to exit silently. Now they are returned to client and logged.",
    "kind": "section"
  },
  {
    "id": "377-release-notes/0.8#breaking-changes",
    "title": "Zero 0.8",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.8",
    "content": "useQuery in React now returns a 2-tuple of [rows, result] where result is an object with a type field. postProposedMutation in write permissions for update renamed to postMutation for consistency. TableScheamToRow renamed to Row to not be so silly long.",
    "kind": "section"
  },
  {
    "id": "49-release-notes/0.9",
    "title": "Zero 0.9",
    "searchTitle": "Zero 0.9",
    "url": "/docs/release-notes/0.9",
    "content": "Install npm install @rocicorp/zero@0.9 See the changes to hello-zero or hello-zero-solid for example updates. Features JWK Support. For auth, you can now specify a JWK containing a public key, or a JWKS url to support autodiscovery of keys. (docs) UUID column. Zero now supports the uuid Postgres column type. (docs) Fixes Readonly Values. Type of values returned from Zero queries are marked readonly. The system always considered them readonly, but now the types reflect that. (docs) Breaking Changes The zero-cache config ZERO_JWT_SECRET has been renamed to ZERO_AUTH_SECRET for consistency with the new JWK-related keys. If you were using the old name, you'll need to update your .env file. All values returned by Zero are now readonly. You'll probably have to add this TS modifier various places. If you find yourself casting away readonly you probably should be cloing the value instead.",
    "headings": [
      {
        "text": "Install",
        "id": "install"
      },
      {
        "text": "Features",
        "id": "features"
      },
      {
        "text": "Fixes",
        "id": "fixes"
      },
      {
        "text": "Breaking Changes",
        "id": "breaking-changes"
      }
    ],
    "kind": "page"
  },
  {
    "id": "378-release-notes/0.9#install",
    "title": "Zero 0.9",
    "searchTitle": "Install",
    "sectionTitle": "Install",
    "sectionId": "install",
    "url": "/docs/release-notes/0.9",
    "content": "npm install @rocicorp/zero@0.9 See the changes to hello-zero or hello-zero-solid for example updates.",
    "kind": "section"
  },
  {
    "id": "379-release-notes/0.9#features",
    "title": "Zero 0.9",
    "searchTitle": "Features",
    "sectionTitle": "Features",
    "sectionId": "features",
    "url": "/docs/release-notes/0.9",
    "content": "JWK Support. For auth, you can now specify a JWK containing a public key, or a JWKS url to support autodiscovery of keys. (docs) UUID column. Zero now supports the uuid Postgres column type. (docs)",
    "kind": "section"
  },
  {
    "id": "380-release-notes/0.9#fixes",
    "title": "Zero 0.9",
    "searchTitle": "Fixes",
    "sectionTitle": "Fixes",
    "sectionId": "fixes",
    "url": "/docs/release-notes/0.9",
    "content": "Readonly Values. Type of values returned from Zero queries are marked readonly. The system always considered them readonly, but now the types reflect that. (docs)",
    "kind": "section"
  },
  {
    "id": "381-release-notes/0.9#breaking-changes",
    "title": "Zero 0.9",
    "searchTitle": "Breaking Changes",
    "sectionTitle": "Breaking Changes",
    "sectionId": "breaking-changes",
    "url": "/docs/release-notes/0.9",
    "content": "The zero-cache config ZERO_JWT_SECRET has been renamed to ZERO_AUTH_SECRET for consistency with the new JWK-related keys. If you were using the old name, you'll need to update your .env file. All values returned by Zero are now readonly. You'll probably have to add this TS modifier various places. If you find yourself casting away readonly you probably should be cloing the value instead.",
    "kind": "section"
  },
  {
    "id": "50-release-notes",
    "title": "Release Notes",
    "searchTitle": "Release Notes",
    "url": "/docs/release-notes",
    "content": "Zero 0.25: DX Overhaul, Query Planning Zero 0.24: Join Flipping, Cookie Auth, Inspector Updates Zero 0.23: Synced Queries and React Native Support Zero 0.22: Simplified TTLs Zero 0.21: PG arrays, TanStack starter, and more Zero 0.20: Full Supabase support, performance improvements Zero 0.19: Many, many bugfixes and cleanups Zero 0.18: Custom Mutators Zero 0.17: Background Queries Zero 0.16: Lambda-Based Permission Deployment Zero 0.15: Live Permission Updates Zero 0.14: Name Mapping and Multischema Zero 0.13: Multinode and SST Zero 0.12: Circular Relationships Zero 0.11: Windows Zero 0.10: Remove Top-Level Await Zero 0.9: JWK Support Zero 0.8: Schema Autobuild, Result Types, and Enums Zero 0.7: Read Perms and Docker Zero 0.6: Relationship Filters Zero 0.5: JSON Columns Zero 0.4: Compound Filters Zero 0.3: Schema Migrations and Write Perms Zero 0.2: Skip Mode and Computed PKs Zero 0.1: First Release",
    "headings": [],
    "kind": "page"
  },
  {
    "id": "51-reporting-bugs",
    "title": "Reporting Bugs",
    "searchTitle": "Reporting Bugs",
    "url": "/docs/reporting-bugs",
    "content": "zbugs You can use zbugs! (password: zql) Our own bug tracker built from the ground up on Zero. Discord Alternately just pinging us on Discord is great too. Join us on Discord",
    "headings": [
      {
        "text": "zbugs",
        "id": "zbugs"
      },
      {
        "text": "Discord",
        "id": "discord"
      }
    ],
    "kind": "page"
  },
  {
    "id": "382-reporting-bugs#zbugs",
    "title": "Reporting Bugs",
    "searchTitle": "zbugs",
    "sectionTitle": "zbugs",
    "sectionId": "zbugs",
    "url": "/docs/reporting-bugs",
    "content": "You can use zbugs! (password: zql) Our own bug tracker built from the ground up on Zero.",
    "kind": "section"
  },
  {
    "id": "383-reporting-bugs#discord",
    "title": "Reporting Bugs",
    "searchTitle": "Discord",
    "sectionTitle": "Discord",
    "sectionId": "discord",
    "url": "/docs/reporting-bugs",
    "content": "Alternately just pinging us on Discord is great too. Join us on Discord",
    "kind": "section"
  },
  {
    "id": "52-roadmap",
    "title": "Roadmap",
    "searchTitle": "Roadmap",
    "url": "/docs/roadmap",
    "content": "We are working toward a beta release of Zero late 2025 or early 2026. We define beta as the point at which Zero is a good choice for the average new rich web application. Feature-wise, we think Zero is pretty close. There are only a few remaining features listed below that we view as critical for beta. There are also several nice-to-have features, but users can typically work around their absence without much trouble. Beyond these features, our main focus for beta is improving the stability, performance, documentation, and polish of Zero as a whole. Q4 2025 Full revamp of APIs, documentation, and onboarding Deprecate legacy mutators and queries Polish syncedQueries APIs with feedback from initial release Unify synced queries and custom mutators Revamp auth-related APIs Document perf expectations and best practices Join planning Gigabugs Disable offline writes SaaS (invite only) Beyond Column permissions Aggregates (count, min, max, group-by) SSR JSON filters First-class text search",
    "headings": [
      {
        "text": "Q4 2025",
        "id": "q4-2025"
      },
      {
        "text": "Beyond",
        "id": "beyond"
      }
    ],
    "kind": "page"
  },
  {
    "id": "384-roadmap#q4-2025",
    "title": "Roadmap",
    "searchTitle": "Q4 2025",
    "sectionTitle": "Q4 2025",
    "sectionId": "q4-2025",
    "url": "/docs/roadmap",
    "content": "Full revamp of APIs, documentation, and onboarding Deprecate legacy mutators and queries Polish syncedQueries APIs with feedback from initial release Unify synced queries and custom mutators Revamp auth-related APIs Document perf expectations and best practices Join planning Gigabugs Disable offline writes SaaS (invite only)",
    "kind": "section"
  },
  {
    "id": "385-roadmap#beyond",
    "title": "Roadmap",
    "searchTitle": "Beyond",
    "sectionTitle": "Beyond",
    "sectionId": "beyond",
    "url": "/docs/roadmap",
    "content": "Column permissions Aggregates (count, min, max, group-by) SSR JSON filters First-class text search",
    "kind": "section"
  },
  {
    "id": "53-samples",
    "title": "Samples",
    "searchTitle": "Samples",
    "url": "/docs/samples",
    "content": "Gigabugs A complete Linear-style bug tracker, populated with 1.2 million bugs, totalling over 1GB of sample data. This demo shows off Zero's support for large datasets and partial sync, loading from cold start in < 2s yet providing instant UI for almost all interactions. But it's not just a demo. We also use a different instance of this app everyday as our actual bug tracker to continuously dogfood Zero. Demo: https://gigabugs.rocicorp.dev/ Stack: Vite/Fastify/React/AWS Source: https://github.com/rocicorp/mono/tree/latest/apps/zbugs Features: Instant reads and writes, realtime updates, Github auth, write permissions, read permissions, complex filters, unread indicators, basic text search, emojis, short numeric bug IDs, notifications, and more. ztunes An ecommerce store built with Zero, TanStack, Drizzle, and PlanetScale for Postgres. Demo: https://ztunes.rocicorp.dev/ Stack: TanStack/Drizzle/Better Auth/Fly.io Source: https://github.com/rocicorp/ztunes Features: 88k artists, 200k albums, single-command dev, full drizzle integration, text search, read permissions, write permissions. zslack Simple Slack-like app built with Expo/React Native. Stack: Expo/Hono/Drizzle/Bun Source: https://github.com/rocicorp/zslack Features: Native iOS/Android, instant reads and writes, realtime updates.",
    "headings": [
      {
        "text": "Gigabugs",
        "id": "gigabugs"
      },
      {
        "text": "ztunes",
        "id": "ztunes"
      },
      {
        "text": "zslack",
        "id": "zslack"
      }
    ],
    "kind": "page"
  },
  {
    "id": "386-samples#gigabugs",
    "title": "Samples",
    "searchTitle": "Gigabugs",
    "sectionTitle": "Gigabugs",
    "sectionId": "gigabugs",
    "url": "/docs/samples",
    "content": "A complete Linear-style bug tracker, populated with 1.2 million bugs, totalling over 1GB of sample data. This demo shows off Zero's support for large datasets and partial sync, loading from cold start in < 2s yet providing instant UI for almost all interactions. But it's not just a demo. We also use a different instance of this app everyday as our actual bug tracker to continuously dogfood Zero. Demo: https://gigabugs.rocicorp.dev/ Stack: Vite/Fastify/React/AWS Source: https://github.com/rocicorp/mono/tree/latest/apps/zbugs Features: Instant reads and writes, realtime updates, Github auth, write permissions, read permissions, complex filters, unread indicators, basic text search, emojis, short numeric bug IDs, notifications, and more.",
    "kind": "section"
  },
  {
    "id": "387-samples#ztunes",
    "title": "Samples",
    "searchTitle": "ztunes",
    "sectionTitle": "ztunes",
    "sectionId": "ztunes",
    "url": "/docs/samples",
    "content": "An ecommerce store built with Zero, TanStack, Drizzle, and PlanetScale for Postgres. Demo: https://ztunes.rocicorp.dev/ Stack: TanStack/Drizzle/Better Auth/Fly.io Source: https://github.com/rocicorp/ztunes Features: 88k artists, 200k albums, single-command dev, full drizzle integration, text search, read permissions, write permissions.",
    "kind": "section"
  },
  {
    "id": "388-samples#zslack",
    "title": "Samples",
    "searchTitle": "zslack",
    "sectionTitle": "zslack",
    "sectionId": "zslack",
    "url": "/docs/samples",
    "content": "Simple Slack-like app built with Expo/React Native. Stack: Expo/Hono/Drizzle/Bun Source: https://github.com/rocicorp/zslack Features: Native iOS/Android, instant reads and writes, realtime updates.",
    "kind": "section"
  },
  {
    "id": "54-schema",
    "title": "Zero Schema",
    "searchTitle": "Zero Schema",
    "url": "/docs/schema",
    "content": "Zero applications have both a database schema (the normal backend schema all web apps have) and a Zero schema. The Zero schema is conventionally located in schema.ts in your app's source code. The Zero schema serves two purposes: Provide typesafety for ZQL queries Define first-class relationships between tables The Zero schema is usually generated from your backend schema, but can be defined by hand for more control. Generating from Database If you use Drizzle or Prisma ORM, you can generate schema.ts with drizzle-zero or prisma-zero: npm install -D drizzle-zero npx drizzle-zero generatepnpm add -D drizzle-zero pnpm dlx drizzle-zero generatebun add -D drizzle-zero bunx drizzle-zero generateyarn add -D drizzle-zero yarn dlx drizzle-zero generatenpm install -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } npx prisma generatepnpm add -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } pnpx prisma generatebun add -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } bunx prisma generateyarn add -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } yarn prisma generate We'd love more! See the source for drizzle-zero and prisma-zero as a guide, or reach out on Discord with questions. Writing by Hand You can also write Zero schemas by hand for full control. Table Schemas Use the table function to define each table in your Zero schema: import {table, string, boolean} from '@rocicorp/zero' const user = table('user') .columns({ id: string(), name: string(), partner: boolean() }) .primaryKey('id') Column types are defined with the boolean(), number(), string(), json(), and enumeration() helpers. See Column Types for how database types are mapped to these types. Name Mapping Use from() to map a TypeScript table or column name to a different database name: const userPref = table('userPref') // Map TS \"userPref\" to DB name \"user_pref\" .from('user_pref') .columns({ id: string(), // Map TS \"orgID\" to DB name \"org_id\" orgID: string().from('org_id') }) Multiple Schemas You can also use from() to access other Postgres schemas: // Sync the \"event\" table from the \"analytics\" schema. const event = table('event').from('analytics.event') Optional Columns Columns can be marked optional. This corresponds to the SQL concept nullable. const user = table('user') .columns({ id: string(), name: string(), nickName: string().optional() }) .primaryKey('id') An optional column can store a value of the specified type or null to mean no value. Note that null and undefined mean different things when working with Zero rows. When reading, if a column is optional, Zero can return null for that field. undefined is not used at all when Reading from Zero. When writing, you can specify null for an optional field to explicitly write null to the datastore, unsetting any previous value. For create and upsert you can set optional fields to undefined (or leave the field off completely) to take the default value as specified by backend schema for that column. For update you can set any non-PK field to undefined to leave the previous value unmodified. Enumerations Use the enumeration helper to define a column that can only take on a specific set of values. This is most often used alongside an enum Postgres column type. import {table, string, enumeration} from '@rocicorp/zero' const user = table('user') .columns({ id: string(), name: string(), mood: enumeration<'happy' | 'sad' | 'taco'>() }) .primaryKey('id') Custom JSON Types Use the json helper to define a column that stores a JSON-compatible value: import {table, string, json} from '@rocicorp/zero' const user = table('user') .columns({ id: string(), name: string(), settings: json<{theme: 'light' | 'dark'}>() }) .primaryKey('id') Compound Primary Keys Pass multiple columns to primaryKey to define a compound primary key: const user = table('user') .columns({ orgID: string(), userID: string(), name: string() }) .primaryKey('orgID', 'userID') Relationships Use the relationships function to define relationships between tables. Use the one and many helpers to define singular and plural relationships, respectively: const messageRelationships = relationships( message, ({one, many}) => ({ sender: one({ sourceField: ['senderID'], destField: ['id'], destSchema: user }), replies: many({ sourceField: ['id'], destSchema: message, destField: ['parentMessageID'] }) }) ) This creates \"sender\" and \"replies\" relationships that can later be queried with the related ZQL clause: const messagesWithSenderAndReplies = z.query.messages .related('sender') .related('replies') This will return an object for each message row. Each message will have a sender field that is a single User object or null, and a replies field that is an array of Message objects. Many-to-Many Relationships You can create many-to-many relationships by chaining the relationship definitions. Assuming issue and label tables, along with an issueLabel junction table, you can define a labels relationship like this: const issueRelationships = relationships( issue, ({many}) => ({ labels: many( { sourceField: ['id'], destSchema: issueLabel, destField: ['issueID'] }, { sourceField: ['labelID'], destSchema: label, destField: ['id'] } ) }) ) See https://bugs.rocicorp.dev/issue/3454. Compound Keys Relationships Relationships can traverse compound keys. Imagine a user table with a compound primary key of orgID and userID, and a message table with a related senderOrgID and senderUserID. This can be represented in your schema with: const messageRelationships = relationships( message, ({one}) => ({ sender: one({ sourceField: ['senderOrgID', 'senderUserID'], destSchema: user, destField: ['orgID', 'userID'] }) }) ) Circular Relationships Circular relationships are fully supported: const commentRelationships = relationships( comment, ({one}) => ({ parent: one({ sourceField: ['parentID'], destSchema: comment, destField: ['id'] }) }) ) Database Schemas Use createSchema to define the entire Zero schema: import {createSchema} from '@rocicorp/zero' export const schema = createSchema({ tables: [user, medium, message], relationships: [ userRelationships, mediumRelationships, messageRelationships ] }) Default Type Parameter Use DefaultTypes to register the your Schema type with Zero: declare module '@rocicorp/zero' { interface DefaultTypes { schema: Schema } } This prevents having to pass Schema manually to every Zero API. Migrations Zero uses TypeScript-style structural typing to detect schema changes and implement smooth migrations. How it Works When the Zero client connects to zero-cache it sends a copy of the schema it was constructed with. zero-cache compares this schema to the one it has, and rejects the connection with a special error code if the schema is incompatible. By default, the Zero client handles this error code by calling location.reload(). The intent is to request a newer version of the app that has been updated to handle the new server schema. It's important to update the database schema first, then the app. Otherwise a reload loop will occur. If a reload loop does occur, Zero uses exponential backoff to avoid overloading the server. If you want to change or delay this reload, you can do so by providing the onUpdateNeeded constructor parameter: new Zero({ onUpdateNeeded: updateReason => { if (reason.type === 'SchemaVersionNotSupported') { // Do something custom here, like show a banner. // When you're ready, call `location.reload()`. } } }) If the schema changes in a compatible way while a client is running, zero-cache syncs the schema change to the client so that it's ready when the app reloads. If the schema changes in an incompatible way while a client is running, zero-cache will close the client connection with the same error code as above. Schema Change Process Like other database-backed applications, Zero schema migrations generally follow an \"expand/migrate/contract\" pattern: Implement and run an \"expand\" migration on the backend that is backwards compatible with existing schemas. Add new columns or tables, plus any defaults and triggers needed for compatibility with existing clients. Update and deploy the API and client app to use the new schema. After a grace period, implement and run a \"contract\" migration on the backend to drop or rename obsolete columns/tables. Steps 1 and 2 can generally be done as part of a single deploy in your CI pipeline, but step 3 should be weeks later, when most open clients have refreshed the application. See Rolling Updates for more details. Certain schema changes require special handling in Postgres. See Schema Changes for details.",
    "headings": [
      {
        "text": "Generating from Database",
        "id": "generating-from-database"
      },
      {
        "text": "Writing by Hand",
        "id": "writing-by-hand"
      },
      {
        "text": "Table Schemas",
        "id": "table-schemas"
      },
      {
        "text": "Name Mapping",
        "id": "name-mapping"
      },
      {
        "text": "Multiple Schemas",
        "id": "multiple-schemas"
      },
      {
        "text": "Optional Columns",
        "id": "optional-columns"
      },
      {
        "text": "Enumerations",
        "id": "enumerations"
      },
      {
        "text": "Custom JSON Types",
        "id": "custom-json-types"
      },
      {
        "text": "Compound Primary Keys",
        "id": "compound-primary-keys"
      },
      {
        "text": "Relationships",
        "id": "relationships"
      },
      {
        "text": "Many-to-Many Relationships",
        "id": "many-to-many-relationships"
      },
      {
        "text": "Compound Keys Relationships",
        "id": "compound-keys-relationships"
      },
      {
        "text": "Circular Relationships",
        "id": "circular-relationships"
      },
      {
        "text": "Database Schemas",
        "id": "database-schemas"
      },
      {
        "text": "Default Type Parameter",
        "id": "default-type-parameter"
      },
      {
        "text": "Migrations",
        "id": "migrations"
      },
      {
        "text": "How it Works",
        "id": "how-it-works"
      },
      {
        "text": "Schema Change Process",
        "id": "schema-change-process"
      }
    ],
    "kind": "page"
  },
  {
    "id": "389-schema#generating-from-database",
    "title": "Zero Schema",
    "searchTitle": "Generating from Database",
    "sectionTitle": "Generating from Database",
    "sectionId": "generating-from-database",
    "url": "/docs/schema",
    "content": "If you use Drizzle or Prisma ORM, you can generate schema.ts with drizzle-zero or prisma-zero: npm install -D drizzle-zero npx drizzle-zero generatepnpm add -D drizzle-zero pnpm dlx drizzle-zero generatebun add -D drizzle-zero bunx drizzle-zero generateyarn add -D drizzle-zero yarn dlx drizzle-zero generatenpm install -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } npx prisma generatepnpm add -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } pnpx prisma generatebun add -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } bunx prisma generateyarn add -D prisma-zero # Add this to your prisma schema: # generator zero { # provider = \"prisma-zero\" # } yarn prisma generate We'd love more! See the source for drizzle-zero and prisma-zero as a guide, or reach out on Discord with questions.",
    "kind": "section"
  },
  {
    "id": "390-schema#writing-by-hand",
    "title": "Zero Schema",
    "searchTitle": "Writing by Hand",
    "sectionTitle": "Writing by Hand",
    "sectionId": "writing-by-hand",
    "url": "/docs/schema",
    "content": "You can also write Zero schemas by hand for full control. Table Schemas Use the table function to define each table in your Zero schema: import {table, string, boolean} from '@rocicorp/zero' const user = table('user') .columns({ id: string(), name: string(), partner: boolean() }) .primaryKey('id') Column types are defined with the boolean(), number(), string(), json(), and enumeration() helpers. See Column Types for how database types are mapped to these types. Name Mapping Use from() to map a TypeScript table or column name to a different database name: const userPref = table('userPref') // Map TS \"userPref\" to DB name \"user_pref\" .from('user_pref') .columns({ id: string(), // Map TS \"orgID\" to DB name \"org_id\" orgID: string().from('org_id') }) Multiple Schemas You can also use from() to access other Postgres schemas: // Sync the \"event\" table from the \"analytics\" schema. const event = table('event').from('analytics.event') Optional Columns Columns can be marked optional. This corresponds to the SQL concept nullable. const user = table('user') .columns({ id: string(), name: string(), nickName: string().optional() }) .primaryKey('id') An optional column can store a value of the specified type or null to mean no value. Note that null and undefined mean different things when working with Zero rows. When reading, if a column is optional, Zero can return null for that field. undefined is not used at all when Reading from Zero. When writing, you can specify null for an optional field to explicitly write null to the datastore, unsetting any previous value. For create and upsert you can set optional fields to undefined (or leave the field off completely) to take the default value as specified by backend schema for that column. For update you can set any non-PK field to undefined to leave the previous value unmodified. Enumerations Use the enumeration helper to define a column that can only take on a specific set of values. This is most often used alongside an enum Postgres column type. import {table, string, enumeration} from '@rocicorp/zero' const user = table('user') .columns({ id: string(), name: string(), mood: enumeration<'happy' | 'sad' | 'taco'>() }) .primaryKey('id') Custom JSON Types Use the json helper to define a column that stores a JSON-compatible value: import {table, string, json} from '@rocicorp/zero' const user = table('user') .columns({ id: string(), name: string(), settings: json<{theme: 'light' | 'dark'}>() }) .primaryKey('id') Compound Primary Keys Pass multiple columns to primaryKey to define a compound primary key: const user = table('user') .columns({ orgID: string(), userID: string(), name: string() }) .primaryKey('orgID', 'userID') Relationships Use the relationships function to define relationships between tables. Use the one and many helpers to define singular and plural relationships, respectively: const messageRelationships = relationships( message, ({one, many}) => ({ sender: one({ sourceField: ['senderID'], destField: ['id'], destSchema: user }), replies: many({ sourceField: ['id'], destSchema: message, destField: ['parentMessageID'] }) }) ) This creates \"sender\" and \"replies\" relationships that can later be queried with the related ZQL clause: const messagesWithSenderAndReplies = z.query.messages .related('sender') .related('replies') This will return an object for each message row. Each message will have a sender field that is a single User object or null, and a replies field that is an array of Message objects. Many-to-Many Relationships You can create many-to-many relationships by chaining the relationship definitions. Assuming issue and label tables, along with an issueLabel junction table, you can define a labels relationship like this: const issueRelationships = relationships( issue, ({many}) => ({ labels: many( { sourceField: ['id'], destSchema: issueLabel, destField: ['issueID'] }, { sourceField: ['labelID'], destSchema: label, destField: ['id'] } ) }) ) See https://bugs.rocicorp.dev/issue/3454. Compound Keys Relationships Relationships can traverse compound keys. Imagine a user table with a compound primary key of orgID and userID, and a message table with a related senderOrgID and senderUserID. This can be represented in your schema with: const messageRelationships = relationships( message, ({one}) => ({ sender: one({ sourceField: ['senderOrgID', 'senderUserID'], destSchema: user, destField: ['orgID', 'userID'] }) }) ) Circular Relationships Circular relationships are fully supported: const commentRelationships = relationships( comment, ({one}) => ({ parent: one({ sourceField: ['parentID'], destSchema: comment, destField: ['id'] }) }) ) Database Schemas Use createSchema to define the entire Zero schema: import {createSchema} from '@rocicorp/zero' export const schema = createSchema({ tables: [user, medium, message], relationships: [ userRelationships, mediumRelationships, messageRelationships ] }) Default Type Parameter Use DefaultTypes to register the your Schema type with Zero: declare module '@rocicorp/zero' { interface DefaultTypes { schema: Schema } } This prevents having to pass Schema manually to every Zero API.",
    "kind": "section"
  },
  {
    "id": "391-schema#table-schemas",
    "title": "Zero Schema",
    "searchTitle": "Table Schemas",
    "sectionTitle": "Table Schemas",
    "sectionId": "table-schemas",
    "url": "/docs/schema",
    "content": "Use the table function to define each table in your Zero schema: import {table, string, boolean} from '@rocicorp/zero' const user = table('user') .columns({ id: string(), name: string(), partner: boolean() }) .primaryKey('id') Column types are defined with the boolean(), number(), string(), json(), and enumeration() helpers. See Column Types for how database types are mapped to these types. Name Mapping Use from() to map a TypeScript table or column name to a different database name: const userPref = table('userPref') // Map TS \"userPref\" to DB name \"user_pref\" .from('user_pref') .columns({ id: string(), // Map TS \"orgID\" to DB name \"org_id\" orgID: string().from('org_id') }) Multiple Schemas You can also use from() to access other Postgres schemas: // Sync the \"event\" table from the \"analytics\" schema. const event = table('event').from('analytics.event') Optional Columns Columns can be marked optional. This corresponds to the SQL concept nullable. const user = table('user') .columns({ id: string(), name: string(), nickName: string().optional() }) .primaryKey('id') An optional column can store a value of the specified type or null to mean no value. Note that null and undefined mean different things when working with Zero rows. When reading, if a column is optional, Zero can return null for that field. undefined is not used at all when Reading from Zero. When writing, you can specify null for an optional field to explicitly write null to the datastore, unsetting any previous value. For create and upsert you can set optional fields to undefined (or leave the field off completely) to take the default value as specified by backend schema for that column. For update you can set any non-PK field to undefined to leave the previous value unmodified. Enumerations Use the enumeration helper to define a column that can only take on a specific set of values. This is most often used alongside an enum Postgres column type. import {table, string, enumeration} from '@rocicorp/zero' const user = table('user') .columns({ id: string(), name: string(), mood: enumeration<'happy' | 'sad' | 'taco'>() }) .primaryKey('id') Custom JSON Types Use the json helper to define a column that stores a JSON-compatible value: import {table, string, json} from '@rocicorp/zero' const user = table('user') .columns({ id: string(), name: string(), settings: json<{theme: 'light' | 'dark'}>() }) .primaryKey('id') Compound Primary Keys Pass multiple columns to primaryKey to define a compound primary key: const user = table('user') .columns({ orgID: string(), userID: string(), name: string() }) .primaryKey('orgID', 'userID')",
    "kind": "section"
  },
  {
    "id": "392-schema#name-mapping",
    "title": "Zero Schema",
    "searchTitle": "Name Mapping",
    "sectionTitle": "Name Mapping",
    "sectionId": "name-mapping",
    "url": "/docs/schema",
    "content": "Use from() to map a TypeScript table or column name to a different database name: const userPref = table('userPref') // Map TS \"userPref\" to DB name \"user_pref\" .from('user_pref') .columns({ id: string(), // Map TS \"orgID\" to DB name \"org_id\" orgID: string().from('org_id') })",
    "kind": "section"
  },
  {
    "id": "393-schema#multiple-schemas",
    "title": "Zero Schema",
    "searchTitle": "Multiple Schemas",
    "sectionTitle": "Multiple Schemas",
    "sectionId": "multiple-schemas",
    "url": "/docs/schema",
    "content": "You can also use from() to access other Postgres schemas: // Sync the \"event\" table from the \"analytics\" schema. const event = table('event').from('analytics.event')",
    "kind": "section"
  },
  {
    "id": "394-schema#optional-columns",
    "title": "Zero Schema",
    "searchTitle": "Optional Columns",
    "sectionTitle": "Optional Columns",
    "sectionId": "optional-columns",
    "url": "/docs/schema",
    "content": "Columns can be marked optional. This corresponds to the SQL concept nullable. const user = table('user') .columns({ id: string(), name: string(), nickName: string().optional() }) .primaryKey('id') An optional column can store a value of the specified type or null to mean no value. Note that null and undefined mean different things when working with Zero rows. When reading, if a column is optional, Zero can return null for that field. undefined is not used at all when Reading from Zero. When writing, you can specify null for an optional field to explicitly write null to the datastore, unsetting any previous value. For create and upsert you can set optional fields to undefined (or leave the field off completely) to take the default value as specified by backend schema for that column. For update you can set any non-PK field to undefined to leave the previous value unmodified.",
    "kind": "section"
  },
  {
    "id": "395-schema#enumerations",
    "title": "Zero Schema",
    "searchTitle": "Enumerations",
    "sectionTitle": "Enumerations",
    "sectionId": "enumerations",
    "url": "/docs/schema",
    "content": "Use the enumeration helper to define a column that can only take on a specific set of values. This is most often used alongside an enum Postgres column type. import {table, string, enumeration} from '@rocicorp/zero' const user = table('user') .columns({ id: string(), name: string(), mood: enumeration<'happy' | 'sad' | 'taco'>() }) .primaryKey('id')",
    "kind": "section"
  },
  {
    "id": "396-schema#custom-json-types",
    "title": "Zero Schema",
    "searchTitle": "Custom JSON Types",
    "sectionTitle": "Custom JSON Types",
    "sectionId": "custom-json-types",
    "url": "/docs/schema",
    "content": "Use the json helper to define a column that stores a JSON-compatible value: import {table, string, json} from '@rocicorp/zero' const user = table('user') .columns({ id: string(), name: string(), settings: json<{theme: 'light' | 'dark'}>() }) .primaryKey('id')",
    "kind": "section"
  },
  {
    "id": "397-schema#compound-primary-keys",
    "title": "Zero Schema",
    "searchTitle": "Compound Primary Keys",
    "sectionTitle": "Compound Primary Keys",
    "sectionId": "compound-primary-keys",
    "url": "/docs/schema",
    "content": "Pass multiple columns to primaryKey to define a compound primary key: const user = table('user') .columns({ orgID: string(), userID: string(), name: string() }) .primaryKey('orgID', 'userID')",
    "kind": "section"
  },
  {
    "id": "398-schema#relationships",
    "title": "Zero Schema",
    "searchTitle": "Relationships",
    "sectionTitle": "Relationships",
    "sectionId": "relationships",
    "url": "/docs/schema",
    "content": "Use the relationships function to define relationships between tables. Use the one and many helpers to define singular and plural relationships, respectively: const messageRelationships = relationships( message, ({one, many}) => ({ sender: one({ sourceField: ['senderID'], destField: ['id'], destSchema: user }), replies: many({ sourceField: ['id'], destSchema: message, destField: ['parentMessageID'] }) }) ) This creates \"sender\" and \"replies\" relationships that can later be queried with the related ZQL clause: const messagesWithSenderAndReplies = z.query.messages .related('sender') .related('replies') This will return an object for each message row. Each message will have a sender field that is a single User object or null, and a replies field that is an array of Message objects. Many-to-Many Relationships You can create many-to-many relationships by chaining the relationship definitions. Assuming issue and label tables, along with an issueLabel junction table, you can define a labels relationship like this: const issueRelationships = relationships( issue, ({many}) => ({ labels: many( { sourceField: ['id'], destSchema: issueLabel, destField: ['issueID'] }, { sourceField: ['labelID'], destSchema: label, destField: ['id'] } ) }) ) See https://bugs.rocicorp.dev/issue/3454. Compound Keys Relationships Relationships can traverse compound keys. Imagine a user table with a compound primary key of orgID and userID, and a message table with a related senderOrgID and senderUserID. This can be represented in your schema with: const messageRelationships = relationships( message, ({one}) => ({ sender: one({ sourceField: ['senderOrgID', 'senderUserID'], destSchema: user, destField: ['orgID', 'userID'] }) }) ) Circular Relationships Circular relationships are fully supported: const commentRelationships = relationships( comment, ({one}) => ({ parent: one({ sourceField: ['parentID'], destSchema: comment, destField: ['id'] }) }) )",
    "kind": "section"
  },
  {
    "id": "399-schema#many-to-many-relationships",
    "title": "Zero Schema",
    "searchTitle": "Many-to-Many Relationships",
    "sectionTitle": "Many-to-Many Relationships",
    "sectionId": "many-to-many-relationships",
    "url": "/docs/schema",
    "content": "You can create many-to-many relationships by chaining the relationship definitions. Assuming issue and label tables, along with an issueLabel junction table, you can define a labels relationship like this: const issueRelationships = relationships( issue, ({many}) => ({ labels: many( { sourceField: ['id'], destSchema: issueLabel, destField: ['issueID'] }, { sourceField: ['labelID'], destSchema: label, destField: ['id'] } ) }) ) See https://bugs.rocicorp.dev/issue/3454.",
    "kind": "section"
  },
  {
    "id": "400-schema#compound-keys-relationships",
    "title": "Zero Schema",
    "searchTitle": "Compound Keys Relationships",
    "sectionTitle": "Compound Keys Relationships",
    "sectionId": "compound-keys-relationships",
    "url": "/docs/schema",
    "content": "Relationships can traverse compound keys. Imagine a user table with a compound primary key of orgID and userID, and a message table with a related senderOrgID and senderUserID. This can be represented in your schema with: const messageRelationships = relationships( message, ({one}) => ({ sender: one({ sourceField: ['senderOrgID', 'senderUserID'], destSchema: user, destField: ['orgID', 'userID'] }) }) )",
    "kind": "section"
  },
  {
    "id": "401-schema#circular-relationships",
    "title": "Zero Schema",
    "searchTitle": "Circular Relationships",
    "sectionTitle": "Circular Relationships",
    "sectionId": "circular-relationships",
    "url": "/docs/schema",
    "content": "Circular relationships are fully supported: const commentRelationships = relationships( comment, ({one}) => ({ parent: one({ sourceField: ['parentID'], destSchema: comment, destField: ['id'] }) }) )",
    "kind": "section"
  },
  {
    "id": "402-schema#database-schemas",
    "title": "Zero Schema",
    "searchTitle": "Database Schemas",
    "sectionTitle": "Database Schemas",
    "sectionId": "database-schemas",
    "url": "/docs/schema",
    "content": "Use createSchema to define the entire Zero schema: import {createSchema} from '@rocicorp/zero' export const schema = createSchema({ tables: [user, medium, message], relationships: [ userRelationships, mediumRelationships, messageRelationships ] })",
    "kind": "section"
  },
  {
    "id": "403-schema#default-type-parameter",
    "title": "Zero Schema",
    "searchTitle": "Default Type Parameter",
    "sectionTitle": "Default Type Parameter",
    "sectionId": "default-type-parameter",
    "url": "/docs/schema",
    "content": "Use DefaultTypes to register the your Schema type with Zero: declare module '@rocicorp/zero' { interface DefaultTypes { schema: Schema } } This prevents having to pass Schema manually to every Zero API.",
    "kind": "section"
  },
  {
    "id": "404-schema#migrations",
    "title": "Zero Schema",
    "searchTitle": "Migrations",
    "sectionTitle": "Migrations",
    "sectionId": "migrations",
    "url": "/docs/schema",
    "content": "Zero uses TypeScript-style structural typing to detect schema changes and implement smooth migrations. How it Works When the Zero client connects to zero-cache it sends a copy of the schema it was constructed with. zero-cache compares this schema to the one it has, and rejects the connection with a special error code if the schema is incompatible. By default, the Zero client handles this error code by calling location.reload(). The intent is to request a newer version of the app that has been updated to handle the new server schema. It's important to update the database schema first, then the app. Otherwise a reload loop will occur. If a reload loop does occur, Zero uses exponential backoff to avoid overloading the server. If you want to change or delay this reload, you can do so by providing the onUpdateNeeded constructor parameter: new Zero({ onUpdateNeeded: updateReason => { if (reason.type === 'SchemaVersionNotSupported') { // Do something custom here, like show a banner. // When you're ready, call `location.reload()`. } } }) If the schema changes in a compatible way while a client is running, zero-cache syncs the schema change to the client so that it's ready when the app reloads. If the schema changes in an incompatible way while a client is running, zero-cache will close the client connection with the same error code as above. Schema Change Process Like other database-backed applications, Zero schema migrations generally follow an \"expand/migrate/contract\" pattern: Implement and run an \"expand\" migration on the backend that is backwards compatible with existing schemas. Add new columns or tables, plus any defaults and triggers needed for compatibility with existing clients. Update and deploy the API and client app to use the new schema. After a grace period, implement and run a \"contract\" migration on the backend to drop or rename obsolete columns/tables. Steps 1 and 2 can generally be done as part of a single deploy in your CI pipeline, but step 3 should be weeks later, when most open clients have refreshed the application. See Rolling Updates for more details. Certain schema changes require special handling in Postgres. See Schema Changes for details.",
    "kind": "section"
  },
  {
    "id": "405-schema#how-it-works",
    "title": "Zero Schema",
    "searchTitle": "How it Works",
    "sectionTitle": "How it Works",
    "sectionId": "how-it-works",
    "url": "/docs/schema",
    "content": "When the Zero client connects to zero-cache it sends a copy of the schema it was constructed with. zero-cache compares this schema to the one it has, and rejects the connection with a special error code if the schema is incompatible. By default, the Zero client handles this error code by calling location.reload(). The intent is to request a newer version of the app that has been updated to handle the new server schema. It's important to update the database schema first, then the app. Otherwise a reload loop will occur. If a reload loop does occur, Zero uses exponential backoff to avoid overloading the server. If you want to change or delay this reload, you can do so by providing the onUpdateNeeded constructor parameter: new Zero({ onUpdateNeeded: updateReason => { if (reason.type === 'SchemaVersionNotSupported') { // Do something custom here, like show a banner. // When you're ready, call `location.reload()`. } } }) If the schema changes in a compatible way while a client is running, zero-cache syncs the schema change to the client so that it's ready when the app reloads. If the schema changes in an incompatible way while a client is running, zero-cache will close the client connection with the same error code as above.",
    "kind": "section"
  },
  {
    "id": "406-schema#schema-change-process",
    "title": "Zero Schema",
    "searchTitle": "Schema Change Process",
    "sectionTitle": "Schema Change Process",
    "sectionId": "schema-change-process",
    "url": "/docs/schema",
    "content": "Like other database-backed applications, Zero schema migrations generally follow an \"expand/migrate/contract\" pattern: Implement and run an \"expand\" migration on the backend that is backwards compatible with existing schemas. Add new columns or tables, plus any defaults and triggers needed for compatibility with existing clients. Update and deploy the API and client app to use the new schema. After a grace period, implement and run a \"contract\" migration on the backend to drop or rename obsolete columns/tables. Steps 1 and 2 can generally be done as part of a single deploy in your CI pipeline, but step 3 should be weeks later, when most open clients have refreshed the application. See Rolling Updates for more details. Certain schema changes require special handling in Postgres. See Schema Changes for details.",
    "kind": "section"
  },
  {
    "id": "55-server-zql",
    "title": "ZQL on the Server",
    "searchTitle": "ZQL on the Server",
    "url": "/docs/server-zql",
    "content": "The Zero package includes utilities to run ZQL on the server directly against your upstream Postgres database. This is useful for many reasons: It allows mutators to read data using ZQL to check permissions or invariants. You can use ZQL to implement standard REST endpoints, allowing you to share code with mutators. In the future (but not yet implemented), this can support server-side rendering. ZQLDatabase currently does a read of your postgres schema before every transaction. This is fine for most usages, but for high scale it may become a problem. Let us know if you need a fix for this. Creating a Database To run ZQL on the database, you will create a ZQLDatabase instance. Zero ships with several built-in factories for popular Postgres bindings libraries. // app/api/mutate/db-provider.ts import {zeroDrizzle} from '@rocicorp/zero/server/adapters/drizzle' import {schema} from '../../zero/schema.ts' import * as drizzleSchema from '../../drizzle/schema.ts' // pass a drizzle client instance. for example: export const drizzleClient = drizzle(pool, { schema: drizzleSchema }) export const dbProvider = zeroDrizzle(schema, drizzleClient) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } }// app/api/mutate/db-provider.ts import {PrismaPg} from '@prisma/adapter-pg' import {PrismaClient} from '@prisma/client' import {zeroPrisma} from '@rocicorp/zero/server/adapters/prisma' import {schema} from '../../zero/schema.ts' const prisma = new PrismaClient({ adapter: new PrismaPg({ connectionString: process.env.ZERO_UPSTREAM_DB! }) }) export const dbProvider = zeroPrisma(schema, prisma) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } }// app/api/mutate/db-provider.ts import {zeroNodePg} from '@rocicorp/zero/server/adapters/pg' import {Pool} from 'pg' import {schema} from '../../zero/schema.ts' const pool = new Pool({ connectionString: process.env.ZERO_UPSTREAM_DB! }) export const dbProvider = zeroNodePg(schema, pool) // You can also pass a client instead of a pool: // // const client = new Client({ // connectionString: process.env.ZERO_UPSTREAM_DB! // }) // await client.connect() // export const dbProvider = zeroNodePg(schema, client) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } }// app/api/mutate/db-provider.ts import {zeroPostgresJS} from '@rocicorp/zero/server/adapters/postgresjs' import postgres from 'postgres' import {schema} from '../../zero/schema.ts' const sql = postgres(process.env.ZERO_UPSTREAM_DB!) export const dbProvider = zeroPostgresJS(schema, sql) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } } Within your mutators, you can access the underlying transaction via tx.dbTransaction.wrappedTransaction: // mutators.ts export const mutators = defineMutators({ createUser: defineMutator( z.object({id: z.string(), name: z.string()}), async ({tx, args: {id, name}}) => { if (tx.location === 'server') { await tx.dbTransaction.wrappedTransaction .insert(drizzleSchema.user) .values({id, name}) } } ) })// mutators.ts export const mutators = defineMutators({ createUser: defineMutator( z.object({id: z.string(), name: z.string()}), async ({tx, args: {id, name}}) => { if (tx.location === 'server') { await tx.dbTransaction.wrappedTransaction.user.create( { data: { id, name, status: 'active' } } ) } } ) })// mutators.ts export const mutators = defineMutators({ createUser: defineMutator( z.object({id: z.string(), name: z.string()}), async ({tx, args: {id, name}}) => { if (tx.location === 'server') { await tx.dbTransaction.wrappedTransaction.query( 'insert into \"user\" (id, name) values ($1, $2) returning *', [id, name] ) } } ) })// mutators.ts export const mutators = defineMutators({ createUser: defineMutator( z.object({id: z.string(), name: z.string()}), async ({tx, args: {id, name}}) => { if (tx.location === 'server') { await tx.dbTransaction.wrappedTransaction< {id: string; name: string}[] >`insert into \"user\" (id, name) values (${id}, ${name}) returning *` } } ) }) Custom Database To implement support for some other Postgres bindings library, you will implement the DBConnection interface. See the implementations for the existing adapters for examples. Running ZQL Once you have an instance of ZQLDatabase, use the transaction() method to run ZQL: await dbProvider.transaction(async tx => { // await tx.mutate... // await tx.query... // await myMutator.fn({tx, ctx, args}) }) SSR Zero doesn't yet have the wiring setup in its bindings layers to really nicely support server-side rendering (patches welcome though!). For now, we don't recommend using Zero with SSR. Use your framework's recommended pattern to prevent SSR execution: import {lazy} from 'react' // Use React lazy to defer loading the ZeroProvider const ZeroProvider = lazy(() => import('@rocicorp/zero/react').then(mod => ({ default: mod.ZeroProvider })) ) function Root() { return ( <ZeroProvider> <App /> </ZeroProvider> ) }// Mark client-only components 'use client' import {ZeroProvider} from '@rocicorp/zero/react' export default function Root() { return ( <ZeroProvider> <App /> </ZeroProvider> ) }import {clientOnly} from '@solidjs/start' const ZeroProvider = clientOnly(async () => { // Optionally dynamic import to code-split return import('@rocicorp/zero/solid').then(mod => ({ default: mod.ZeroProvider })) }) export default function Root() { return ( <ZeroProvider> <App /> </ZeroProvider> ) }",
    "headings": [
      {
        "text": "Creating a Database",
        "id": "creating-a-database"
      },
      {
        "text": "Custom Database",
        "id": "custom-database"
      },
      {
        "text": "Running ZQL",
        "id": "running-zql"
      },
      {
        "text": "SSR",
        "id": "ssr"
      }
    ],
    "kind": "page"
  },
  {
    "id": "407-server-zql#creating-a-database",
    "title": "ZQL on the Server",
    "searchTitle": "Creating a Database",
    "sectionTitle": "Creating a Database",
    "sectionId": "creating-a-database",
    "url": "/docs/server-zql",
    "content": "To run ZQL on the database, you will create a ZQLDatabase instance. Zero ships with several built-in factories for popular Postgres bindings libraries. // app/api/mutate/db-provider.ts import {zeroDrizzle} from '@rocicorp/zero/server/adapters/drizzle' import {schema} from '../../zero/schema.ts' import * as drizzleSchema from '../../drizzle/schema.ts' // pass a drizzle client instance. for example: export const drizzleClient = drizzle(pool, { schema: drizzleSchema }) export const dbProvider = zeroDrizzle(schema, drizzleClient) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } }// app/api/mutate/db-provider.ts import {PrismaPg} from '@prisma/adapter-pg' import {PrismaClient} from '@prisma/client' import {zeroPrisma} from '@rocicorp/zero/server/adapters/prisma' import {schema} from '../../zero/schema.ts' const prisma = new PrismaClient({ adapter: new PrismaPg({ connectionString: process.env.ZERO_UPSTREAM_DB! }) }) export const dbProvider = zeroPrisma(schema, prisma) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } }// app/api/mutate/db-provider.ts import {zeroNodePg} from '@rocicorp/zero/server/adapters/pg' import {Pool} from 'pg' import {schema} from '../../zero/schema.ts' const pool = new Pool({ connectionString: process.env.ZERO_UPSTREAM_DB! }) export const dbProvider = zeroNodePg(schema, pool) // You can also pass a client instead of a pool: // // const client = new Client({ // connectionString: process.env.ZERO_UPSTREAM_DB! // }) // await client.connect() // export const dbProvider = zeroNodePg(schema, client) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } }// app/api/mutate/db-provider.ts import {zeroPostgresJS} from '@rocicorp/zero/server/adapters/postgresjs' import postgres from 'postgres' import {schema} from '../../zero/schema.ts' const sql = postgres(process.env.ZERO_UPSTREAM_DB!) export const dbProvider = zeroPostgresJS(schema, sql) // Register the database provider for type safety declare module '@rocicorp/zero' { interface DefaultTypes { dbProvider: typeof dbProvider } } Within your mutators, you can access the underlying transaction via tx.dbTransaction.wrappedTransaction: // mutators.ts export const mutators = defineMutators({ createUser: defineMutator( z.object({id: z.string(), name: z.string()}), async ({tx, args: {id, name}}) => { if (tx.location === 'server') { await tx.dbTransaction.wrappedTransaction .insert(drizzleSchema.user) .values({id, name}) } } ) })// mutators.ts export const mutators = defineMutators({ createUser: defineMutator( z.object({id: z.string(), name: z.string()}), async ({tx, args: {id, name}}) => { if (tx.location === 'server') { await tx.dbTransaction.wrappedTransaction.user.create( { data: { id, name, status: 'active' } } ) } } ) })// mutators.ts export const mutators = defineMutators({ createUser: defineMutator( z.object({id: z.string(), name: z.string()}), async ({tx, args: {id, name}}) => { if (tx.location === 'server') { await tx.dbTransaction.wrappedTransaction.query( 'insert into \"user\" (id, name) values ($1, $2) returning *', [id, name] ) } } ) })// mutators.ts export const mutators = defineMutators({ createUser: defineMutator( z.object({id: z.string(), name: z.string()}), async ({tx, args: {id, name}}) => { if (tx.location === 'server') { await tx.dbTransaction.wrappedTransaction< {id: string; name: string}[] >`insert into \"user\" (id, name) values (${id}, ${name}) returning *` } } ) }) Custom Database To implement support for some other Postgres bindings library, you will implement the DBConnection interface. See the implementations for the existing adapters for examples.",
    "kind": "section"
  },
  {
    "id": "408-server-zql#custom-database",
    "title": "ZQL on the Server",
    "searchTitle": "Custom Database",
    "sectionTitle": "Custom Database",
    "sectionId": "custom-database",
    "url": "/docs/server-zql",
    "content": "To implement support for some other Postgres bindings library, you will implement the DBConnection interface. See the implementations for the existing adapters for examples.",
    "kind": "section"
  },
  {
    "id": "409-server-zql#running-zql",
    "title": "ZQL on the Server",
    "searchTitle": "Running ZQL",
    "sectionTitle": "Running ZQL",
    "sectionId": "running-zql",
    "url": "/docs/server-zql",
    "content": "Once you have an instance of ZQLDatabase, use the transaction() method to run ZQL: await dbProvider.transaction(async tx => { // await tx.mutate... // await tx.query... // await myMutator.fn({tx, ctx, args}) })",
    "kind": "section"
  },
  {
    "id": "410-server-zql#ssr",
    "title": "ZQL on the Server",
    "searchTitle": "SSR",
    "sectionTitle": "SSR",
    "sectionId": "ssr",
    "url": "/docs/server-zql",
    "content": "Zero doesn't yet have the wiring setup in its bindings layers to really nicely support server-side rendering (patches welcome though!). For now, we don't recommend using Zero with SSR. Use your framework's recommended pattern to prevent SSR execution: import {lazy} from 'react' // Use React lazy to defer loading the ZeroProvider const ZeroProvider = lazy(() => import('@rocicorp/zero/react').then(mod => ({ default: mod.ZeroProvider })) ) function Root() { return ( <ZeroProvider> <App /> </ZeroProvider> ) }// Mark client-only components 'use client' import {ZeroProvider} from '@rocicorp/zero/react' export default function Root() { return ( <ZeroProvider> <App /> </ZeroProvider> ) }import {clientOnly} from '@solidjs/start' const ZeroProvider = clientOnly(async () => { // Optionally dynamic import to code-split return import('@rocicorp/zero/solid').then(mod => ({ default: mod.ZeroProvider })) }) export default function Root() { return ( <ZeroProvider> <App /> </ZeroProvider> ) }",
    "kind": "section"
  },
  {
    "id": "56-solidjs",
    "title": "SolidJS",
    "searchTitle": "SolidJS",
    "url": "/docs/solidjs",
    "content": "Zero has built-in support for Solid. Here‚Äôs what basic usage looks like: Setup Use the ZeroProvider component to setup Zero. It takes care of creating and destroying Zero instances reactively: import {ZeroProvider} from '@rocicorp/zero/solid' import {useSession} from 'my-auth-provider' import App from 'App.tsx' import {schema} from 'schema.ts' import {mutators} from 'mutators.ts' const cacheURL = import.meta.env.VITE_PUBLIC_ZERO_CACHE_URL! function Root() { const session = useSession() const {userID} = session const context = {userID} return ( <ZeroProvider {...{userID, context, cacheURL, schema, mutators}} > <App /> </ZeroProvider> ) } You can also pass a Zero instance to the ZeroProvider if you want to control the lifecycle of the Zero instance yourself: // ZeroProvider just sets up the context, it doesn't manage // the lifecycle of the Zero instance. <ZeroProvider zero={zero}> <App /> </ZeroProvider> Usage Use useQuery to run queries: import {useQuery} from '@rocicorp/zero/solid' import {queries} from 'queries.ts' function App() { const [posts] = useQuery(() => queries.posts.byStatus({status: 'draft'}) ) return ( <For each={posts()}> {post => ( <div key={post.id}> {post.title} - ({post.comments.length} comments) </div> )} </For> ) } Use useZero to get access to the Zero instance, for example to run mutators: import {useZero} from '@rocicorp/zero/solid' import {mutators} from 'mutators.ts' function CompleteButton({issueID}: {issueID: string}) { const zero = useZero() const onClick = () => { zero().mutate(mutators.issues.complete({id: issueID})) } return <button onClick={onClick}>Complete Issue</button> } Examples See the complete quickstart here: https://github.com/rocicorp/hello-zero-solid",
    "headings": [
      {
        "text": "Setup",
        "id": "setup"
      },
      {
        "text": "Usage",
        "id": "usage"
      },
      {
        "text": "Examples",
        "id": "examples"
      }
    ],
    "kind": "page"
  },
  {
    "id": "411-solidjs#setup",
    "title": "SolidJS",
    "searchTitle": "Setup",
    "sectionTitle": "Setup",
    "sectionId": "setup",
    "url": "/docs/solidjs",
    "content": "Use the ZeroProvider component to setup Zero. It takes care of creating and destroying Zero instances reactively: import {ZeroProvider} from '@rocicorp/zero/solid' import {useSession} from 'my-auth-provider' import App from 'App.tsx' import {schema} from 'schema.ts' import {mutators} from 'mutators.ts' const cacheURL = import.meta.env.VITE_PUBLIC_ZERO_CACHE_URL! function Root() { const session = useSession() const {userID} = session const context = {userID} return ( <ZeroProvider {...{userID, context, cacheURL, schema, mutators}} > <App /> </ZeroProvider> ) } You can also pass a Zero instance to the ZeroProvider if you want to control the lifecycle of the Zero instance yourself: // ZeroProvider just sets up the context, it doesn't manage // the lifecycle of the Zero instance. <ZeroProvider zero={zero}> <App /> </ZeroProvider>",
    "kind": "section"
  },
  {
    "id": "412-solidjs#usage",
    "title": "SolidJS",
    "searchTitle": "Usage",
    "sectionTitle": "Usage",
    "sectionId": "usage",
    "url": "/docs/solidjs",
    "content": "Use useQuery to run queries: import {useQuery} from '@rocicorp/zero/solid' import {queries} from 'queries.ts' function App() { const [posts] = useQuery(() => queries.posts.byStatus({status: 'draft'}) ) return ( <For each={posts()}> {post => ( <div key={post.id}> {post.title} - ({post.comments.length} comments) </div> )} </For> ) } Use useZero to get access to the Zero instance, for example to run mutators: import {useZero} from '@rocicorp/zero/solid' import {mutators} from 'mutators.ts' function CompleteButton({issueID}: {issueID: string}) { const zero = useZero() const onClick = () => { zero().mutate(mutators.issues.complete({id: issueID})) } return <button onClick={onClick}>Complete Issue</button> }",
    "kind": "section"
  },
  {
    "id": "413-solidjs#examples",
    "title": "SolidJS",
    "searchTitle": "Examples",
    "sectionTitle": "Examples",
    "sectionId": "examples",
    "url": "/docs/solidjs",
    "content": "See the complete quickstart here: https://github.com/rocicorp/hello-zero-solid",
    "kind": "section"
  },
  {
    "id": "57-status",
    "title": "Project Status",
    "searchTitle": "Project Status",
    "url": "/docs/status",
    "content": "Zero is a new sync engine based on a novel streaming query engine. This is an ambitious project at an early stage. You will encounter bugs. You may encounter pathologically slow queries. You are likely to encounter situations where ZQL is not powerful enough to express the query you want. That said, we are building Zero live. It has been running our own bug tracker for months, and is used in production by a small set of customer applications that are an extremely good fit. This page describes the current state of Zero at a high level. To understand whether Zero makes sense for you, please also see When to Use Zero. Platforms and Frameworks React, React Native, and SolidJS are fully supported. Svelte and Vue have community support. We have strong support for TanStack. Databases Most Postgres providers are supported. Drizzle and Prisma are fully supported. API The new APIs are still being refined and have some rough edges. Query Language Filters, sorts, limits, relationships, and exists are supported. Queries can have ttl to keep data synced across sessions. Aggregates (count, min, max, group-by) are not yet supported. Full-text search is not yet supported (you can sometimes simulate with ILIKE, though it scales linearly). Infinite/virtual scroll is possible, but we do not yet have a library/API for it. See zbugs source for how to do this. Performance Zero plans single-table and multi-table queries. You can also manually plan queries using the flip parameter. Zero has a basic console-based inspector that can help to understand query and sync performance. It does not yet have a GUI inspector. We share queries within a \"client group\" (e.g. all tabs in a browser), but not across groups. This means that if you have many users doing the same query, they will duplicate all that work server-side. Miscellaneous Running Zero requires deploying it yourself to AWS or similar. Running in a multinode, zero-downtime way is possible (we do it for zbugs), but significant effort. Running single node is easier, but updating the server takes it down for a minute or so (we are working on a SaaS solution).",
    "headings": [
      {
        "text": "Platforms and Frameworks",
        "id": "platforms-and-frameworks"
      },
      {
        "text": "Databases",
        "id": "databases"
      },
      {
        "text": "API",
        "id": "api"
      },
      {
        "text": "Query Language",
        "id": "query-language"
      },
      {
        "text": "Performance",
        "id": "performance"
      },
      {
        "text": "Miscellaneous",
        "id": "miscellaneous"
      }
    ],
    "kind": "page"
  },
  {
    "id": "414-status#platforms-and-frameworks",
    "title": "Project Status",
    "searchTitle": "Platforms and Frameworks",
    "sectionTitle": "Platforms and Frameworks",
    "sectionId": "platforms-and-frameworks",
    "url": "/docs/status",
    "content": "React, React Native, and SolidJS are fully supported. Svelte and Vue have community support. We have strong support for TanStack.",
    "kind": "section"
  },
  {
    "id": "415-status#databases",
    "title": "Project Status",
    "searchTitle": "Databases",
    "sectionTitle": "Databases",
    "sectionId": "databases",
    "url": "/docs/status",
    "content": "Most Postgres providers are supported. Drizzle and Prisma are fully supported.",
    "kind": "section"
  },
  {
    "id": "416-status#api",
    "title": "Project Status",
    "searchTitle": "API",
    "sectionTitle": "API",
    "sectionId": "api",
    "url": "/docs/status",
    "content": "The new APIs are still being refined and have some rough edges.",
    "kind": "section"
  },
  {
    "id": "417-status#query-language",
    "title": "Project Status",
    "searchTitle": "Query Language",
    "sectionTitle": "Query Language",
    "sectionId": "query-language",
    "url": "/docs/status",
    "content": "Filters, sorts, limits, relationships, and exists are supported. Queries can have ttl to keep data synced across sessions. Aggregates (count, min, max, group-by) are not yet supported. Full-text search is not yet supported (you can sometimes simulate with ILIKE, though it scales linearly). Infinite/virtual scroll is possible, but we do not yet have a library/API for it. See zbugs source for how to do this.",
    "kind": "section"
  },
  {
    "id": "418-status#performance",
    "title": "Project Status",
    "searchTitle": "Performance",
    "sectionTitle": "Performance",
    "sectionId": "performance",
    "url": "/docs/status",
    "content": "Zero plans single-table and multi-table queries. You can also manually plan queries using the flip parameter. Zero has a basic console-based inspector that can help to understand query and sync performance. It does not yet have a GUI inspector. We share queries within a \"client group\" (e.g. all tabs in a browser), but not across groups. This means that if you have many users doing the same query, they will duplicate all that work server-side.",
    "kind": "section"
  },
  {
    "id": "419-status#miscellaneous",
    "title": "Project Status",
    "searchTitle": "Miscellaneous",
    "sectionTitle": "Miscellaneous",
    "sectionId": "miscellaneous",
    "url": "/docs/status",
    "content": "Running Zero requires deploying it yourself to AWS or similar. Running in a multinode, zero-downtime way is possible (we do it for zbugs), but significant effort. Running single node is easier, but updating the server takes it down for a minute or so (we are working on a SaaS solution).",
    "kind": "section"
  },
  {
    "id": "58-sync",
    "title": "What is Sync?",
    "searchTitle": "What is Sync?",
    "url": "/docs/sync",
    "content": "We say that Zero is a sync engine. But what even is that? And why does it matter? A sync engine is a type of software that keeps multiple copies of changing data consistent across devices and users. When the data changes, the sync engine ensures that all copies reflect those changes. Problem Let's say you have some data that you want to read and write from multiple devices. The most common way to do this today is to put that data into a central database and expose access to it via APIs. This works, but has downsides: Slow access. Every read and write has to go to the server, adding hundreds of milliseconds to each interaction. Stale data. API responses are immediately stale. The client has no way to know when to refresh them. Users may make decisions based on old information, and the views on different devices diverge over time. Online-only. If the server or the user's network connection is down, the app stops working completely. Solution Sync engines can solve these problems by keeping a local copy of the data on each device. The app reads and writes only to the local copy, not to the network. The sync engine pushes changes back and forth between the local copy and the server in the background, when connectivity allows. If the sync engine allows writes from multiple devices, conflicts can occur. This is a central part of sync engine design, and different sync engines handle conflicts differently. Zero uses server reconciliation ‚Äì an elegant and flexible technique pioneered by the video game industry. This architecture can enable: Instant UI. Reads and writes are to local storage, effectively instant. Realtime updates. By running the sync engine continuously, users can see updates from other devices and users in realtime. The data is always fresh. Offline support. Because data is stored locally, it is possible to support at least limited offline access. For example, Zero supports read-only access while offline, and other sync engines support some limited offline writes. Sync engines also simplify the development of complex apps. Big parts of modern app development are just data plumbing: fetching data, updating data, caching data, invalidating caches, keeping different copies of data consistent, and so-on. A sync engine abstracts all this away and lets you focus on what your app actually does. History of Sync Sync Engines have been around a long time. The first mass-market sync engine was probably Lotus Notes, released way back in 1989! Since then, there has been a steady trickle of important software built on sync engines: Microsoft Exchange (1996) Google Docs (2006) Dropbox (2007) Figma (2016) Superhuman (2017) Linear (2019) But sync engines are very hard to build. Typically, a new custom sync engine is built for each application at great expense. Knowledge about the specific application and its data model must be built into each sync engine to correctly handle conflicts and partial sync. There have also been some attempts at general-purpose sync engines: Firebase Realtime Database (2012) - a cloud-hosted database and that syncs. PouchDB (2013) - a sync engine attachment for CouchDB. Realm (2016) - a mobile database with sync capabilities. Replicache (2020) - The predecessor to Zero, a JavaScript library for building collaborative applications with real-time sync. But all have suffered from one or more significant problems that have prevented widespread adoption: No support for fine-grained authorization Limited support for partial sync ‚Äì users have to sync all data, even if they only need a small subset Required adoption of non-standard backend databases or data models Limited ability to put custom business logic on read or write paths We are building Zero to address these limitations, and bring the benefits of sync to many more applications.",
    "headings": [
      {
        "text": "Problem",
        "id": "problem"
      },
      {
        "text": "Solution",
        "id": "solution"
      },
      {
        "text": "History of Sync",
        "id": "history-of-sync"
      }
    ],
    "kind": "page"
  },
  {
    "id": "420-sync#problem",
    "title": "What is Sync?",
    "searchTitle": "Problem",
    "sectionTitle": "Problem",
    "sectionId": "problem",
    "url": "/docs/sync",
    "content": "Let's say you have some data that you want to read and write from multiple devices. The most common way to do this today is to put that data into a central database and expose access to it via APIs. This works, but has downsides: Slow access. Every read and write has to go to the server, adding hundreds of milliseconds to each interaction. Stale data. API responses are immediately stale. The client has no way to know when to refresh them. Users may make decisions based on old information, and the views on different devices diverge over time. Online-only. If the server or the user's network connection is down, the app stops working completely.",
    "kind": "section"
  },
  {
    "id": "421-sync#solution",
    "title": "What is Sync?",
    "searchTitle": "Solution",
    "sectionTitle": "Solution",
    "sectionId": "solution",
    "url": "/docs/sync",
    "content": "Sync engines can solve these problems by keeping a local copy of the data on each device. The app reads and writes only to the local copy, not to the network. The sync engine pushes changes back and forth between the local copy and the server in the background, when connectivity allows. If the sync engine allows writes from multiple devices, conflicts can occur. This is a central part of sync engine design, and different sync engines handle conflicts differently. Zero uses server reconciliation ‚Äì an elegant and flexible technique pioneered by the video game industry. This architecture can enable: Instant UI. Reads and writes are to local storage, effectively instant. Realtime updates. By running the sync engine continuously, users can see updates from other devices and users in realtime. The data is always fresh. Offline support. Because data is stored locally, it is possible to support at least limited offline access. For example, Zero supports read-only access while offline, and other sync engines support some limited offline writes. Sync engines also simplify the development of complex apps. Big parts of modern app development are just data plumbing: fetching data, updating data, caching data, invalidating caches, keeping different copies of data consistent, and so-on. A sync engine abstracts all this away and lets you focus on what your app actually does.",
    "kind": "section"
  },
  {
    "id": "422-sync#history-of-sync",
    "title": "What is Sync?",
    "searchTitle": "History of Sync",
    "sectionTitle": "History of Sync",
    "sectionId": "history-of-sync",
    "url": "/docs/sync",
    "content": "Sync Engines have been around a long time. The first mass-market sync engine was probably Lotus Notes, released way back in 1989! Since then, there has been a steady trickle of important software built on sync engines: Microsoft Exchange (1996) Google Docs (2006) Dropbox (2007) Figma (2016) Superhuman (2017) Linear (2019) But sync engines are very hard to build. Typically, a new custom sync engine is built for each application at great expense. Knowledge about the specific application and its data model must be built into each sync engine to correctly handle conflicts and partial sync. There have also been some attempts at general-purpose sync engines: Firebase Realtime Database (2012) - a cloud-hosted database and that syncs. PouchDB (2013) - a sync engine attachment for CouchDB. Realm (2016) - a mobile database with sync capabilities. Replicache (2020) - The predecessor to Zero, a JavaScript library for building collaborative applications with real-time sync. But all have suffered from one or more significant problems that have prevented widespread adoption: No support for fine-grained authorization Limited support for partial sync ‚Äì users have to sync all data, even if they only need a small subset Required adoption of non-standard backend databases or data models Limited ability to put custom business logic on read or write paths We are building Zero to address these limitations, and bring the benefits of sync to many more applications.",
    "kind": "section"
  },
  {
    "id": "59-when-to-use",
    "title": "When To Use Zero",
    "searchTitle": "When To Use Zero",
    "url": "/docs/when-to-use",
    "content": "We are trying to make Zero a great choice for a wide variety of applications. But every tool has tradeoffs, and Zero especially so while in alpha. This page will help you understand if Zero is a good fit for you today. Zero Might be a Good Fit You want to sync only a small subset of data to client Zero's query-driven sync is a powerful solution for partial sync. You can define the data you want to sync with a set of Zero queries. By using partial sync, Zero apps can commonly load in < 1s, yet still maintain the interaction perf of sync. You need fine-grained read or write permissions Zero's mutators allow you to run arbitrary authorization, validation, or business logic on the write path. You can enforce that a write depends on what group a user is in, what has been shared with them, their role, etc. Read permissions are very expressive, allowing similar control over what data is synced to the client. You are building a traditional client-server web app Zero was designed from the ground up to be as close to a classic web app as a sync engine can be. If you have a traditional web app, you can try Zero side-by-side with your existing REST or GraphQL API, and incrementally migrate over time. You use PostgreSQL Some tools in our space require you to use a non-standard backend database or data model. Zero works with PostgreSQL, and uses your existing schema. Your app is broadly \"like Linear\" Zero is currently best suited for productivity apps with lots of interactivity. Interaction performance is very important to you Zero was built by people obsessed with interaction performance. If you share this goal you'll be going with the grain of Zero's design choices. Zero Might Not be a Good Fit You need the privacy or data ownership benefits of local-first Zero is not local-first. It's a client-server system with an authoritative server. You need to support offline writes or long periods offline Zero doesn't support offline writes yet. You are building a native mobile app Zero is written in TypeScript and only supports TypeScript clients. The total backend dataset is > ~100GB Zero stores a replica of your database (at least the subset you want to be syncable to clients) in a SQLite database owned by zero-cache. Zero's query engine is built assuming very fast local access to this replica (i.e., attached NVMe). But other setups are technically supported and work for smaller data. The ultimate size limit on the database that Zero can work with is the size limit of this SQLite database. So up to 45TB on EC2 at time of writing. However, most of our experience with Zero so far is with much smaller workloads. We currently recommend Zero for use with datasets smaller than 100GB, but are working to improve this in the beta timeframe. Zero Might Not be a Good Fit Yet While Zero is in alpha, there are additional reasons not to choose it: You don't want to run server-side infra Zero is a Docker container that you currently have to self-host. We're working on a SaaS solution but it's not ready yet. You can't tolerate occasional downtime The easiest way to run Zero today is single-node, which requires downtime for updates. Also there are occasional regressions. You need support for SSR Zero doesn't support SSR yet, but it is planned. Alternatives If Zero isn't right for you, here are some good alternatives to consider: Automerge: Local-first, CRDT-based solution. Pioneering branch-based offline support. Convex: Not a sync engine (reads and writes are server-first), but a very nice reactive database that is in GA. Ditto: CRDT-based, with high quality offline support. Electric: Postgres-based sync engine with a SaaS cloud. LiveStore: Interesting event sourced design from one of the founders of Prisma. Jazz: Batteries-included local-first. PowerSync: Sync engine that works with Postgres, MySQL, and MongoDB.",
    "headings": [
      {
        "text": "Zero Might be a Good Fit",
        "id": "zero-might-be-a-good-fit"
      },
      {
        "text": "You want to sync only a small subset of data to client",
        "id": "you-want-to-sync-only-a-small-subset-of-data-to-client"
      },
      {
        "text": "You need fine-grained read or write permissions",
        "id": "you-need-fine-grained-read-or-write-permissions"
      },
      {
        "text": "You are building a traditional client-server web app",
        "id": "you-are-building-a-traditional-client-server-web-app"
      },
      {
        "text": "You use PostgreSQL",
        "id": "you-use-postgresql"
      },
      {
        "text": "Your app is broadly \"like Linear\"",
        "id": "your-app-is-broadly-like-linear"
      },
      {
        "text": "Interaction performance is very important to you",
        "id": "interaction-performance-is-very-important-to-you"
      },
      {
        "text": "Zero Might Not be a Good Fit",
        "id": "zero-might-not-be-a-good-fit"
      },
      {
        "text": "You need the privacy or data ownership benefits of local-first",
        "id": "you-need-the-privacy-or-data-ownership-benefits-of-local-first"
      },
      {
        "text": "You need to support offline writes or long periods offline",
        "id": "you-need-to-support-offline-writes-or-long-periods-offline"
      },
      {
        "text": "You are building a native mobile app",
        "id": "you-are-building-a-native-mobile-app"
      },
      {
        "text": "The total backend dataset is > ~100GB",
        "id": "the-total-backend-dataset-is--100gb"
      },
      {
        "text": "Zero Might Not be a Good Fit Yet",
        "id": "zero-might-not-be-a-good-fit-yet"
      },
      {
        "text": "You don't want to run server-side infra",
        "id": "you-dont-want-to-run-server-side-infra"
      },
      {
        "text": "You can't tolerate occasional downtime",
        "id": "you-cant-tolerate-occasional-downtime"
      },
      {
        "text": "You need support for SSR",
        "id": "you-need-support-for-ssr"
      },
      {
        "text": "Alternatives",
        "id": "alternatives"
      }
    ],
    "kind": "page"
  },
  {
    "id": "423-when-to-use#zero-might-be-a-good-fit",
    "title": "When To Use Zero",
    "searchTitle": "Zero Might be a Good Fit",
    "sectionTitle": "Zero Might be a Good Fit",
    "sectionId": "zero-might-be-a-good-fit",
    "url": "/docs/when-to-use",
    "content": "You want to sync only a small subset of data to client Zero's query-driven sync is a powerful solution for partial sync. You can define the data you want to sync with a set of Zero queries. By using partial sync, Zero apps can commonly load in < 1s, yet still maintain the interaction perf of sync. You need fine-grained read or write permissions Zero's mutators allow you to run arbitrary authorization, validation, or business logic on the write path. You can enforce that a write depends on what group a user is in, what has been shared with them, their role, etc. Read permissions are very expressive, allowing similar control over what data is synced to the client. You are building a traditional client-server web app Zero was designed from the ground up to be as close to a classic web app as a sync engine can be. If you have a traditional web app, you can try Zero side-by-side with your existing REST or GraphQL API, and incrementally migrate over time. You use PostgreSQL Some tools in our space require you to use a non-standard backend database or data model. Zero works with PostgreSQL, and uses your existing schema. Your app is broadly \"like Linear\" Zero is currently best suited for productivity apps with lots of interactivity. Interaction performance is very important to you Zero was built by people obsessed with interaction performance. If you share this goal you'll be going with the grain of Zero's design choices.",
    "kind": "section"
  },
  {
    "id": "424-when-to-use#you-want-to-sync-only-a-small-subset-of-data-to-client",
    "title": "When To Use Zero",
    "searchTitle": "You want to sync only a small subset of data to client",
    "sectionTitle": "You want to sync only a small subset of data to client",
    "sectionId": "you-want-to-sync-only-a-small-subset-of-data-to-client",
    "url": "/docs/when-to-use",
    "content": "Zero's query-driven sync is a powerful solution for partial sync. You can define the data you want to sync with a set of Zero queries. By using partial sync, Zero apps can commonly load in < 1s, yet still maintain the interaction perf of sync.",
    "kind": "section"
  },
  {
    "id": "425-when-to-use#you-need-fine-grained-read-or-write-permissions",
    "title": "When To Use Zero",
    "searchTitle": "You need fine-grained read or write permissions",
    "sectionTitle": "You need fine-grained read or write permissions",
    "sectionId": "you-need-fine-grained-read-or-write-permissions",
    "url": "/docs/when-to-use",
    "content": "Zero's mutators allow you to run arbitrary authorization, validation, or business logic on the write path. You can enforce that a write depends on what group a user is in, what has been shared with them, their role, etc. Read permissions are very expressive, allowing similar control over what data is synced to the client.",
    "kind": "section"
  },
  {
    "id": "426-when-to-use#you-are-building-a-traditional-client-server-web-app",
    "title": "When To Use Zero",
    "searchTitle": "You are building a traditional client-server web app",
    "sectionTitle": "You are building a traditional client-server web app",
    "sectionId": "you-are-building-a-traditional-client-server-web-app",
    "url": "/docs/when-to-use",
    "content": "Zero was designed from the ground up to be as close to a classic web app as a sync engine can be. If you have a traditional web app, you can try Zero side-by-side with your existing REST or GraphQL API, and incrementally migrate over time.",
    "kind": "section"
  },
  {
    "id": "427-when-to-use#you-use-postgresql",
    "title": "When To Use Zero",
    "searchTitle": "You use PostgreSQL",
    "sectionTitle": "You use PostgreSQL",
    "sectionId": "you-use-postgresql",
    "url": "/docs/when-to-use",
    "content": "Some tools in our space require you to use a non-standard backend database or data model. Zero works with PostgreSQL, and uses your existing schema.",
    "kind": "section"
  },
  {
    "id": "428-when-to-use#your-app-is-broadly-like-linear",
    "title": "When To Use Zero",
    "searchTitle": "Your app is broadly \"like Linear\"",
    "sectionTitle": "Your app is broadly \"like Linear\"",
    "sectionId": "your-app-is-broadly-like-linear",
    "url": "/docs/when-to-use",
    "content": "Zero is currently best suited for productivity apps with lots of interactivity.",
    "kind": "section"
  },
  {
    "id": "429-when-to-use#interaction-performance-is-very-important-to-you",
    "title": "When To Use Zero",
    "searchTitle": "Interaction performance is very important to you",
    "sectionTitle": "Interaction performance is very important to you",
    "sectionId": "interaction-performance-is-very-important-to-you",
    "url": "/docs/when-to-use",
    "content": "Zero was built by people obsessed with interaction performance. If you share this goal you'll be going with the grain of Zero's design choices.",
    "kind": "section"
  },
  {
    "id": "430-when-to-use#zero-might-not-be-a-good-fit",
    "title": "When To Use Zero",
    "searchTitle": "Zero Might Not be a Good Fit",
    "sectionTitle": "Zero Might Not be a Good Fit",
    "sectionId": "zero-might-not-be-a-good-fit",
    "url": "/docs/when-to-use",
    "content": "You need the privacy or data ownership benefits of local-first Zero is not local-first. It's a client-server system with an authoritative server. You need to support offline writes or long periods offline Zero doesn't support offline writes yet. You are building a native mobile app Zero is written in TypeScript and only supports TypeScript clients. The total backend dataset is > ~100GB Zero stores a replica of your database (at least the subset you want to be syncable to clients) in a SQLite database owned by zero-cache. Zero's query engine is built assuming very fast local access to this replica (i.e., attached NVMe). But other setups are technically supported and work for smaller data. The ultimate size limit on the database that Zero can work with is the size limit of this SQLite database. So up to 45TB on EC2 at time of writing. However, most of our experience with Zero so far is with much smaller workloads. We currently recommend Zero for use with datasets smaller than 100GB, but are working to improve this in the beta timeframe.",
    "kind": "section"
  },
  {
    "id": "431-when-to-use#you-need-the-privacy-or-data-ownership-benefits-of-local-first",
    "title": "When To Use Zero",
    "searchTitle": "You need the privacy or data ownership benefits of local-first",
    "sectionTitle": "You need the privacy or data ownership benefits of local-first",
    "sectionId": "you-need-the-privacy-or-data-ownership-benefits-of-local-first",
    "url": "/docs/when-to-use",
    "content": "Zero is not local-first. It's a client-server system with an authoritative server.",
    "kind": "section"
  },
  {
    "id": "432-when-to-use#you-need-to-support-offline-writes-or-long-periods-offline",
    "title": "When To Use Zero",
    "searchTitle": "You need to support offline writes or long periods offline",
    "sectionTitle": "You need to support offline writes or long periods offline",
    "sectionId": "you-need-to-support-offline-writes-or-long-periods-offline",
    "url": "/docs/when-to-use",
    "content": "Zero doesn't support offline writes yet.",
    "kind": "section"
  },
  {
    "id": "433-when-to-use#you-are-building-a-native-mobile-app",
    "title": "When To Use Zero",
    "searchTitle": "You are building a native mobile app",
    "sectionTitle": "You are building a native mobile app",
    "sectionId": "you-are-building-a-native-mobile-app",
    "url": "/docs/when-to-use",
    "content": "Zero is written in TypeScript and only supports TypeScript clients.",
    "kind": "section"
  },
  {
    "id": "434-when-to-use#the-total-backend-dataset-is--100gb",
    "title": "When To Use Zero",
    "searchTitle": "The total backend dataset is > ~100GB",
    "sectionTitle": "The total backend dataset is > ~100GB",
    "sectionId": "the-total-backend-dataset-is--100gb",
    "url": "/docs/when-to-use",
    "content": "Zero stores a replica of your database (at least the subset you want to be syncable to clients) in a SQLite database owned by zero-cache. Zero's query engine is built assuming very fast local access to this replica (i.e., attached NVMe). But other setups are technically supported and work for smaller data. The ultimate size limit on the database that Zero can work with is the size limit of this SQLite database. So up to 45TB on EC2 at time of writing. However, most of our experience with Zero so far is with much smaller workloads. We currently recommend Zero for use with datasets smaller than 100GB, but are working to improve this in the beta timeframe.",
    "kind": "section"
  },
  {
    "id": "435-when-to-use#zero-might-not-be-a-good-fit-yet",
    "title": "When To Use Zero",
    "searchTitle": "Zero Might Not be a Good Fit Yet",
    "sectionTitle": "Zero Might Not be a Good Fit Yet",
    "sectionId": "zero-might-not-be-a-good-fit-yet",
    "url": "/docs/when-to-use",
    "content": "While Zero is in alpha, there are additional reasons not to choose it: You don't want to run server-side infra Zero is a Docker container that you currently have to self-host. We're working on a SaaS solution but it's not ready yet. You can't tolerate occasional downtime The easiest way to run Zero today is single-node, which requires downtime for updates. Also there are occasional regressions. You need support for SSR Zero doesn't support SSR yet, but it is planned.",
    "kind": "section"
  },
  {
    "id": "436-when-to-use#you-dont-want-to-run-server-side-infra",
    "title": "When To Use Zero",
    "searchTitle": "You don't want to run server-side infra",
    "sectionTitle": "You don't want to run server-side infra",
    "sectionId": "you-dont-want-to-run-server-side-infra",
    "url": "/docs/when-to-use",
    "content": "Zero is a Docker container that you currently have to self-host. We're working on a SaaS solution but it's not ready yet.",
    "kind": "section"
  },
  {
    "id": "437-when-to-use#you-cant-tolerate-occasional-downtime",
    "title": "When To Use Zero",
    "searchTitle": "You can't tolerate occasional downtime",
    "sectionTitle": "You can't tolerate occasional downtime",
    "sectionId": "you-cant-tolerate-occasional-downtime",
    "url": "/docs/when-to-use",
    "content": "The easiest way to run Zero today is single-node, which requires downtime for updates. Also there are occasional regressions.",
    "kind": "section"
  },
  {
    "id": "438-when-to-use#you-need-support-for-ssr",
    "title": "When To Use Zero",
    "searchTitle": "You need support for SSR",
    "sectionTitle": "You need support for SSR",
    "sectionId": "you-need-support-for-ssr",
    "url": "/docs/when-to-use",
    "content": "Zero doesn't support SSR yet, but it is planned.",
    "kind": "section"
  },
  {
    "id": "439-when-to-use#alternatives",
    "title": "When To Use Zero",
    "searchTitle": "Alternatives",
    "sectionTitle": "Alternatives",
    "sectionId": "alternatives",
    "url": "/docs/when-to-use",
    "content": "If Zero isn't right for you, here are some good alternatives to consider: Automerge: Local-first, CRDT-based solution. Pioneering branch-based offline support. Convex: Not a sync engine (reads and writes are server-first), but a very nice reactive database that is in GA. Ditto: CRDT-based, with high quality offline support. Electric: Postgres-based sync engine with a SaaS cloud. LiveStore: Interesting event sourced design from one of the founders of Prisma. Jazz: Batteries-included local-first. PowerSync: Sync engine that works with Postgres, MySQL, and MongoDB.",
    "kind": "section"
  },
  {
    "id": "60-zero-cache-config",
    "title": "zero-cache Config",
    "searchTitle": "zero-cache Config",
    "url": "/docs/zero-cache-config",
    "content": "zero-cache is configured either via CLI flag or environment variable. There is no separate zero.config file. You can also see all available flags by running zero-cache --help. Required Flags Upstream DB The \"upstream\" authoritative postgres database. In the future we will support other types of upstream besides PG. flag: --upstream-db env: ZERO_UPSTREAM_DB required: true Admin Password A password used to administer zero-cache server, for example to access the /statz endpoint and the inspector. This is required in production (when NODE_ENV=production) because we want all Zero servers to be debuggable using admin tools by default, without needing a restart. But we also don't want to expose sensitive data using them. flag: --admin-password env: ZERO_ADMIN_PASSWORD required: in production (when NODE_ENV=production) Optional Flags App ID Unique identifier for the app. Multiple zero-cache apps can run on a single upstream database, each of which is isolated from the others, with its own permissions, sharding (future feature), and change/cvr databases. The metadata of an app is stored in an upstream schema with the same name, e.g. zero, and the metadata for each app shard, e.g. client and mutation ids, is stored in the {app-id}_{#} schema. (Currently there is only a single \"0\" shard, but this will change with sharding). The CVR and Change data are managed in schemas named {app-id}_{shard-num}/cvr and {app-id}_{shard-num}/cdc, respectively, allowing multiple apps and shards to share the same database instance (e.g. a Postgres \"cluster\") for CVR and Change management. Due to constraints on replication slot names, an App ID may only consist of lower-case letters, numbers, and the underscore character. Note that this option is used by both zero-cache and zero-deploy-permissions. flag: --app-id env: ZERO_APP_ID default: zero App Publications Postgres PUBLICATIONs that define the tables and columns to replicate. Publication names may not begin with an underscore, as zero reserves that prefix for internal use. If unspecified, zero-cache will create and use an internal publication that publishes all tables in the public schema, i.e.: CREATE PUBLICATION _{app-id}_public_0 FOR TABLES IN SCHEMA public; Note that changing the set of publications will result in resyncing the replica, which may involve downtime (replication lag) while the new replica is initializing. To change the set of publications without disrupting an existing app, a new app should be created. flag: --app-publications env: ZERO_APP_PUBLICATIONS default: [] Auto Reset Automatically wipe and resync the replica when replication is halted. This situation can occur for configurations in which the upstream database provider prohibits event trigger creation, preventing the zero-cache from being able to correctly replicate schema changes. For such configurations, an upstream schema change will instead result in halting replication with an error indicating that the replica needs to be reset. When auto-reset is enabled, zero-cache will respond to such situations by shutting down, and when restarted, resetting the replica and all synced clients. This is a heavy-weight operation and can result in user-visible slowness or downtime if compute resources are scarce. flag: --auto-reset env: ZERO_AUTO_RESET default: true Change DB The Postgres database used to store recent replication log entries, in order to sync multiple view-syncers without requiring multiple replication slots on the upstream database. If unspecified, the upstream-db will be used. flag: --change-db env: ZERO_CHANGE_DB Change Max Connections The maximum number of connections to open to the change database. This is used by the change-streamer for catching up zero-cache replication subscriptions. flag: --change-max-conns env: ZERO_CHANGE_MAX_CONNS default: 5 Change Streamer Mode The mode for running or connecting to the change-streamer: dedicated: runs the change-streamer and shuts down when another change-streamer takes over the replication slot. This is appropriate in a single-node configuration, or for the replication-manager in a multi-node configuration. discover: connects to the change-streamer as internally advertised in the change-db. This is appropriate for the view-syncers in a multi-node setup. This may not work in all networking configurations (e.g., some private networking or port forwarding setups). Using ZERO_CHANGE_STREAMER_URI with an explicit routable hostname is recommended instead. This option is ignored if ZERO_CHANGE_STREAMER_URI is set. flag: --change-streamer-mode env: ZERO_CHANGE_STREAMER_MODE default: dedicated Change Streamer Port The port on which the change-streamer runs. This is an internal protocol between the replication-manager and view-syncers, which runs in the same process tree in local development or a single-node configuration. If unspecified, defaults to --port + 1. flag: --change-streamer-port env: ZERO_CHANGE_STREAMER_PORT default: --port + 1 Change Streamer Startup Delay (ms) The delay to wait before the change-streamer takes over the replication stream (i.e. the handoff during replication-manager updates), to allow load balancers to register the task as healthy based on healthcheck parameters. If a change stream request is received during this interval, the delay will be canceled and the takeover will happen immediately, since the incoming request indicates that the task is registered as a target. flag: --change-streamer-startup-delay-ms env: ZERO_CHANGE_STREAMER_STARTUP_DELAY_MS default: 15000 Change Streamer URI When set, connects to the change-streamer at the given URI. In a multi-node setup, this should be specified in view-syncer options, pointing to the replication-manager URI, which runs a change-streamer on port 4849. flag: --change-streamer-uri env: ZERO_CHANGE_STREAMER_URI CVR DB The Postgres database used to store CVRs. CVRs (client view records) keep track of the data synced to clients in order to determine the diff to send on reconnect. If unspecified, the upstream-db will be used. flag: --cvr-db env: ZERO_CVR_DB CVR Garbage Collection Inactivity Threshold Hours The duration after which an inactive CVR is eligible for garbage collection. Garbage collection is incremental and periodic, so eligible CVRs are not necessarily purged immediately. flag: --cvr-garbage-collection-inactivity-threshold-hours env: ZERO_CVR_GARBAGE_COLLECTION_INACTIVITY_THRESHOLD_HOURS default: 48 CVR Garbage Collection Initial Batch Size The initial number of CVRs to purge per garbage collection interval. This number is increased linearly if the rate of new CVRs exceeds the rate of purged CVRs, in order to reach a steady state. Setting this to 0 effectively disables CVR garbage collection. flag: --cvr-garbage-collection-initial-batch-size env: ZERO_CVR_GARBAGE_COLLECTION_INITIAL_BATCH_SIZE default: 25 CVR Garbage Collection Initial Interval Seconds The initial interval at which to check and garbage collect inactive CVRs. This interval is increased exponentially (up to 16 minutes) when there is nothing to purge. flag: --cvr-garbage-collection-initial-interval-seconds env: ZERO_CVR_GARBAGE_COLLECTION_INITIAL_INTERVAL_SECONDS default: 60 CVR Max Connections The maximum number of connections to open to the CVR database. This is divided evenly amongst sync workers. Note that this number must allow for at least one connection per sync worker, or zero-cache will fail to start. See num-sync-workers. flag: --cvr-max-conns env: ZERO_CVR_MAX_CONNS default: 30 Enable Query Planner Enable the query planner for optimizing ZQL queries. The query planner analyzes and optimizes query execution by determining the most efficient join strategies. You can disable the planner if it is picking bad strategies. flag: --enable-query-planner env: ZERO_ENABLE_QUERY_PLANNER default: true Enable Telemetry Zero collects anonymous telemetry data to help us understand usage. We collect: Zero version Uptime General machine information, like the number of CPUs, OS, CI/CD environment, etc. Information about usage, such as number of queries or mutations processed per hour. This is completely optional and can be disabled at any time. You can also opt-out by setting DO_NOT_TRACK=1. flag: --enable-telemetry env: ZERO_ENABLE_TELEMETRY default: true Initial Sync Table Copy Workers The number of parallel workers used to copy tables during initial sync. Each worker uses a database connection, copies a single table at a time, and buffers up to (approximately) 10 MB of table data in memory during initial sync. Increasing the number of workers may improve initial sync speed; however, local disk throughput (IOPS), upstream CPU, and network bandwidth may also be bottlenecks. flag: --initial-sync-table-copy-workers env: ZERO_INITIAL_SYNC_TABLE_COPY_WORKERS default: 5 Lazy Startup Delay starting the majority of zero-cache until first request. This is mainly intended to avoid connecting to Postgres replication stream until the first request is received, which can be useful i.e., for preview instances. Currently only supported in single-node mode. flag: --lazy-startup env: ZERO_LAZY_STARTUP default: false Litestream Backup URL The location of the litestream backup, usually an s3:// URL. This is only consulted by the replication-manager. view-syncers receive this information from the replication-manager. In multi-node deployments, this is required on the replication-manager so view-syncers can reserve snapshots; in single-node deployments it is optional. flag: --litestream-backup-url env: ZERO_LITESTREAM_BACKUP_URL Litestream Checkpoint Threshold MB The size of the WAL file at which to perform an SQlite checkpoint to apply the writes in the WAL to the main database file. Each checkpoint creates a new WAL segment file that will be backed up by litestream. Smaller thresholds may improve read performance, at the expense of creating more files to download when restoring the replica from the backup. flag: --litestream-checkpoint-threshold-mb env: ZERO_LITESTREAM_CHECKPOINT_THRESHOLD_MB default: 40 Litestream Config Path Path to the litestream yaml config file. zero-cache will run this with its environment variables, which can be referenced in the file via ${ENV} substitution, for example: ZERO_REPLICA_FILE for the db Path ZERO_LITESTREAM_BACKUP_LOCATION for the db replica url ZERO_LITESTREAM_LOG_LEVEL for the log Level ZERO_LOG_FORMAT for the log type flag: --litestream-config-path env: ZERO_LITESTREAM_CONFIG_PATH default: ./src/services/litestream/config.yml Litestream Executable Path to the litestream executable. This option has no effect if litestream-backup-url is unspecified. flag: --litestream-executable env: ZERO_LITESTREAM_EXECUTABLE Litestream Incremental Backup Interval Minutes The interval between incremental backups of the replica. Shorter intervals reduce the amount of change history that needs to be replayed when catching up a new view-syncer, at the expense of increasing the number of files needed to download for the initial litestream restore. flag: --litestream-incremental-backup-interval-minutes env: ZERO_LITESTREAM_INCREMENTAL_BACKUP_INTERVAL_MINUTES default: 15 Litestream Maximum Checkpoint Page Count The WAL page count at which SQLite performs a RESTART checkpoint, which blocks writers until complete. Defaults to minCheckpointPageCount * 10. Set to 0 to disable RESTART checkpoints entirely. flag: --litestream-max-checkpoint-page-count env: ZERO_LITESTREAM_MAX_CHECKPOINT_PAGE_COUNT default: minCheckpointPageCount * 10 Litestream Minimum Checkpoint Page Count The WAL page count at which SQLite attempts a PASSIVE checkpoint, which transfers pages to the main database file without blocking writers. Defaults to checkpointThresholdMB * 250 (since SQLite page size is 4KB). flag: --litestream-min-checkpoint-page-count env: ZERO_LITESTREAM_MIN_CHECKPOINT_PAGE_COUNT default: checkpointThresholdMB * 250 Litestream Multipart Concurrency The number of parts (of size --litestream-multipart-size bytes) to upload or download in parallel when backing up or restoring the snapshot. flag: --litestream-multipart-concurrency env: ZERO_LITESTREAM_MULTIPART_CONCURRENCY default: 48 Litestream Multipart Size The size of each part when uploading or downloading the snapshot with --litestream-multipart-concurrency. Note that up to concurrency * size bytes of memory are used when backing up or restoring the snapshot. flag: --litestream-multipart-size env: ZERO_LITESTREAM_MULTIPART_SIZE default: 16777216 (16 MiB) Litestream Log Level flag: --litestream-log-level env: ZERO_LITESTREAM_LOG_LEVEL default: warn values: debug, info, warn, error Litestream Port Port on which litestream exports metrics, used to determine the replication watermark up to which it is safe to purge change log records. flag: --litestream-port env: ZERO_LITESTREAM_PORT default: --port + 2 Litestream Restore Parallelism The number of WAL files to download in parallel when performing the initial restore of the replica from the backup. flag: --litestream-restore-parallelism env: ZERO_LITESTREAM_RESTORE_PARALLELISM default: 48 Litestream Snapshot Backup Interval Hours The interval between snapshot backups of the replica. Snapshot backups make a full copy of the database to a new litestream generation. This improves restore time at the expense of bandwidth. Applications with a large database and low write rate can increase this interval to reduce network usage for backups (litestream defaults to 24 hours). flag: --litestream-snapshot-backup-interval-hours env: ZERO_LITESTREAM_SNAPSHOT_BACKUP_INTERVAL_HOURS default: 12 Log Format Use text for developer-friendly console logging and json for consumption by structured-logging services. flag: --log-format env: ZERO_LOG_FORMAT default: \"text\" values: text, json Log IVM Sampling How often to collect IVM metrics. 1 out of N requests will be sampled where N is this value. flag: --log-ivm-sampling env: ZERO_LOG_IVM_SAMPLING default: 5000 Log Level Sets the logging level for the application. flag: --log-level env: ZERO_LOG_LEVEL default: \"info\" values: debug, info, warn, error Log Slow Hydrate Threshold The number of milliseconds a query hydration must take to print a slow warning. flag: --log-slow-hydrate-threshold env: ZERO_LOG_SLOW_HYDRATE_THRESHOLD default: 100 Log Slow Row Threshold The number of ms a row must take to fetch from table-source before it is considered slow. flag: --log-slow-row-threshold env: ZERO_LOG_SLOW_ROW_THRESHOLD default: 2 Mutate API Key An optional secret used to authorize zero-cache to call the API server handling writes. This is sent from zero-cache to your mutate endpoint in an X-Api-Key header. flag: --mutate-api-key env: ZERO_MUTATE_API_KEY Mutate Forward Cookies If true, zero-cache will forward cookies from the request to zero-cache to your mutate endpoint. This is useful for passing authentication cookies to the API server. If false, cookies are not forwarded. flag: --mutate-forward-cookies env: ZERO_MUTATE_FORWARD_COOKIES default: false Mutate URL The URL of the API server to which zero-cache will push mutations. URLs are matched using URLPattern, a standard Web API. Pattern syntax (similar to Express routes): Exact URL match: \"https://api.example.com/mutate\" Any subdomain using wildcard: \"https://*.example.com/mutate\" Multiple subdomain levels: \"https://*.*.example.com/mutate\" Any path under a domain: \"https://api.example.com/*\" Named path parameters: \"https://api.example.com/:version/mutate\" Matches https://api.example.com/v1/mutate, https://api.example.com/v2/mutate, etc. Advanced patterns: Optional path segments: \"https://api.example.com/:path?\" Regex in segments (for specific patterns): \"https://api.example.com/:version(v\\\\d+)/mutate\" matches only v followed by digits. Multiple patterns can be specified, for example: [\"https://api1.example.com/mutate\", \"https://api2.example.com/mutate\"] Query parameters and URL fragments (#) are ignored during matching. See URLPattern for full syntax. flag: --mutate-url env: ZERO_MUTATE_URL Number of Sync Workers The number of processes to use for view syncing. Leave this unset to use the maximum available parallelism. If set to 0, the server runs without sync workers, which is the configuration for running the replication-manager in multi-node deployments. flag: --num-sync-workers env: ZERO_NUM_SYNC_WORKERS Per User Mutation Limit Max The maximum mutations per user within the specified windowMs. flag: --per-user-mutation-limit-max env: ZERO_PER_USER_MUTATION_LIMIT_MAX Per User Mutation Limit Window (ms) The sliding window over which the perUserMutationLimitMax is enforced. flag: --per-user-mutation-limit-window-ms env: ZERO_PER_USER_MUTATION_LIMIT_WINDOW_MS default: 60000 Port The port for sync connections. flag: --port env: ZERO_PORT default: 4848 Query API Key An optional secret used to authorize zero-cache to call the API server handling queries. This is sent from zero-cache to your query endpoint in an X-Api-Key header. flag: --query-api-key env: ZERO_QUERY_API_KEY Query Forward Cookies If true, zero-cache will forward cookies from the request to zero-cache to your query endpoint. This is useful for passing authentication cookies to the API server. If false, cookies are not forwarded. flag: --query-forward-cookies env: ZERO_QUERY_FORWARD_COOKIES default: false Query Hydration Stats Track and log the number of rows considered by query hydrations which take longer than log-slow-hydrate-threshold milliseconds. This is useful for debugging and performance tuning. flag: --query-hydration-stats env: ZERO_QUERY_HYDRATION_STATS Query URL The URL of the API server to which zero-cache will send synced queries. URLs are matched using URLPattern, a standard Web API. Pattern syntax (similar to Express routes): Exact URL match: \"https://api.example.com/query\" Any subdomain using wildcard: \"https://*.example.com/query\" Multiple subdomain levels: \"https://*.*.example.com/query\" Any path under a domain: \"https://api.example.com/*\" Named path parameters: \"https://api.example.com/:version/query\" Matches https://api.example.com/v1/query, https://api.example.com/v2/query, etc. Advanced patterns: Optional path segments: \"https://api.example.com/:path?\" Regex in segments (for specific patterns): \"https://api.example.com/:version(v\\\\d+)/query\" matches only v followed by digits. Multiple patterns can be specified, for example: [\"https://api1.example.com/query\", \"https://api2.example.com/query\"] Query parameters and URL fragments (#) are ignored during matching. See URLPattern for full syntax. flag: --query-url env: ZERO_QUERY_URL Replica File File path to the SQLite replica that zero-cache maintains. This can be lost, but if it is, zero-cache will have to re-replicate next time it starts up. flag: --replica-file env: ZERO_REPLICA_FILE default: \"zero.db\" Replica Vacuum Interval Hours Performs a VACUUM at server startup if the specified number of hours has elapsed since the last VACUUM (or initial-sync). The VACUUM operation is heavyweight and requires double the size of the db in disk space. If unspecified, VACUUM operations are not performed. flag: --replica-vacuum-interval-hours env: ZERO_REPLICA_VACUUM_INTERVAL_HOURS Replica Page Cache Size KiB The SQLite page cache size in kibibytes (KiB) for view-syncer connections. The page cache stores recently accessed database pages in memory to reduce disk I/O. Larger cache sizes improve performance for workloads that fit in cache. If unspecified, SQLite's default (~2 MB) is used. Note that the effective memory use of this setting will be: 2 * cache_size * num_cores, as each connection to the replica gets its own cache and each core maintains 2 connections. flag: --replica-page-cache-size-kib env: ZERO_REPLICA_PAGE_CACHE_SIZE_KIB Server Version The version string outputted to logs when the server starts up. flag: --server-version env: ZERO_SERVER_VERSION Storage DB Temp Dir Temporary directory for IVM operator storage. Leave unset to use os.tmpdir(). flag: --storage-db-tmp-dir env: ZERO_STORAGE_DB_TMP_DIR Task ID Globally unique identifier for the zero-cache instance. Setting this to a platform specific task identifier can be useful for debugging. If unspecified, zero-cache will attempt to extract the TaskARN if run from within an AWS ECS container, and otherwise use a random string. flag: --task-id env: ZERO_TASK_ID Upstream Max Connections The maximum number of connections to open to the upstream database for committing mutations. This is divided evenly amongst sync workers. In addition to this number, zero-cache uses one connection for the replication stream. Note that this number must allow for at least one connection per sync worker, or zero-cache will fail to start. See num-sync-workers. flag: --upstream-max-conns env: ZERO_UPSTREAM_MAX_CONNS default: 20 Websocket Compression Enable WebSocket per-message deflate compression. Compression can reduce bandwidth usage for sync traffic but increases CPU usage on both client and server. Disabled by default. See: https://github.com/websockets/ws#websocket-compression flag: --websocket-compression env: ZERO_WEBSOCKET_COMPRESSION default: false Websocket Compression Options JSON string containing WebSocket compression options. Only used if websocket-compression is enabled. Example: {\"zlibDeflateOptions\":{\"level\":3},\"threshold\":1024}. See https://github.com/websockets/ws/blob/master/doc/ws.md#new-websocketserveroptions-callback for available options. flag: --websocket-compression-options env: ZERO_WEBSOCKET_COMPRESSION_OPTIONS Yield Threshold (ms) The maximum amount of time in milliseconds that a sync worker will spend in IVM (processing query hydration and advancement) before yielding to the event loop. Lower values increase responsiveness and fairness at the cost of reduced throughput. flag: --yield-threshold-ms env: ZERO_YIELD_THRESHOLD_MS default: 10 Deprecated Flags Auth JWK A public key in JWK format used to verify JWTs. Only one of jwk, jwksUrl and secret may be set. flag: --auth-jwk env: ZERO_AUTH_JWK Auth JWKS URL A URL that returns a JWK set used to verify JWTs. Only one of jwk, jwksUrl and secret may be set. flag: --auth-jwks-url env: ZERO_AUTH_JWKS_URL Auth Secret A symmetric key used to verify JWTs. Only one of jwk, jwksUrl and secret may be set. flag: --auth-secret env: ZERO_AUTH_SECRET",
    "headings": [
      {
        "text": "Required Flags",
        "id": "required-flags"
      },
      {
        "text": "Upstream DB",
        "id": "upstream-db"
      },
      {
        "text": "Admin Password",
        "id": "admin-password"
      },
      {
        "text": "Optional Flags",
        "id": "optional-flags"
      },
      {
        "text": "App ID",
        "id": "app-id"
      },
      {
        "text": "App Publications",
        "id": "app-publications"
      },
      {
        "text": "Auto Reset",
        "id": "auto-reset"
      },
      {
        "text": "Change DB",
        "id": "change-db"
      },
      {
        "text": "Change Max Connections",
        "id": "change-max-connections"
      },
      {
        "text": "Change Streamer Mode",
        "id": "change-streamer-mode"
      },
      {
        "text": "Change Streamer Port",
        "id": "change-streamer-port"
      },
      {
        "text": "Change Streamer Startup Delay (ms)",
        "id": "change-streamer-startup-delay-ms"
      },
      {
        "text": "Change Streamer URI",
        "id": "change-streamer-uri"
      },
      {
        "text": "CVR DB",
        "id": "cvr-db"
      },
      {
        "text": "CVR Garbage Collection Inactivity Threshold Hours",
        "id": "cvr-garbage-collection-inactivity-threshold-hours"
      },
      {
        "text": "CVR Garbage Collection Initial Batch Size",
        "id": "cvr-garbage-collection-initial-batch-size"
      },
      {
        "text": "CVR Garbage Collection Initial Interval Seconds",
        "id": "cvr-garbage-collection-initial-interval-seconds"
      },
      {
        "text": "CVR Max Connections",
        "id": "cvr-max-connections"
      },
      {
        "text": "Enable Query Planner",
        "id": "enable-query-planner"
      },
      {
        "text": "Enable Telemetry",
        "id": "enable-telemetry"
      },
      {
        "text": "Initial Sync Table Copy Workers",
        "id": "initial-sync-table-copy-workers"
      },
      {
        "text": "Lazy Startup",
        "id": "lazy-startup"
      },
      {
        "text": "Litestream Backup URL",
        "id": "litestream-backup-url"
      },
      {
        "text": "Litestream Checkpoint Threshold MB",
        "id": "litestream-checkpoint-threshold-mb"
      },
      {
        "text": "Litestream Config Path",
        "id": "litestream-config-path"
      },
      {
        "text": "Litestream Executable",
        "id": "litestream-executable"
      },
      {
        "text": "Litestream Incremental Backup Interval Minutes",
        "id": "litestream-incremental-backup-interval-minutes"
      },
      {
        "text": "Litestream Maximum Checkpoint Page Count",
        "id": "litestream-maximum-checkpoint-page-count"
      },
      {
        "text": "Litestream Minimum Checkpoint Page Count",
        "id": "litestream-minimum-checkpoint-page-count"
      },
      {
        "text": "Litestream Multipart Concurrency",
        "id": "litestream-multipart-concurrency"
      },
      {
        "text": "Litestream Multipart Size",
        "id": "litestream-multipart-size"
      },
      {
        "text": "Litestream Log Level",
        "id": "litestream-log-level"
      },
      {
        "text": "Litestream Port",
        "id": "litestream-port"
      },
      {
        "text": "Litestream Restore Parallelism",
        "id": "litestream-restore-parallelism"
      },
      {
        "text": "Litestream Snapshot Backup Interval Hours",
        "id": "litestream-snapshot-backup-interval-hours"
      },
      {
        "text": "Log Format",
        "id": "log-format"
      },
      {
        "text": "Log IVM Sampling",
        "id": "log-ivm-sampling"
      },
      {
        "text": "Log Level",
        "id": "log-level"
      },
      {
        "text": "Log Slow Hydrate Threshold",
        "id": "log-slow-hydrate-threshold"
      },
      {
        "text": "Log Slow Row Threshold",
        "id": "log-slow-row-threshold"
      },
      {
        "text": "Mutate API Key",
        "id": "mutate-api-key"
      },
      {
        "text": "Mutate Forward Cookies",
        "id": "mutate-forward-cookies"
      },
      {
        "text": "Mutate URL",
        "id": "mutate-url"
      },
      {
        "text": "Number of Sync Workers",
        "id": "number-of-sync-workers"
      },
      {
        "text": "Per User Mutation Limit Max",
        "id": "per-user-mutation-limit-max"
      },
      {
        "text": "Per User Mutation Limit Window (ms)",
        "id": "per-user-mutation-limit-window-ms"
      },
      {
        "text": "Port",
        "id": "port"
      },
      {
        "text": "Query API Key",
        "id": "query-api-key"
      },
      {
        "text": "Query Forward Cookies",
        "id": "query-forward-cookies"
      },
      {
        "text": "Query Hydration Stats",
        "id": "query-hydration-stats"
      },
      {
        "text": "Query URL",
        "id": "query-url"
      },
      {
        "text": "Replica File",
        "id": "replica-file"
      },
      {
        "text": "Replica Vacuum Interval Hours",
        "id": "replica-vacuum-interval-hours"
      },
      {
        "text": "Replica Page Cache Size KiB",
        "id": "replica-page-cache-size-kib"
      },
      {
        "text": "Server Version",
        "id": "server-version"
      },
      {
        "text": "Storage DB Temp Dir",
        "id": "storage-db-temp-dir"
      },
      {
        "text": "Task ID",
        "id": "task-id"
      },
      {
        "text": "Upstream Max Connections",
        "id": "upstream-max-connections"
      },
      {
        "text": "Websocket Compression",
        "id": "websocket-compression"
      },
      {
        "text": "Websocket Compression Options",
        "id": "websocket-compression-options"
      },
      {
        "text": "Yield Threshold (ms)",
        "id": "yield-threshold-ms"
      },
      {
        "text": "Deprecated Flags",
        "id": "deprecated-flags"
      },
      {
        "text": "Auth JWK",
        "id": "auth-jwk"
      },
      {
        "text": "Auth JWKS URL",
        "id": "auth-jwks-url"
      },
      {
        "text": "Auth Secret",
        "id": "auth-secret"
      }
    ],
    "kind": "page"
  },
  {
    "id": "440-zero-cache-config#required-flags",
    "title": "zero-cache Config",
    "searchTitle": "Required Flags",
    "sectionTitle": "Required Flags",
    "sectionId": "required-flags",
    "url": "/docs/zero-cache-config",
    "content": "Upstream DB The \"upstream\" authoritative postgres database. In the future we will support other types of upstream besides PG. flag: --upstream-db env: ZERO_UPSTREAM_DB required: true Admin Password A password used to administer zero-cache server, for example to access the /statz endpoint and the inspector. This is required in production (when NODE_ENV=production) because we want all Zero servers to be debuggable using admin tools by default, without needing a restart. But we also don't want to expose sensitive data using them. flag: --admin-password env: ZERO_ADMIN_PASSWORD required: in production (when NODE_ENV=production)",
    "kind": "section"
  },
  {
    "id": "441-zero-cache-config#upstream-db",
    "title": "zero-cache Config",
    "searchTitle": "Upstream DB",
    "sectionTitle": "Upstream DB",
    "sectionId": "upstream-db",
    "url": "/docs/zero-cache-config",
    "content": "The \"upstream\" authoritative postgres database. In the future we will support other types of upstream besides PG. flag: --upstream-db env: ZERO_UPSTREAM_DB required: true",
    "kind": "section"
  },
  {
    "id": "442-zero-cache-config#admin-password",
    "title": "zero-cache Config",
    "searchTitle": "Admin Password",
    "sectionTitle": "Admin Password",
    "sectionId": "admin-password",
    "url": "/docs/zero-cache-config",
    "content": "A password used to administer zero-cache server, for example to access the /statz endpoint and the inspector. This is required in production (when NODE_ENV=production) because we want all Zero servers to be debuggable using admin tools by default, without needing a restart. But we also don't want to expose sensitive data using them. flag: --admin-password env: ZERO_ADMIN_PASSWORD required: in production (when NODE_ENV=production)",
    "kind": "section"
  },
  {
    "id": "443-zero-cache-config#optional-flags",
    "title": "zero-cache Config",
    "searchTitle": "Optional Flags",
    "sectionTitle": "Optional Flags",
    "sectionId": "optional-flags",
    "url": "/docs/zero-cache-config",
    "content": "App ID Unique identifier for the app. Multiple zero-cache apps can run on a single upstream database, each of which is isolated from the others, with its own permissions, sharding (future feature), and change/cvr databases. The metadata of an app is stored in an upstream schema with the same name, e.g. zero, and the metadata for each app shard, e.g. client and mutation ids, is stored in the {app-id}_{#} schema. (Currently there is only a single \"0\" shard, but this will change with sharding). The CVR and Change data are managed in schemas named {app-id}_{shard-num}/cvr and {app-id}_{shard-num}/cdc, respectively, allowing multiple apps and shards to share the same database instance (e.g. a Postgres \"cluster\") for CVR and Change management. Due to constraints on replication slot names, an App ID may only consist of lower-case letters, numbers, and the underscore character. Note that this option is used by both zero-cache and zero-deploy-permissions. flag: --app-id env: ZERO_APP_ID default: zero App Publications Postgres PUBLICATIONs that define the tables and columns to replicate. Publication names may not begin with an underscore, as zero reserves that prefix for internal use. If unspecified, zero-cache will create and use an internal publication that publishes all tables in the public schema, i.e.: CREATE PUBLICATION _{app-id}_public_0 FOR TABLES IN SCHEMA public; Note that changing the set of publications will result in resyncing the replica, which may involve downtime (replication lag) while the new replica is initializing. To change the set of publications without disrupting an existing app, a new app should be created. flag: --app-publications env: ZERO_APP_PUBLICATIONS default: [] Auto Reset Automatically wipe and resync the replica when replication is halted. This situation can occur for configurations in which the upstream database provider prohibits event trigger creation, preventing the zero-cache from being able to correctly replicate schema changes. For such configurations, an upstream schema change will instead result in halting replication with an error indicating that the replica needs to be reset. When auto-reset is enabled, zero-cache will respond to such situations by shutting down, and when restarted, resetting the replica and all synced clients. This is a heavy-weight operation and can result in user-visible slowness or downtime if compute resources are scarce. flag: --auto-reset env: ZERO_AUTO_RESET default: true Change DB The Postgres database used to store recent replication log entries, in order to sync multiple view-syncers without requiring multiple replication slots on the upstream database. If unspecified, the upstream-db will be used. flag: --change-db env: ZERO_CHANGE_DB Change Max Connections The maximum number of connections to open to the change database. This is used by the change-streamer for catching up zero-cache replication subscriptions. flag: --change-max-conns env: ZERO_CHANGE_MAX_CONNS default: 5 Change Streamer Mode The mode for running or connecting to the change-streamer: dedicated: runs the change-streamer and shuts down when another change-streamer takes over the replication slot. This is appropriate in a single-node configuration, or for the replication-manager in a multi-node configuration. discover: connects to the change-streamer as internally advertised in the change-db. This is appropriate for the view-syncers in a multi-node setup. This may not work in all networking configurations (e.g., some private networking or port forwarding setups). Using ZERO_CHANGE_STREAMER_URI with an explicit routable hostname is recommended instead. This option is ignored if ZERO_CHANGE_STREAMER_URI is set. flag: --change-streamer-mode env: ZERO_CHANGE_STREAMER_MODE default: dedicated Change Streamer Port The port on which the change-streamer runs. This is an internal protocol between the replication-manager and view-syncers, which runs in the same process tree in local development or a single-node configuration. If unspecified, defaults to --port + 1. flag: --change-streamer-port env: ZERO_CHANGE_STREAMER_PORT default: --port + 1 Change Streamer Startup Delay (ms) The delay to wait before the change-streamer takes over the replication stream (i.e. the handoff during replication-manager updates), to allow load balancers to register the task as healthy based on healthcheck parameters. If a change stream request is received during this interval, the delay will be canceled and the takeover will happen immediately, since the incoming request indicates that the task is registered as a target. flag: --change-streamer-startup-delay-ms env: ZERO_CHANGE_STREAMER_STARTUP_DELAY_MS default: 15000 Change Streamer URI When set, connects to the change-streamer at the given URI. In a multi-node setup, this should be specified in view-syncer options, pointing to the replication-manager URI, which runs a change-streamer on port 4849. flag: --change-streamer-uri env: ZERO_CHANGE_STREAMER_URI CVR DB The Postgres database used to store CVRs. CVRs (client view records) keep track of the data synced to clients in order to determine the diff to send on reconnect. If unspecified, the upstream-db will be used. flag: --cvr-db env: ZERO_CVR_DB CVR Garbage Collection Inactivity Threshold Hours The duration after which an inactive CVR is eligible for garbage collection. Garbage collection is incremental and periodic, so eligible CVRs are not necessarily purged immediately. flag: --cvr-garbage-collection-inactivity-threshold-hours env: ZERO_CVR_GARBAGE_COLLECTION_INACTIVITY_THRESHOLD_HOURS default: 48 CVR Garbage Collection Initial Batch Size The initial number of CVRs to purge per garbage collection interval. This number is increased linearly if the rate of new CVRs exceeds the rate of purged CVRs, in order to reach a steady state. Setting this to 0 effectively disables CVR garbage collection. flag: --cvr-garbage-collection-initial-batch-size env: ZERO_CVR_GARBAGE_COLLECTION_INITIAL_BATCH_SIZE default: 25 CVR Garbage Collection Initial Interval Seconds The initial interval at which to check and garbage collect inactive CVRs. This interval is increased exponentially (up to 16 minutes) when there is nothing to purge. flag: --cvr-garbage-collection-initial-interval-seconds env: ZERO_CVR_GARBAGE_COLLECTION_INITIAL_INTERVAL_SECONDS default: 60 CVR Max Connections The maximum number of connections to open to the CVR database. This is divided evenly amongst sync workers. Note that this number must allow for at least one connection per sync worker, or zero-cache will fail to start. See num-sync-workers. flag: --cvr-max-conns env: ZERO_CVR_MAX_CONNS default: 30 Enable Query Planner Enable the query planner for optimizing ZQL queries. The query planner analyzes and optimizes query execution by determining the most efficient join strategies. You can disable the planner if it is picking bad strategies. flag: --enable-query-planner env: ZERO_ENABLE_QUERY_PLANNER default: true Enable Telemetry Zero collects anonymous telemetry data to help us understand usage. We collect: Zero version Uptime General machine information, like the number of CPUs, OS, CI/CD environment, etc. Information about usage, such as number of queries or mutations processed per hour. This is completely optional and can be disabled at any time. You can also opt-out by setting DO_NOT_TRACK=1. flag: --enable-telemetry env: ZERO_ENABLE_TELEMETRY default: true Initial Sync Table Copy Workers The number of parallel workers used to copy tables during initial sync. Each worker uses a database connection, copies a single table at a time, and buffers up to (approximately) 10 MB of table data in memory during initial sync. Increasing the number of workers may improve initial sync speed; however, local disk throughput (IOPS), upstream CPU, and network bandwidth may also be bottlenecks. flag: --initial-sync-table-copy-workers env: ZERO_INITIAL_SYNC_TABLE_COPY_WORKERS default: 5 Lazy Startup Delay starting the majority of zero-cache until first request. This is mainly intended to avoid connecting to Postgres replication stream until the first request is received, which can be useful i.e., for preview instances. Currently only supported in single-node mode. flag: --lazy-startup env: ZERO_LAZY_STARTUP default: false Litestream Backup URL The location of the litestream backup, usually an s3:// URL. This is only consulted by the replication-manager. view-syncers receive this information from the replication-manager. In multi-node deployments, this is required on the replication-manager so view-syncers can reserve snapshots; in single-node deployments it is optional. flag: --litestream-backup-url env: ZERO_LITESTREAM_BACKUP_URL Litestream Checkpoint Threshold MB The size of the WAL file at which to perform an SQlite checkpoint to apply the writes in the WAL to the main database file. Each checkpoint creates a new WAL segment file that will be backed up by litestream. Smaller thresholds may improve read performance, at the expense of creating more files to download when restoring the replica from the backup. flag: --litestream-checkpoint-threshold-mb env: ZERO_LITESTREAM_CHECKPOINT_THRESHOLD_MB default: 40 Litestream Config Path Path to the litestream yaml config file. zero-cache will run this with its environment variables, which can be referenced in the file via ${ENV} substitution, for example: ZERO_REPLICA_FILE for the db Path ZERO_LITESTREAM_BACKUP_LOCATION for the db replica url ZERO_LITESTREAM_LOG_LEVEL for the log Level ZERO_LOG_FORMAT for the log type flag: --litestream-config-path env: ZERO_LITESTREAM_CONFIG_PATH default: ./src/services/litestream/config.yml Litestream Executable Path to the litestream executable. This option has no effect if litestream-backup-url is unspecified. flag: --litestream-executable env: ZERO_LITESTREAM_EXECUTABLE Litestream Incremental Backup Interval Minutes The interval between incremental backups of the replica. Shorter intervals reduce the amount of change history that needs to be replayed when catching up a new view-syncer, at the expense of increasing the number of files needed to download for the initial litestream restore. flag: --litestream-incremental-backup-interval-minutes env: ZERO_LITESTREAM_INCREMENTAL_BACKUP_INTERVAL_MINUTES default: 15 Litestream Maximum Checkpoint Page Count The WAL page count at which SQLite performs a RESTART checkpoint, which blocks writers until complete. Defaults to minCheckpointPageCount * 10. Set to 0 to disable RESTART checkpoints entirely. flag: --litestream-max-checkpoint-page-count env: ZERO_LITESTREAM_MAX_CHECKPOINT_PAGE_COUNT default: minCheckpointPageCount * 10 Litestream Minimum Checkpoint Page Count The WAL page count at which SQLite attempts a PASSIVE checkpoint, which transfers pages to the main database file without blocking writers. Defaults to checkpointThresholdMB * 250 (since SQLite page size is 4KB). flag: --litestream-min-checkpoint-page-count env: ZERO_LITESTREAM_MIN_CHECKPOINT_PAGE_COUNT default: checkpointThresholdMB * 250 Litestream Multipart Concurrency The number of parts (of size --litestream-multipart-size bytes) to upload or download in parallel when backing up or restoring the snapshot. flag: --litestream-multipart-concurrency env: ZERO_LITESTREAM_MULTIPART_CONCURRENCY default: 48 Litestream Multipart Size The size of each part when uploading or downloading the snapshot with --litestream-multipart-concurrency. Note that up to concurrency * size bytes of memory are used when backing up or restoring the snapshot. flag: --litestream-multipart-size env: ZERO_LITESTREAM_MULTIPART_SIZE default: 16777216 (16 MiB) Litestream Log Level flag: --litestream-log-level env: ZERO_LITESTREAM_LOG_LEVEL default: warn values: debug, info, warn, error Litestream Port Port on which litestream exports metrics, used to determine the replication watermark up to which it is safe to purge change log records. flag: --litestream-port env: ZERO_LITESTREAM_PORT default: --port + 2 Litestream Restore Parallelism The number of WAL files to download in parallel when performing the initial restore of the replica from the backup. flag: --litestream-restore-parallelism env: ZERO_LITESTREAM_RESTORE_PARALLELISM default: 48 Litestream Snapshot Backup Interval Hours The interval between snapshot backups of the replica. Snapshot backups make a full copy of the database to a new litestream generation. This improves restore time at the expense of bandwidth. Applications with a large database and low write rate can increase this interval to reduce network usage for backups (litestream defaults to 24 hours). flag: --litestream-snapshot-backup-interval-hours env: ZERO_LITESTREAM_SNAPSHOT_BACKUP_INTERVAL_HOURS default: 12 Log Format Use text for developer-friendly console logging and json for consumption by structured-logging services. flag: --log-format env: ZERO_LOG_FORMAT default: \"text\" values: text, json Log IVM Sampling How often to collect IVM metrics. 1 out of N requests will be sampled where N is this value. flag: --log-ivm-sampling env: ZERO_LOG_IVM_SAMPLING default: 5000 Log Level Sets the logging level for the application. flag: --log-level env: ZERO_LOG_LEVEL default: \"info\" values: debug, info, warn, error Log Slow Hydrate Threshold The number of milliseconds a query hydration must take to print a slow warning. flag: --log-slow-hydrate-threshold env: ZERO_LOG_SLOW_HYDRATE_THRESHOLD default: 100 Log Slow Row Threshold The number of ms a row must take to fetch from table-source before it is considered slow. flag: --log-slow-row-threshold env: ZERO_LOG_SLOW_ROW_THRESHOLD default: 2 Mutate API Key An optional secret used to authorize zero-cache to call the API server handling writes. This is sent from zero-cache to your mutate endpoint in an X-Api-Key header. flag: --mutate-api-key env: ZERO_MUTATE_API_KEY Mutate Forward Cookies If true, zero-cache will forward cookies from the request to zero-cache to your mutate endpoint. This is useful for passing authentication cookies to the API server. If false, cookies are not forwarded. flag: --mutate-forward-cookies env: ZERO_MUTATE_FORWARD_COOKIES default: false Mutate URL The URL of the API server to which zero-cache will push mutations. URLs are matched using URLPattern, a standard Web API. Pattern syntax (similar to Express routes): Exact URL match: \"https://api.example.com/mutate\" Any subdomain using wildcard: \"https://*.example.com/mutate\" Multiple subdomain levels: \"https://*.*.example.com/mutate\" Any path under a domain: \"https://api.example.com/*\" Named path parameters: \"https://api.example.com/:version/mutate\" Matches https://api.example.com/v1/mutate, https://api.example.com/v2/mutate, etc. Advanced patterns: Optional path segments: \"https://api.example.com/:path?\" Regex in segments (for specific patterns): \"https://api.example.com/:version(v\\\\d+)/mutate\" matches only v followed by digits. Multiple patterns can be specified, for example: [\"https://api1.example.com/mutate\", \"https://api2.example.com/mutate\"] Query parameters and URL fragments (#) are ignored during matching. See URLPattern for full syntax. flag: --mutate-url env: ZERO_MUTATE_URL Number of Sync Workers The number of processes to use for view syncing. Leave this unset to use the maximum available parallelism. If set to 0, the server runs without sync workers, which is the configuration for running the replication-manager in multi-node deployments. flag: --num-sync-workers env: ZERO_NUM_SYNC_WORKERS Per User Mutation Limit Max The maximum mutations per user within the specified windowMs. flag: --per-user-mutation-limit-max env: ZERO_PER_USER_MUTATION_LIMIT_MAX Per User Mutation Limit Window (ms) The sliding window over which the perUserMutationLimitMax is enforced. flag: --per-user-mutation-limit-window-ms env: ZERO_PER_USER_MUTATION_LIMIT_WINDOW_MS default: 60000 Port The port for sync connections. flag: --port env: ZERO_PORT default: 4848 Query API Key An optional secret used to authorize zero-cache to call the API server handling queries. This is sent from zero-cache to your query endpoint in an X-Api-Key header. flag: --query-api-key env: ZERO_QUERY_API_KEY Query Forward Cookies If true, zero-cache will forward cookies from the request to zero-cache to your query endpoint. This is useful for passing authentication cookies to the API server. If false, cookies are not forwarded. flag: --query-forward-cookies env: ZERO_QUERY_FORWARD_COOKIES default: false Query Hydration Stats Track and log the number of rows considered by query hydrations which take longer than log-slow-hydrate-threshold milliseconds. This is useful for debugging and performance tuning. flag: --query-hydration-stats env: ZERO_QUERY_HYDRATION_STATS Query URL The URL of the API server to which zero-cache will send synced queries. URLs are matched using URLPattern, a standard Web API. Pattern syntax (similar to Express routes): Exact URL match: \"https://api.example.com/query\" Any subdomain using wildcard: \"https://*.example.com/query\" Multiple subdomain levels: \"https://*.*.example.com/query\" Any path under a domain: \"https://api.example.com/*\" Named path parameters: \"https://api.example.com/:version/query\" Matches https://api.example.com/v1/query, https://api.example.com/v2/query, etc. Advanced patterns: Optional path segments: \"https://api.example.com/:path?\" Regex in segments (for specific patterns): \"https://api.example.com/:version(v\\\\d+)/query\" matches only v followed by digits. Multiple patterns can be specified, for example: [\"https://api1.example.com/query\", \"https://api2.example.com/query\"] Query parameters and URL fragments (#) are ignored during matching. See URLPattern for full syntax. flag: --query-url env: ZERO_QUERY_URL Replica File File path to the SQLite replica that zero-cache maintains. This can be lost, but if it is, zero-cache will have to re-replicate next time it starts up. flag: --replica-file env: ZERO_REPLICA_FILE default: \"zero.db\" Replica Vacuum Interval Hours Performs a VACUUM at server startup if the specified number of hours has elapsed since the last VACUUM (or initial-sync). The VACUUM operation is heavyweight and requires double the size of the db in disk space. If unspecified, VACUUM operations are not performed. flag: --replica-vacuum-interval-hours env: ZERO_REPLICA_VACUUM_INTERVAL_HOURS Replica Page Cache Size KiB The SQLite page cache size in kibibytes (KiB) for view-syncer connections. The page cache stores recently accessed database pages in memory to reduce disk I/O. Larger cache sizes improve performance for workloads that fit in cache. If unspecified, SQLite's default (~2 MB) is used. Note that the effective memory use of this setting will be: 2 * cache_size * num_cores, as each connection to the replica gets its own cache and each core maintains 2 connections. flag: --replica-page-cache-size-kib env: ZERO_REPLICA_PAGE_CACHE_SIZE_KIB Server Version The version string outputted to logs when the server starts up. flag: --server-version env: ZERO_SERVER_VERSION Storage DB Temp Dir Temporary directory for IVM operator storage. Leave unset to use os.tmpdir(). flag: --storage-db-tmp-dir env: ZERO_STORAGE_DB_TMP_DIR Task ID Globally unique identifier for the zero-cache instance. Setting this to a platform specific task identifier can be useful for debugging. If unspecified, zero-cache will attempt to extract the TaskARN if run from within an AWS ECS container, and otherwise use a random string. flag: --task-id env: ZERO_TASK_ID Upstream Max Connections The maximum number of connections to open to the upstream database for committing mutations. This is divided evenly amongst sync workers. In addition to this number, zero-cache uses one connection for the replication stream. Note that this number must allow for at least one connection per sync worker, or zero-cache will fail to start. See num-sync-workers. flag: --upstream-max-conns env: ZERO_UPSTREAM_MAX_CONNS default: 20 Websocket Compression Enable WebSocket per-message deflate compression. Compression can reduce bandwidth usage for sync traffic but increases CPU usage on both client and server. Disabled by default. See: https://github.com/websockets/ws#websocket-compression flag: --websocket-compression env: ZERO_WEBSOCKET_COMPRESSION default: false Websocket Compression Options JSON string containing WebSocket compression options. Only used if websocket-compression is enabled. Example: {\"zlibDeflateOptions\":{\"level\":3},\"threshold\":1024}. See https://github.com/websockets/ws/blob/master/doc/ws.md#new-websocketserveroptions-callback for available options. flag: --websocket-compression-options env: ZERO_WEBSOCKET_COMPRESSION_OPTIONS Yield Threshold (ms) The maximum amount of time in milliseconds that a sync worker will spend in IVM (processing query hydration and advancement) before yielding to the event loop. Lower values increase responsiveness and fairness at the cost of reduced throughput. flag: --yield-threshold-ms env: ZERO_YIELD_THRESHOLD_MS default: 10",
    "kind": "section"
  },
  {
    "id": "444-zero-cache-config#app-id",
    "title": "zero-cache Config",
    "searchTitle": "App ID",
    "sectionTitle": "App ID",
    "sectionId": "app-id",
    "url": "/docs/zero-cache-config",
    "content": "Unique identifier for the app. Multiple zero-cache apps can run on a single upstream database, each of which is isolated from the others, with its own permissions, sharding (future feature), and change/cvr databases. The metadata of an app is stored in an upstream schema with the same name, e.g. zero, and the metadata for each app shard, e.g. client and mutation ids, is stored in the {app-id}_{#} schema. (Currently there is only a single \"0\" shard, but this will change with sharding). The CVR and Change data are managed in schemas named {app-id}_{shard-num}/cvr and {app-id}_{shard-num}/cdc, respectively, allowing multiple apps and shards to share the same database instance (e.g. a Postgres \"cluster\") for CVR and Change management. Due to constraints on replication slot names, an App ID may only consist of lower-case letters, numbers, and the underscore character. Note that this option is used by both zero-cache and zero-deploy-permissions. flag: --app-id env: ZERO_APP_ID default: zero",
    "kind": "section"
  },
  {
    "id": "445-zero-cache-config#app-publications",
    "title": "zero-cache Config",
    "searchTitle": "App Publications",
    "sectionTitle": "App Publications",
    "sectionId": "app-publications",
    "url": "/docs/zero-cache-config",
    "content": "Postgres PUBLICATIONs that define the tables and columns to replicate. Publication names may not begin with an underscore, as zero reserves that prefix for internal use. If unspecified, zero-cache will create and use an internal publication that publishes all tables in the public schema, i.e.: CREATE PUBLICATION _{app-id}_public_0 FOR TABLES IN SCHEMA public; Note that changing the set of publications will result in resyncing the replica, which may involve downtime (replication lag) while the new replica is initializing. To change the set of publications without disrupting an existing app, a new app should be created. flag: --app-publications env: ZERO_APP_PUBLICATIONS default: []",
    "kind": "section"
  },
  {
    "id": "446-zero-cache-config#auto-reset",
    "title": "zero-cache Config",
    "searchTitle": "Auto Reset",
    "sectionTitle": "Auto Reset",
    "sectionId": "auto-reset",
    "url": "/docs/zero-cache-config",
    "content": "Automatically wipe and resync the replica when replication is halted. This situation can occur for configurations in which the upstream database provider prohibits event trigger creation, preventing the zero-cache from being able to correctly replicate schema changes. For such configurations, an upstream schema change will instead result in halting replication with an error indicating that the replica needs to be reset. When auto-reset is enabled, zero-cache will respond to such situations by shutting down, and when restarted, resetting the replica and all synced clients. This is a heavy-weight operation and can result in user-visible slowness or downtime if compute resources are scarce. flag: --auto-reset env: ZERO_AUTO_RESET default: true",
    "kind": "section"
  },
  {
    "id": "447-zero-cache-config#change-db",
    "title": "zero-cache Config",
    "searchTitle": "Change DB",
    "sectionTitle": "Change DB",
    "sectionId": "change-db",
    "url": "/docs/zero-cache-config",
    "content": "The Postgres database used to store recent replication log entries, in order to sync multiple view-syncers without requiring multiple replication slots on the upstream database. If unspecified, the upstream-db will be used. flag: --change-db env: ZERO_CHANGE_DB",
    "kind": "section"
  },
  {
    "id": "448-zero-cache-config#change-max-connections",
    "title": "zero-cache Config",
    "searchTitle": "Change Max Connections",
    "sectionTitle": "Change Max Connections",
    "sectionId": "change-max-connections",
    "url": "/docs/zero-cache-config",
    "content": "The maximum number of connections to open to the change database. This is used by the change-streamer for catching up zero-cache replication subscriptions. flag: --change-max-conns env: ZERO_CHANGE_MAX_CONNS default: 5",
    "kind": "section"
  },
  {
    "id": "449-zero-cache-config#change-streamer-mode",
    "title": "zero-cache Config",
    "searchTitle": "Change Streamer Mode",
    "sectionTitle": "Change Streamer Mode",
    "sectionId": "change-streamer-mode",
    "url": "/docs/zero-cache-config",
    "content": "The mode for running or connecting to the change-streamer: dedicated: runs the change-streamer and shuts down when another change-streamer takes over the replication slot. This is appropriate in a single-node configuration, or for the replication-manager in a multi-node configuration. discover: connects to the change-streamer as internally advertised in the change-db. This is appropriate for the view-syncers in a multi-node setup. This may not work in all networking configurations (e.g., some private networking or port forwarding setups). Using ZERO_CHANGE_STREAMER_URI with an explicit routable hostname is recommended instead. This option is ignored if ZERO_CHANGE_STREAMER_URI is set. flag: --change-streamer-mode env: ZERO_CHANGE_STREAMER_MODE default: dedicated",
    "kind": "section"
  },
  {
    "id": "450-zero-cache-config#change-streamer-port",
    "title": "zero-cache Config",
    "searchTitle": "Change Streamer Port",
    "sectionTitle": "Change Streamer Port",
    "sectionId": "change-streamer-port",
    "url": "/docs/zero-cache-config",
    "content": "The port on which the change-streamer runs. This is an internal protocol between the replication-manager and view-syncers, which runs in the same process tree in local development or a single-node configuration. If unspecified, defaults to --port + 1. flag: --change-streamer-port env: ZERO_CHANGE_STREAMER_PORT default: --port + 1",
    "kind": "section"
  },
  {
    "id": "451-zero-cache-config#change-streamer-startup-delay-ms",
    "title": "zero-cache Config",
    "searchTitle": "Change Streamer Startup Delay (ms)",
    "sectionTitle": "Change Streamer Startup Delay (ms)",
    "sectionId": "change-streamer-startup-delay-ms",
    "url": "/docs/zero-cache-config",
    "content": "The delay to wait before the change-streamer takes over the replication stream (i.e. the handoff during replication-manager updates), to allow load balancers to register the task as healthy based on healthcheck parameters. If a change stream request is received during this interval, the delay will be canceled and the takeover will happen immediately, since the incoming request indicates that the task is registered as a target. flag: --change-streamer-startup-delay-ms env: ZERO_CHANGE_STREAMER_STARTUP_DELAY_MS default: 15000",
    "kind": "section"
  },
  {
    "id": "452-zero-cache-config#change-streamer-uri",
    "title": "zero-cache Config",
    "searchTitle": "Change Streamer URI",
    "sectionTitle": "Change Streamer URI",
    "sectionId": "change-streamer-uri",
    "url": "/docs/zero-cache-config",
    "content": "When set, connects to the change-streamer at the given URI. In a multi-node setup, this should be specified in view-syncer options, pointing to the replication-manager URI, which runs a change-streamer on port 4849. flag: --change-streamer-uri env: ZERO_CHANGE_STREAMER_URI",
    "kind": "section"
  },
  {
    "id": "453-zero-cache-config#cvr-db",
    "title": "zero-cache Config",
    "searchTitle": "CVR DB",
    "sectionTitle": "CVR DB",
    "sectionId": "cvr-db",
    "url": "/docs/zero-cache-config",
    "content": "The Postgres database used to store CVRs. CVRs (client view records) keep track of the data synced to clients in order to determine the diff to send on reconnect. If unspecified, the upstream-db will be used. flag: --cvr-db env: ZERO_CVR_DB",
    "kind": "section"
  },
  {
    "id": "454-zero-cache-config#cvr-garbage-collection-inactivity-threshold-hours",
    "title": "zero-cache Config",
    "searchTitle": "CVR Garbage Collection Inactivity Threshold Hours",
    "sectionTitle": "CVR Garbage Collection Inactivity Threshold Hours",
    "sectionId": "cvr-garbage-collection-inactivity-threshold-hours",
    "url": "/docs/zero-cache-config",
    "content": "The duration after which an inactive CVR is eligible for garbage collection. Garbage collection is incremental and periodic, so eligible CVRs are not necessarily purged immediately. flag: --cvr-garbage-collection-inactivity-threshold-hours env: ZERO_CVR_GARBAGE_COLLECTION_INACTIVITY_THRESHOLD_HOURS default: 48",
    "kind": "section"
  },
  {
    "id": "455-zero-cache-config#cvr-garbage-collection-initial-batch-size",
    "title": "zero-cache Config",
    "searchTitle": "CVR Garbage Collection Initial Batch Size",
    "sectionTitle": "CVR Garbage Collection Initial Batch Size",
    "sectionId": "cvr-garbage-collection-initial-batch-size",
    "url": "/docs/zero-cache-config",
    "content": "The initial number of CVRs to purge per garbage collection interval. This number is increased linearly if the rate of new CVRs exceeds the rate of purged CVRs, in order to reach a steady state. Setting this to 0 effectively disables CVR garbage collection. flag: --cvr-garbage-collection-initial-batch-size env: ZERO_CVR_GARBAGE_COLLECTION_INITIAL_BATCH_SIZE default: 25",
    "kind": "section"
  },
  {
    "id": "456-zero-cache-config#cvr-garbage-collection-initial-interval-seconds",
    "title": "zero-cache Config",
    "searchTitle": "CVR Garbage Collection Initial Interval Seconds",
    "sectionTitle": "CVR Garbage Collection Initial Interval Seconds",
    "sectionId": "cvr-garbage-collection-initial-interval-seconds",
    "url": "/docs/zero-cache-config",
    "content": "The initial interval at which to check and garbage collect inactive CVRs. This interval is increased exponentially (up to 16 minutes) when there is nothing to purge. flag: --cvr-garbage-collection-initial-interval-seconds env: ZERO_CVR_GARBAGE_COLLECTION_INITIAL_INTERVAL_SECONDS default: 60",
    "kind": "section"
  },
  {
    "id": "457-zero-cache-config#cvr-max-connections",
    "title": "zero-cache Config",
    "searchTitle": "CVR Max Connections",
    "sectionTitle": "CVR Max Connections",
    "sectionId": "cvr-max-connections",
    "url": "/docs/zero-cache-config",
    "content": "The maximum number of connections to open to the CVR database. This is divided evenly amongst sync workers. Note that this number must allow for at least one connection per sync worker, or zero-cache will fail to start. See num-sync-workers. flag: --cvr-max-conns env: ZERO_CVR_MAX_CONNS default: 30",
    "kind": "section"
  },
  {
    "id": "458-zero-cache-config#enable-query-planner",
    "title": "zero-cache Config",
    "searchTitle": "Enable Query Planner",
    "sectionTitle": "Enable Query Planner",
    "sectionId": "enable-query-planner",
    "url": "/docs/zero-cache-config",
    "content": "Enable the query planner for optimizing ZQL queries. The query planner analyzes and optimizes query execution by determining the most efficient join strategies. You can disable the planner if it is picking bad strategies. flag: --enable-query-planner env: ZERO_ENABLE_QUERY_PLANNER default: true",
    "kind": "section"
  },
  {
    "id": "459-zero-cache-config#enable-telemetry",
    "title": "zero-cache Config",
    "searchTitle": "Enable Telemetry",
    "sectionTitle": "Enable Telemetry",
    "sectionId": "enable-telemetry",
    "url": "/docs/zero-cache-config",
    "content": "Zero collects anonymous telemetry data to help us understand usage. We collect: Zero version Uptime General machine information, like the number of CPUs, OS, CI/CD environment, etc. Information about usage, such as number of queries or mutations processed per hour. This is completely optional and can be disabled at any time. You can also opt-out by setting DO_NOT_TRACK=1. flag: --enable-telemetry env: ZERO_ENABLE_TELEMETRY default: true",
    "kind": "section"
  },
  {
    "id": "460-zero-cache-config#initial-sync-table-copy-workers",
    "title": "zero-cache Config",
    "searchTitle": "Initial Sync Table Copy Workers",
    "sectionTitle": "Initial Sync Table Copy Workers",
    "sectionId": "initial-sync-table-copy-workers",
    "url": "/docs/zero-cache-config",
    "content": "The number of parallel workers used to copy tables during initial sync. Each worker uses a database connection, copies a single table at a time, and buffers up to (approximately) 10 MB of table data in memory during initial sync. Increasing the number of workers may improve initial sync speed; however, local disk throughput (IOPS), upstream CPU, and network bandwidth may also be bottlenecks. flag: --initial-sync-table-copy-workers env: ZERO_INITIAL_SYNC_TABLE_COPY_WORKERS default: 5",
    "kind": "section"
  },
  {
    "id": "461-zero-cache-config#lazy-startup",
    "title": "zero-cache Config",
    "searchTitle": "Lazy Startup",
    "sectionTitle": "Lazy Startup",
    "sectionId": "lazy-startup",
    "url": "/docs/zero-cache-config",
    "content": "Delay starting the majority of zero-cache until first request. This is mainly intended to avoid connecting to Postgres replication stream until the first request is received, which can be useful i.e., for preview instances. Currently only supported in single-node mode. flag: --lazy-startup env: ZERO_LAZY_STARTUP default: false",
    "kind": "section"
  },
  {
    "id": "462-zero-cache-config#litestream-backup-url",
    "title": "zero-cache Config",
    "searchTitle": "Litestream Backup URL",
    "sectionTitle": "Litestream Backup URL",
    "sectionId": "litestream-backup-url",
    "url": "/docs/zero-cache-config",
    "content": "The location of the litestream backup, usually an s3:// URL. This is only consulted by the replication-manager. view-syncers receive this information from the replication-manager. In multi-node deployments, this is required on the replication-manager so view-syncers can reserve snapshots; in single-node deployments it is optional. flag: --litestream-backup-url env: ZERO_LITESTREAM_BACKUP_URL",
    "kind": "section"
  },
  {
    "id": "463-zero-cache-config#litestream-checkpoint-threshold-mb",
    "title": "zero-cache Config",
    "searchTitle": "Litestream Checkpoint Threshold MB",
    "sectionTitle": "Litestream Checkpoint Threshold MB",
    "sectionId": "litestream-checkpoint-threshold-mb",
    "url": "/docs/zero-cache-config",
    "content": "The size of the WAL file at which to perform an SQlite checkpoint to apply the writes in the WAL to the main database file. Each checkpoint creates a new WAL segment file that will be backed up by litestream. Smaller thresholds may improve read performance, at the expense of creating more files to download when restoring the replica from the backup. flag: --litestream-checkpoint-threshold-mb env: ZERO_LITESTREAM_CHECKPOINT_THRESHOLD_MB default: 40",
    "kind": "section"
  },
  {
    "id": "464-zero-cache-config#litestream-config-path",
    "title": "zero-cache Config",
    "searchTitle": "Litestream Config Path",
    "sectionTitle": "Litestream Config Path",
    "sectionId": "litestream-config-path",
    "url": "/docs/zero-cache-config",
    "content": "Path to the litestream yaml config file. zero-cache will run this with its environment variables, which can be referenced in the file via ${ENV} substitution, for example: ZERO_REPLICA_FILE for the db Path ZERO_LITESTREAM_BACKUP_LOCATION for the db replica url ZERO_LITESTREAM_LOG_LEVEL for the log Level ZERO_LOG_FORMAT for the log type flag: --litestream-config-path env: ZERO_LITESTREAM_CONFIG_PATH default: ./src/services/litestream/config.yml",
    "kind": "section"
  },
  {
    "id": "465-zero-cache-config#litestream-executable",
    "title": "zero-cache Config",
    "searchTitle": "Litestream Executable",
    "sectionTitle": "Litestream Executable",
    "sectionId": "litestream-executable",
    "url": "/docs/zero-cache-config",
    "content": "Path to the litestream executable. This option has no effect if litestream-backup-url is unspecified. flag: --litestream-executable env: ZERO_LITESTREAM_EXECUTABLE",
    "kind": "section"
  },
  {
    "id": "466-zero-cache-config#litestream-incremental-backup-interval-minutes",
    "title": "zero-cache Config",
    "searchTitle": "Litestream Incremental Backup Interval Minutes",
    "sectionTitle": "Litestream Incremental Backup Interval Minutes",
    "sectionId": "litestream-incremental-backup-interval-minutes",
    "url": "/docs/zero-cache-config",
    "content": "The interval between incremental backups of the replica. Shorter intervals reduce the amount of change history that needs to be replayed when catching up a new view-syncer, at the expense of increasing the number of files needed to download for the initial litestream restore. flag: --litestream-incremental-backup-interval-minutes env: ZERO_LITESTREAM_INCREMENTAL_BACKUP_INTERVAL_MINUTES default: 15",
    "kind": "section"
  },
  {
    "id": "467-zero-cache-config#litestream-maximum-checkpoint-page-count",
    "title": "zero-cache Config",
    "searchTitle": "Litestream Maximum Checkpoint Page Count",
    "sectionTitle": "Litestream Maximum Checkpoint Page Count",
    "sectionId": "litestream-maximum-checkpoint-page-count",
    "url": "/docs/zero-cache-config",
    "content": "The WAL page count at which SQLite performs a RESTART checkpoint, which blocks writers until complete. Defaults to minCheckpointPageCount * 10. Set to 0 to disable RESTART checkpoints entirely. flag: --litestream-max-checkpoint-page-count env: ZERO_LITESTREAM_MAX_CHECKPOINT_PAGE_COUNT default: minCheckpointPageCount * 10",
    "kind": "section"
  },
  {
    "id": "468-zero-cache-config#litestream-minimum-checkpoint-page-count",
    "title": "zero-cache Config",
    "searchTitle": "Litestream Minimum Checkpoint Page Count",
    "sectionTitle": "Litestream Minimum Checkpoint Page Count",
    "sectionId": "litestream-minimum-checkpoint-page-count",
    "url": "/docs/zero-cache-config",
    "content": "The WAL page count at which SQLite attempts a PASSIVE checkpoint, which transfers pages to the main database file without blocking writers. Defaults to checkpointThresholdMB * 250 (since SQLite page size is 4KB). flag: --litestream-min-checkpoint-page-count env: ZERO_LITESTREAM_MIN_CHECKPOINT_PAGE_COUNT default: checkpointThresholdMB * 250",
    "kind": "section"
  },
  {
    "id": "469-zero-cache-config#litestream-multipart-concurrency",
    "title": "zero-cache Config",
    "searchTitle": "Litestream Multipart Concurrency",
    "sectionTitle": "Litestream Multipart Concurrency",
    "sectionId": "litestream-multipart-concurrency",
    "url": "/docs/zero-cache-config",
    "content": "The number of parts (of size --litestream-multipart-size bytes) to upload or download in parallel when backing up or restoring the snapshot. flag: --litestream-multipart-concurrency env: ZERO_LITESTREAM_MULTIPART_CONCURRENCY default: 48",
    "kind": "section"
  },
  {
    "id": "470-zero-cache-config#litestream-multipart-size",
    "title": "zero-cache Config",
    "searchTitle": "Litestream Multipart Size",
    "sectionTitle": "Litestream Multipart Size",
    "sectionId": "litestream-multipart-size",
    "url": "/docs/zero-cache-config",
    "content": "The size of each part when uploading or downloading the snapshot with --litestream-multipart-concurrency. Note that up to concurrency * size bytes of memory are used when backing up or restoring the snapshot. flag: --litestream-multipart-size env: ZERO_LITESTREAM_MULTIPART_SIZE default: 16777216 (16 MiB)",
    "kind": "section"
  },
  {
    "id": "471-zero-cache-config#litestream-log-level",
    "title": "zero-cache Config",
    "searchTitle": "Litestream Log Level",
    "sectionTitle": "Litestream Log Level",
    "sectionId": "litestream-log-level",
    "url": "/docs/zero-cache-config",
    "content": "flag: --litestream-log-level env: ZERO_LITESTREAM_LOG_LEVEL default: warn values: debug, info, warn, error",
    "kind": "section"
  },
  {
    "id": "472-zero-cache-config#litestream-port",
    "title": "zero-cache Config",
    "searchTitle": "Litestream Port",
    "sectionTitle": "Litestream Port",
    "sectionId": "litestream-port",
    "url": "/docs/zero-cache-config",
    "content": "Port on which litestream exports metrics, used to determine the replication watermark up to which it is safe to purge change log records. flag: --litestream-port env: ZERO_LITESTREAM_PORT default: --port + 2",
    "kind": "section"
  },
  {
    "id": "473-zero-cache-config#litestream-restore-parallelism",
    "title": "zero-cache Config",
    "searchTitle": "Litestream Restore Parallelism",
    "sectionTitle": "Litestream Restore Parallelism",
    "sectionId": "litestream-restore-parallelism",
    "url": "/docs/zero-cache-config",
    "content": "The number of WAL files to download in parallel when performing the initial restore of the replica from the backup. flag: --litestream-restore-parallelism env: ZERO_LITESTREAM_RESTORE_PARALLELISM default: 48",
    "kind": "section"
  },
  {
    "id": "474-zero-cache-config#litestream-snapshot-backup-interval-hours",
    "title": "zero-cache Config",
    "searchTitle": "Litestream Snapshot Backup Interval Hours",
    "sectionTitle": "Litestream Snapshot Backup Interval Hours",
    "sectionId": "litestream-snapshot-backup-interval-hours",
    "url": "/docs/zero-cache-config",
    "content": "The interval between snapshot backups of the replica. Snapshot backups make a full copy of the database to a new litestream generation. This improves restore time at the expense of bandwidth. Applications with a large database and low write rate can increase this interval to reduce network usage for backups (litestream defaults to 24 hours). flag: --litestream-snapshot-backup-interval-hours env: ZERO_LITESTREAM_SNAPSHOT_BACKUP_INTERVAL_HOURS default: 12",
    "kind": "section"
  },
  {
    "id": "475-zero-cache-config#log-format",
    "title": "zero-cache Config",
    "searchTitle": "Log Format",
    "sectionTitle": "Log Format",
    "sectionId": "log-format",
    "url": "/docs/zero-cache-config",
    "content": "Use text for developer-friendly console logging and json for consumption by structured-logging services. flag: --log-format env: ZERO_LOG_FORMAT default: \"text\" values: text, json",
    "kind": "section"
  },
  {
    "id": "476-zero-cache-config#log-ivm-sampling",
    "title": "zero-cache Config",
    "searchTitle": "Log IVM Sampling",
    "sectionTitle": "Log IVM Sampling",
    "sectionId": "log-ivm-sampling",
    "url": "/docs/zero-cache-config",
    "content": "How often to collect IVM metrics. 1 out of N requests will be sampled where N is this value. flag: --log-ivm-sampling env: ZERO_LOG_IVM_SAMPLING default: 5000",
    "kind": "section"
  },
  {
    "id": "477-zero-cache-config#log-level",
    "title": "zero-cache Config",
    "searchTitle": "Log Level",
    "sectionTitle": "Log Level",
    "sectionId": "log-level",
    "url": "/docs/zero-cache-config",
    "content": "Sets the logging level for the application. flag: --log-level env: ZERO_LOG_LEVEL default: \"info\" values: debug, info, warn, error",
    "kind": "section"
  },
  {
    "id": "478-zero-cache-config#log-slow-hydrate-threshold",
    "title": "zero-cache Config",
    "searchTitle": "Log Slow Hydrate Threshold",
    "sectionTitle": "Log Slow Hydrate Threshold",
    "sectionId": "log-slow-hydrate-threshold",
    "url": "/docs/zero-cache-config",
    "content": "The number of milliseconds a query hydration must take to print a slow warning. flag: --log-slow-hydrate-threshold env: ZERO_LOG_SLOW_HYDRATE_THRESHOLD default: 100",
    "kind": "section"
  },
  {
    "id": "479-zero-cache-config#log-slow-row-threshold",
    "title": "zero-cache Config",
    "searchTitle": "Log Slow Row Threshold",
    "sectionTitle": "Log Slow Row Threshold",
    "sectionId": "log-slow-row-threshold",
    "url": "/docs/zero-cache-config",
    "content": "The number of ms a row must take to fetch from table-source before it is considered slow. flag: --log-slow-row-threshold env: ZERO_LOG_SLOW_ROW_THRESHOLD default: 2",
    "kind": "section"
  },
  {
    "id": "480-zero-cache-config#mutate-api-key",
    "title": "zero-cache Config",
    "searchTitle": "Mutate API Key",
    "sectionTitle": "Mutate API Key",
    "sectionId": "mutate-api-key",
    "url": "/docs/zero-cache-config",
    "content": "An optional secret used to authorize zero-cache to call the API server handling writes. This is sent from zero-cache to your mutate endpoint in an X-Api-Key header. flag: --mutate-api-key env: ZERO_MUTATE_API_KEY",
    "kind": "section"
  },
  {
    "id": "481-zero-cache-config#mutate-forward-cookies",
    "title": "zero-cache Config",
    "searchTitle": "Mutate Forward Cookies",
    "sectionTitle": "Mutate Forward Cookies",
    "sectionId": "mutate-forward-cookies",
    "url": "/docs/zero-cache-config",
    "content": "If true, zero-cache will forward cookies from the request to zero-cache to your mutate endpoint. This is useful for passing authentication cookies to the API server. If false, cookies are not forwarded. flag: --mutate-forward-cookies env: ZERO_MUTATE_FORWARD_COOKIES default: false",
    "kind": "section"
  },
  {
    "id": "482-zero-cache-config#mutate-url",
    "title": "zero-cache Config",
    "searchTitle": "Mutate URL",
    "sectionTitle": "Mutate URL",
    "sectionId": "mutate-url",
    "url": "/docs/zero-cache-config",
    "content": "The URL of the API server to which zero-cache will push mutations. URLs are matched using URLPattern, a standard Web API. Pattern syntax (similar to Express routes): Exact URL match: \"https://api.example.com/mutate\" Any subdomain using wildcard: \"https://*.example.com/mutate\" Multiple subdomain levels: \"https://*.*.example.com/mutate\" Any path under a domain: \"https://api.example.com/*\" Named path parameters: \"https://api.example.com/:version/mutate\" Matches https://api.example.com/v1/mutate, https://api.example.com/v2/mutate, etc. Advanced patterns: Optional path segments: \"https://api.example.com/:path?\" Regex in segments (for specific patterns): \"https://api.example.com/:version(v\\\\d+)/mutate\" matches only v followed by digits. Multiple patterns can be specified, for example: [\"https://api1.example.com/mutate\", \"https://api2.example.com/mutate\"] Query parameters and URL fragments (#) are ignored during matching. See URLPattern for full syntax. flag: --mutate-url env: ZERO_MUTATE_URL",
    "kind": "section"
  },
  {
    "id": "483-zero-cache-config#number-of-sync-workers",
    "title": "zero-cache Config",
    "searchTitle": "Number of Sync Workers",
    "sectionTitle": "Number of Sync Workers",
    "sectionId": "number-of-sync-workers",
    "url": "/docs/zero-cache-config",
    "content": "The number of processes to use for view syncing. Leave this unset to use the maximum available parallelism. If set to 0, the server runs without sync workers, which is the configuration for running the replication-manager in multi-node deployments. flag: --num-sync-workers env: ZERO_NUM_SYNC_WORKERS",
    "kind": "section"
  },
  {
    "id": "484-zero-cache-config#per-user-mutation-limit-max",
    "title": "zero-cache Config",
    "searchTitle": "Per User Mutation Limit Max",
    "sectionTitle": "Per User Mutation Limit Max",
    "sectionId": "per-user-mutation-limit-max",
    "url": "/docs/zero-cache-config",
    "content": "The maximum mutations per user within the specified windowMs. flag: --per-user-mutation-limit-max env: ZERO_PER_USER_MUTATION_LIMIT_MAX",
    "kind": "section"
  },
  {
    "id": "485-zero-cache-config#per-user-mutation-limit-window-ms",
    "title": "zero-cache Config",
    "searchTitle": "Per User Mutation Limit Window (ms)",
    "sectionTitle": "Per User Mutation Limit Window (ms)",
    "sectionId": "per-user-mutation-limit-window-ms",
    "url": "/docs/zero-cache-config",
    "content": "The sliding window over which the perUserMutationLimitMax is enforced. flag: --per-user-mutation-limit-window-ms env: ZERO_PER_USER_MUTATION_LIMIT_WINDOW_MS default: 60000",
    "kind": "section"
  },
  {
    "id": "486-zero-cache-config#port",
    "title": "zero-cache Config",
    "searchTitle": "Port",
    "sectionTitle": "Port",
    "sectionId": "port",
    "url": "/docs/zero-cache-config",
    "content": "The port for sync connections. flag: --port env: ZERO_PORT default: 4848",
    "kind": "section"
  },
  {
    "id": "487-zero-cache-config#query-api-key",
    "title": "zero-cache Config",
    "searchTitle": "Query API Key",
    "sectionTitle": "Query API Key",
    "sectionId": "query-api-key",
    "url": "/docs/zero-cache-config",
    "content": "An optional secret used to authorize zero-cache to call the API server handling queries. This is sent from zero-cache to your query endpoint in an X-Api-Key header. flag: --query-api-key env: ZERO_QUERY_API_KEY",
    "kind": "section"
  },
  {
    "id": "488-zero-cache-config#query-forward-cookies",
    "title": "zero-cache Config",
    "searchTitle": "Query Forward Cookies",
    "sectionTitle": "Query Forward Cookies",
    "sectionId": "query-forward-cookies",
    "url": "/docs/zero-cache-config",
    "content": "If true, zero-cache will forward cookies from the request to zero-cache to your query endpoint. This is useful for passing authentication cookies to the API server. If false, cookies are not forwarded. flag: --query-forward-cookies env: ZERO_QUERY_FORWARD_COOKIES default: false",
    "kind": "section"
  },
  {
    "id": "489-zero-cache-config#query-hydration-stats",
    "title": "zero-cache Config",
    "searchTitle": "Query Hydration Stats",
    "sectionTitle": "Query Hydration Stats",
    "sectionId": "query-hydration-stats",
    "url": "/docs/zero-cache-config",
    "content": "Track and log the number of rows considered by query hydrations which take longer than log-slow-hydrate-threshold milliseconds. This is useful for debugging and performance tuning. flag: --query-hydration-stats env: ZERO_QUERY_HYDRATION_STATS",
    "kind": "section"
  },
  {
    "id": "490-zero-cache-config#query-url",
    "title": "zero-cache Config",
    "searchTitle": "Query URL",
    "sectionTitle": "Query URL",
    "sectionId": "query-url",
    "url": "/docs/zero-cache-config",
    "content": "The URL of the API server to which zero-cache will send synced queries. URLs are matched using URLPattern, a standard Web API. Pattern syntax (similar to Express routes): Exact URL match: \"https://api.example.com/query\" Any subdomain using wildcard: \"https://*.example.com/query\" Multiple subdomain levels: \"https://*.*.example.com/query\" Any path under a domain: \"https://api.example.com/*\" Named path parameters: \"https://api.example.com/:version/query\" Matches https://api.example.com/v1/query, https://api.example.com/v2/query, etc. Advanced patterns: Optional path segments: \"https://api.example.com/:path?\" Regex in segments (for specific patterns): \"https://api.example.com/:version(v\\\\d+)/query\" matches only v followed by digits. Multiple patterns can be specified, for example: [\"https://api1.example.com/query\", \"https://api2.example.com/query\"] Query parameters and URL fragments (#) are ignored during matching. See URLPattern for full syntax. flag: --query-url env: ZERO_QUERY_URL",
    "kind": "section"
  },
  {
    "id": "491-zero-cache-config#replica-file",
    "title": "zero-cache Config",
    "searchTitle": "Replica File",
    "sectionTitle": "Replica File",
    "sectionId": "replica-file",
    "url": "/docs/zero-cache-config",
    "content": "File path to the SQLite replica that zero-cache maintains. This can be lost, but if it is, zero-cache will have to re-replicate next time it starts up. flag: --replica-file env: ZERO_REPLICA_FILE default: \"zero.db\"",
    "kind": "section"
  },
  {
    "id": "492-zero-cache-config#replica-vacuum-interval-hours",
    "title": "zero-cache Config",
    "searchTitle": "Replica Vacuum Interval Hours",
    "sectionTitle": "Replica Vacuum Interval Hours",
    "sectionId": "replica-vacuum-interval-hours",
    "url": "/docs/zero-cache-config",
    "content": "Performs a VACUUM at server startup if the specified number of hours has elapsed since the last VACUUM (or initial-sync). The VACUUM operation is heavyweight and requires double the size of the db in disk space. If unspecified, VACUUM operations are not performed. flag: --replica-vacuum-interval-hours env: ZERO_REPLICA_VACUUM_INTERVAL_HOURS",
    "kind": "section"
  },
  {
    "id": "493-zero-cache-config#replica-page-cache-size-kib",
    "title": "zero-cache Config",
    "searchTitle": "Replica Page Cache Size KiB",
    "sectionTitle": "Replica Page Cache Size KiB",
    "sectionId": "replica-page-cache-size-kib",
    "url": "/docs/zero-cache-config",
    "content": "The SQLite page cache size in kibibytes (KiB) for view-syncer connections. The page cache stores recently accessed database pages in memory to reduce disk I/O. Larger cache sizes improve performance for workloads that fit in cache. If unspecified, SQLite's default (~2 MB) is used. Note that the effective memory use of this setting will be: 2 * cache_size * num_cores, as each connection to the replica gets its own cache and each core maintains 2 connections. flag: --replica-page-cache-size-kib env: ZERO_REPLICA_PAGE_CACHE_SIZE_KIB",
    "kind": "section"
  },
  {
    "id": "494-zero-cache-config#server-version",
    "title": "zero-cache Config",
    "searchTitle": "Server Version",
    "sectionTitle": "Server Version",
    "sectionId": "server-version",
    "url": "/docs/zero-cache-config",
    "content": "The version string outputted to logs when the server starts up. flag: --server-version env: ZERO_SERVER_VERSION",
    "kind": "section"
  },
  {
    "id": "495-zero-cache-config#storage-db-temp-dir",
    "title": "zero-cache Config",
    "searchTitle": "Storage DB Temp Dir",
    "sectionTitle": "Storage DB Temp Dir",
    "sectionId": "storage-db-temp-dir",
    "url": "/docs/zero-cache-config",
    "content": "Temporary directory for IVM operator storage. Leave unset to use os.tmpdir(). flag: --storage-db-tmp-dir env: ZERO_STORAGE_DB_TMP_DIR",
    "kind": "section"
  },
  {
    "id": "496-zero-cache-config#task-id",
    "title": "zero-cache Config",
    "searchTitle": "Task ID",
    "sectionTitle": "Task ID",
    "sectionId": "task-id",
    "url": "/docs/zero-cache-config",
    "content": "Globally unique identifier for the zero-cache instance. Setting this to a platform specific task identifier can be useful for debugging. If unspecified, zero-cache will attempt to extract the TaskARN if run from within an AWS ECS container, and otherwise use a random string. flag: --task-id env: ZERO_TASK_ID",
    "kind": "section"
  },
  {
    "id": "497-zero-cache-config#upstream-max-connections",
    "title": "zero-cache Config",
    "searchTitle": "Upstream Max Connections",
    "sectionTitle": "Upstream Max Connections",
    "sectionId": "upstream-max-connections",
    "url": "/docs/zero-cache-config",
    "content": "The maximum number of connections to open to the upstream database for committing mutations. This is divided evenly amongst sync workers. In addition to this number, zero-cache uses one connection for the replication stream. Note that this number must allow for at least one connection per sync worker, or zero-cache will fail to start. See num-sync-workers. flag: --upstream-max-conns env: ZERO_UPSTREAM_MAX_CONNS default: 20",
    "kind": "section"
  },
  {
    "id": "498-zero-cache-config#websocket-compression",
    "title": "zero-cache Config",
    "searchTitle": "Websocket Compression",
    "sectionTitle": "Websocket Compression",
    "sectionId": "websocket-compression",
    "url": "/docs/zero-cache-config",
    "content": "Enable WebSocket per-message deflate compression. Compression can reduce bandwidth usage for sync traffic but increases CPU usage on both client and server. Disabled by default. See: https://github.com/websockets/ws#websocket-compression flag: --websocket-compression env: ZERO_WEBSOCKET_COMPRESSION default: false",
    "kind": "section"
  },
  {
    "id": "499-zero-cache-config#websocket-compression-options",
    "title": "zero-cache Config",
    "searchTitle": "Websocket Compression Options",
    "sectionTitle": "Websocket Compression Options",
    "sectionId": "websocket-compression-options",
    "url": "/docs/zero-cache-config",
    "content": "JSON string containing WebSocket compression options. Only used if websocket-compression is enabled. Example: {\"zlibDeflateOptions\":{\"level\":3},\"threshold\":1024}. See https://github.com/websockets/ws/blob/master/doc/ws.md#new-websocketserveroptions-callback for available options. flag: --websocket-compression-options env: ZERO_WEBSOCKET_COMPRESSION_OPTIONS",
    "kind": "section"
  },
  {
    "id": "500-zero-cache-config#yield-threshold-ms",
    "title": "zero-cache Config",
    "searchTitle": "Yield Threshold (ms)",
    "sectionTitle": "Yield Threshold (ms)",
    "sectionId": "yield-threshold-ms",
    "url": "/docs/zero-cache-config",
    "content": "The maximum amount of time in milliseconds that a sync worker will spend in IVM (processing query hydration and advancement) before yielding to the event loop. Lower values increase responsiveness and fairness at the cost of reduced throughput. flag: --yield-threshold-ms env: ZERO_YIELD_THRESHOLD_MS default: 10",
    "kind": "section"
  },
  {
    "id": "501-zero-cache-config#deprecated-flags",
    "title": "zero-cache Config",
    "searchTitle": "Deprecated Flags",
    "sectionTitle": "Deprecated Flags",
    "sectionId": "deprecated-flags",
    "url": "/docs/zero-cache-config",
    "content": "Auth JWK A public key in JWK format used to verify JWTs. Only one of jwk, jwksUrl and secret may be set. flag: --auth-jwk env: ZERO_AUTH_JWK Auth JWKS URL A URL that returns a JWK set used to verify JWTs. Only one of jwk, jwksUrl and secret may be set. flag: --auth-jwks-url env: ZERO_AUTH_JWKS_URL Auth Secret A symmetric key used to verify JWTs. Only one of jwk, jwksUrl and secret may be set. flag: --auth-secret env: ZERO_AUTH_SECRET",
    "kind": "section"
  },
  {
    "id": "502-zero-cache-config#auth-jwk",
    "title": "zero-cache Config",
    "searchTitle": "Auth JWK",
    "sectionTitle": "Auth JWK",
    "sectionId": "auth-jwk",
    "url": "/docs/zero-cache-config",
    "content": "A public key in JWK format used to verify JWTs. Only one of jwk, jwksUrl and secret may be set. flag: --auth-jwk env: ZERO_AUTH_JWK",
    "kind": "section"
  },
  {
    "id": "503-zero-cache-config#auth-jwks-url",
    "title": "zero-cache Config",
    "searchTitle": "Auth JWKS URL",
    "sectionTitle": "Auth JWKS URL",
    "sectionId": "auth-jwks-url",
    "url": "/docs/zero-cache-config",
    "content": "A URL that returns a JWK set used to verify JWTs. Only one of jwk, jwksUrl and secret may be set. flag: --auth-jwks-url env: ZERO_AUTH_JWKS_URL",
    "kind": "section"
  },
  {
    "id": "504-zero-cache-config#auth-secret",
    "title": "zero-cache Config",
    "searchTitle": "Auth Secret",
    "sectionTitle": "Auth Secret",
    "sectionId": "auth-secret",
    "url": "/docs/zero-cache-config",
    "content": "A symmetric key used to verify JWTs. Only one of jwk, jwksUrl and secret may be set. flag: --auth-secret env: ZERO_AUTH_SECRET",
    "kind": "section"
  },
  {
    "id": "61-zql",
    "title": "ZQL",
    "searchTitle": "ZQL",
    "url": "/docs/zql",
    "content": "Inspired by SQL, ZQL is expressed in TypeScript with heavy use of the builder pattern. If you have used Drizzle or Kysely, ZQL will feel familiar. ZQL queries are composed of one or more clauses that are chained together into a query. Create a Builder To get started, use createBuilder. If you use drizzle-zero or prisma-zero, this happens automatically and an instance is stored in the zql constant exported from schema.ts: import {zql} from 'schema.ts' // zql.myTable.where(...) Otherwise, create an instance manually: // schema.ts // ... export const zql = createBuilder(schema) Select ZQL queries start by selecting a table. There is no way to select a subset of columns; ZQL queries always return the entire row, if permissions allow it. import {zql} from 'zero.ts' // Returns a query that selects all rows and columns from the // issue table. zql.issue This is a design tradeoff that allows Zero to better reuse the row locally for future queries. This also makes it easier to share types between different parts of the code. This means you should not modify the data directly. Instead, clone the data and modify the clone. ZQL caches values and returns them multiple times. If you modify a value returned from ZQL, you will modify it everywhere it is used. This can lead to subtle bugs. JavaScript and TypeScript lack true immutable types so we use readonly to help enforce it. But it's easy to cast away the readonly accidentally. Ordering You can sort query results by adding an orderBy clause: zql.issue.orderBy('created', 'desc') Multiple orderBy clauses can be present, in which case the data is sorted by those clauses in order: // Order by priority descending. For any rows with same priority, // then order by created desc. zql.issue .orderBy('priority', 'desc') .orderBy('created', 'desc') All queries in ZQL have a default final order of their primary key. Assuming the issue table has a primary key on the id column, then: // Actually means: zql.issue.orderBy('id', 'asc'); zql.issue // Actually means: zql.issue.orderBy('priority', 'desc').orderBy('id', 'asc'); zql.issue.orderBy('priority', 'desc') Limit You can limit the number of rows to return with limit(): zql.issue.orderBy('created', 'desc').limit(100) Paging You can start the results at or after a particular row with start(): let start: IssueRow | undefined while (true) { let q = zql.issue .orderBy('created', 'desc') .limit(100) if (start) { q = q.start(start) } const batch = await q.run() console.log('got batch', batch) if (batch.length < 100) { break } start = batch[batch.length - 1] } By default start() is exclusive - it returns rows starting after the supplied reference row. This is what you usually want for paging. If you want inclusive results, you can do: zql.issue.start(row, {inclusive: true}) Getting a Single Result If you want exactly zero or one results, use the one() clause. This causes ZQL to return Row|undefined rather than Row[]. const result = await zql.issue .where('id', 42) .one() .run() if (!result) { console.error('not found') } one() overrides any limit() clause that is also present. Relationships You can query related rows using relationships that are defined in your Zero schema. // Get all issues and their related comments zql.issue.related('comments') Relationships are returned as hierarchical data. In the above example, each row will have a comments field, which is an array of the corresponding comments rows. You can fetch multiple relationships in a single query: zql.issue .related('comments') .related('reactions') .related('assignees') Refining Relationships By default all matching relationship rows are returned, but this can be refined. The related method accepts an optional second function which is itself a query. zql.issue.related( 'comments', // It is common to use the 'q' shorthand variable for this parameter, // but it is a _comment_ query in particular here, exactly as if you // had done zql.comment. q => q .orderBy('modified', 'desc') .limit(100) .start(lastSeenComment) ) This relationship query can have all the same clauses that top-level queries can have. Using orderBy or limit in a relationship that goes through a junction table (i.e., a many-to-many relationship) is not currently supported and will throw a runtime error. See bug 3527. You can sometimes work around this by making the junction relationship explicit, depending on your schema and usage. Nested Relationships You can nest relationships arbitrarily: // Get all issues, first 100 comments for each (ordered by modified,desc), // and for each comment all of its reactions. zql.issue.related('comments', q => q .orderBy('modified', 'desc') .limit(100) .related('reactions') ) Where You can filter a query with where(): zql.issue.where('priority', '=', 'high') The first parameter is always a column name from the table being queried. TypeScript completion will offer available options (sourced from your Zero Schema). Comparison Operators Where supports the following comparison operators: TypeScript will restrict you from using operators with types that don‚Äôt make sense ‚Äì you can‚Äôt use > with boolean for example. If you don‚Äôt see the comparison operator you need, let us know, many are easy to add. Equals is the Default Comparison Operator Because comparing by = is so common, you can leave it out and where defaults to =. zql.issue.where('priority', 'high') Comparing to null As in SQL, ZQL‚Äôs null cannot be compared with =, !=, <, or any other normal comparison operator. Comparing any value to null with such operators is always false: These semantics feel a bit weird, but they are consistent with SQL. The reason SQL does it this way is to make join semantics work: if you‚Äôre joining employee.orgID on org.id you do not want an employee in no organization to match an org that hasn‚Äôt yet been assigned an ID. For when you purposely do want to compare to null ZQL supports IS and IS NOT operators that also work just like in SQL: // Find employees not in any org. zql.employee.where('orgID', 'IS', null) // Find employees in an org other than 42 OR employees in NO org zql.employee.where('orgID', 'IS NOT', 42) TypeScript will prevent you from comparing to null with other operators. Compound Filters The argument to where can also be a callback that returns a complex expression: // Get all issues that have priority 'critical' or else have both // priority 'medium' and not more than 100 votes. zql.issue.where(({cmp, and, or, not}) => or( cmp('priority', 'critical'), and( cmp('priority', 'medium'), not(cmp('numVotes', '>', 100)) ) ) ) cmp is short for compare and works the same as where at the top-level except that it can‚Äôt be chained and it only accepts comparison operators (no relationship filters ‚Äì see below). Note that chaining where() is also a one-level and: // Find issues with priority 3 or higher, owned by aa zql.issue .where('priority', '>=', 3) .where('owner', 'aa') Comparing Literal Values The where clause always expects its first parameter to be a column name as a string. Same with the cmp helper: // \"foo\" is a column name, not a string: zql.issue.where('foo', 'bar') // \"foo\" is a column name, not a string: zql.issue.where(({cmp}) => cmp('foo', 'bar')) To compare to a literal value, use the cmpLit helper: zql.issue.where(cmpLit('foobar', 'foo' + 'bar')) This is particularly useful for implementing permissions, because the first parameter can be a field of your context: zql.issue.where(cmpLit(ctx.role, 'admin')) Relationship Filters Your filter can also test properties of relationships. Currently the only supported test is existence: // Find all orgs that have at least one employee zql.organization.whereExists('employees') The argument to whereExists is a relationship, so just like other relationships, it can be refined with a query: // Find all orgs that have at least one cool employee zql.organization.whereExists('employees', q => q.where('location', 'Hawaii') ) As with querying relationships, relationship filters can be arbitrarily nested: // Get all issues that have comments that have reactions zql.issue.whereExists('comments', q => q.whereExists('reactions') ) The exists helper is also provided which can be used with and, or, cmp, and not to build compound filters that check relationship existence: // Find issues that have at least one comment or are high priority zql.issue.where({cmp, or, exists} => or( cmp('priority', 'high'), exists('comments'), ), ) Type Helpers You can get the TypeScript type of the result of a query using the QueryResultType helper: import type {QueryResultType} from '@rocicorp/zero' const complexQuery = zql.issue.related( 'comments', q => q.related('author') ) type MyComplexResult = QueryResultType<typeof complexQuery> // MyComplexResult is: readonly IssueRow & { // readonly comments: readonly (CommentRow & { // readonly author: readonly AuthorRow|undefined; // })[]; // }[] You can get the type of a single row with QueryRowType: import type {QueryRowType} from '@rocicorp/zero' type MySingleRow = QueryRowType<typeof complexQuery> // MySingleRow is: readonly IssueRow & { // readonly comments: readonly (CommentRow & { // readonly author: readonly AuthorRow|undefined; // })[]; // } Planning Zero automatically plans queries, selecting the best indexes and join orders in most cases. Inspecting Query Plans You can inspect the plan that Zero generates for any ZQL query using the inspector. Manually Flipping Joins The process Zero uses to optimize joins is called \"join flipping\", because it involves \"flipping\" the order of joins to minimize the number of rows processed. Typically the Zero planner will pick the joins to flip automatically. But in some rare cases, you may want to manually specify the join order. This can be done by passing the flip:true option to whereExists: // Find the first 100 documents that user 42 can edit, // ordered by created desc. Because each user is an editor // of only a few documents, flip:true is much faster than // flip:false. zql.documents.whereExists('editors', e => e.where('userID', 42), {flip: true} ), .orderBy('created', 'desc') .limit(100) Or with exists: // Find issues created by user 42 or that have a comment // by user 42. Because user 42 has commented on only a // few issues, flip:true is much faster than flip:false. zql.issue.where({cmp, or, exists} => or( cmp('creatorID', 42), exists('comments', c => c.where('creatorID', 42), {flip: true}), ), ) You can manually flip just one or a few of the whereExists clauses in a query, leaving the rest to be planned automatically.",
    "headings": [
      {
        "text": "Create a Builder",
        "id": "create-a-builder"
      },
      {
        "text": "Select",
        "id": "select"
      },
      {
        "text": "Ordering",
        "id": "ordering"
      },
      {
        "text": "Limit",
        "id": "limit"
      },
      {
        "text": "Paging",
        "id": "paging"
      },
      {
        "text": "Getting a Single Result",
        "id": "getting-a-single-result"
      },
      {
        "text": "Relationships",
        "id": "relationships"
      },
      {
        "text": "Refining Relationships",
        "id": "refining-relationships"
      },
      {
        "text": "Nested Relationships",
        "id": "nested-relationships"
      },
      {
        "text": "Where",
        "id": "where"
      },
      {
        "text": "Comparison Operators",
        "id": "comparison-operators"
      },
      {
        "text": "Equals is the Default Comparison Operator",
        "id": "equals-is-the-default-comparison-operator"
      },
      {
        "text": "Comparing to null",
        "id": "comparing-to-null"
      },
      {
        "text": "Compound Filters",
        "id": "compound-filters"
      },
      {
        "text": "Comparing Literal Values",
        "id": "comparing-literal-values"
      },
      {
        "text": "Relationship Filters",
        "id": "relationship-filters"
      },
      {
        "text": "Type Helpers",
        "id": "type-helpers"
      },
      {
        "text": "Planning",
        "id": "planning"
      },
      {
        "text": "Inspecting Query Plans",
        "id": "inspecting-query-plans"
      },
      {
        "text": "Manually Flipping Joins",
        "id": "manually-flipping-joins"
      }
    ],
    "kind": "page"
  },
  {
    "id": "505-zql#create-a-builder",
    "title": "ZQL",
    "searchTitle": "Create a Builder",
    "sectionTitle": "Create a Builder",
    "sectionId": "create-a-builder",
    "url": "/docs/zql",
    "content": "To get started, use createBuilder. If you use drizzle-zero or prisma-zero, this happens automatically and an instance is stored in the zql constant exported from schema.ts: import {zql} from 'schema.ts' // zql.myTable.where(...) Otherwise, create an instance manually: // schema.ts // ... export const zql = createBuilder(schema)",
    "kind": "section"
  },
  {
    "id": "506-zql#select",
    "title": "ZQL",
    "searchTitle": "Select",
    "sectionTitle": "Select",
    "sectionId": "select",
    "url": "/docs/zql",
    "content": "ZQL queries start by selecting a table. There is no way to select a subset of columns; ZQL queries always return the entire row, if permissions allow it. import {zql} from 'zero.ts' // Returns a query that selects all rows and columns from the // issue table. zql.issue This is a design tradeoff that allows Zero to better reuse the row locally for future queries. This also makes it easier to share types between different parts of the code. This means you should not modify the data directly. Instead, clone the data and modify the clone. ZQL caches values and returns them multiple times. If you modify a value returned from ZQL, you will modify it everywhere it is used. This can lead to subtle bugs. JavaScript and TypeScript lack true immutable types so we use readonly to help enforce it. But it's easy to cast away the readonly accidentally.",
    "kind": "section"
  },
  {
    "id": "507-zql#ordering",
    "title": "ZQL",
    "searchTitle": "Ordering",
    "sectionTitle": "Ordering",
    "sectionId": "ordering",
    "url": "/docs/zql",
    "content": "You can sort query results by adding an orderBy clause: zql.issue.orderBy('created', 'desc') Multiple orderBy clauses can be present, in which case the data is sorted by those clauses in order: // Order by priority descending. For any rows with same priority, // then order by created desc. zql.issue .orderBy('priority', 'desc') .orderBy('created', 'desc') All queries in ZQL have a default final order of their primary key. Assuming the issue table has a primary key on the id column, then: // Actually means: zql.issue.orderBy('id', 'asc'); zql.issue // Actually means: zql.issue.orderBy('priority', 'desc').orderBy('id', 'asc'); zql.issue.orderBy('priority', 'desc')",
    "kind": "section"
  },
  {
    "id": "508-zql#limit",
    "title": "ZQL",
    "searchTitle": "Limit",
    "sectionTitle": "Limit",
    "sectionId": "limit",
    "url": "/docs/zql",
    "content": "You can limit the number of rows to return with limit(): zql.issue.orderBy('created', 'desc').limit(100)",
    "kind": "section"
  },
  {
    "id": "509-zql#paging",
    "title": "ZQL",
    "searchTitle": "Paging",
    "sectionTitle": "Paging",
    "sectionId": "paging",
    "url": "/docs/zql",
    "content": "You can start the results at or after a particular row with start(): let start: IssueRow | undefined while (true) { let q = zql.issue .orderBy('created', 'desc') .limit(100) if (start) { q = q.start(start) } const batch = await q.run() console.log('got batch', batch) if (batch.length < 100) { break } start = batch[batch.length - 1] } By default start() is exclusive - it returns rows starting after the supplied reference row. This is what you usually want for paging. If you want inclusive results, you can do: zql.issue.start(row, {inclusive: true})",
    "kind": "section"
  },
  {
    "id": "510-zql#getting-a-single-result",
    "title": "ZQL",
    "searchTitle": "Getting a Single Result",
    "sectionTitle": "Getting a Single Result",
    "sectionId": "getting-a-single-result",
    "url": "/docs/zql",
    "content": "If you want exactly zero or one results, use the one() clause. This causes ZQL to return Row|undefined rather than Row[]. const result = await zql.issue .where('id', 42) .one() .run() if (!result) { console.error('not found') } one() overrides any limit() clause that is also present.",
    "kind": "section"
  },
  {
    "id": "511-zql#relationships",
    "title": "ZQL",
    "searchTitle": "Relationships",
    "sectionTitle": "Relationships",
    "sectionId": "relationships",
    "url": "/docs/zql",
    "content": "You can query related rows using relationships that are defined in your Zero schema. // Get all issues and their related comments zql.issue.related('comments') Relationships are returned as hierarchical data. In the above example, each row will have a comments field, which is an array of the corresponding comments rows. You can fetch multiple relationships in a single query: zql.issue .related('comments') .related('reactions') .related('assignees') Refining Relationships By default all matching relationship rows are returned, but this can be refined. The related method accepts an optional second function which is itself a query. zql.issue.related( 'comments', // It is common to use the 'q' shorthand variable for this parameter, // but it is a _comment_ query in particular here, exactly as if you // had done zql.comment. q => q .orderBy('modified', 'desc') .limit(100) .start(lastSeenComment) ) This relationship query can have all the same clauses that top-level queries can have. Using orderBy or limit in a relationship that goes through a junction table (i.e., a many-to-many relationship) is not currently supported and will throw a runtime error. See bug 3527. You can sometimes work around this by making the junction relationship explicit, depending on your schema and usage. Nested Relationships You can nest relationships arbitrarily: // Get all issues, first 100 comments for each (ordered by modified,desc), // and for each comment all of its reactions. zql.issue.related('comments', q => q .orderBy('modified', 'desc') .limit(100) .related('reactions') )",
    "kind": "section"
  },
  {
    "id": "512-zql#refining-relationships",
    "title": "ZQL",
    "searchTitle": "Refining Relationships",
    "sectionTitle": "Refining Relationships",
    "sectionId": "refining-relationships",
    "url": "/docs/zql",
    "content": "By default all matching relationship rows are returned, but this can be refined. The related method accepts an optional second function which is itself a query. zql.issue.related( 'comments', // It is common to use the 'q' shorthand variable for this parameter, // but it is a _comment_ query in particular here, exactly as if you // had done zql.comment. q => q .orderBy('modified', 'desc') .limit(100) .start(lastSeenComment) ) This relationship query can have all the same clauses that top-level queries can have. Using orderBy or limit in a relationship that goes through a junction table (i.e., a many-to-many relationship) is not currently supported and will throw a runtime error. See bug 3527. You can sometimes work around this by making the junction relationship explicit, depending on your schema and usage.",
    "kind": "section"
  },
  {
    "id": "513-zql#nested-relationships",
    "title": "ZQL",
    "searchTitle": "Nested Relationships",
    "sectionTitle": "Nested Relationships",
    "sectionId": "nested-relationships",
    "url": "/docs/zql",
    "content": "You can nest relationships arbitrarily: // Get all issues, first 100 comments for each (ordered by modified,desc), // and for each comment all of its reactions. zql.issue.related('comments', q => q .orderBy('modified', 'desc') .limit(100) .related('reactions') )",
    "kind": "section"
  },
  {
    "id": "514-zql#where",
    "title": "ZQL",
    "searchTitle": "Where",
    "sectionTitle": "Where",
    "sectionId": "where",
    "url": "/docs/zql",
    "content": "You can filter a query with where(): zql.issue.where('priority', '=', 'high') The first parameter is always a column name from the table being queried. TypeScript completion will offer available options (sourced from your Zero Schema). Comparison Operators Where supports the following comparison operators: TypeScript will restrict you from using operators with types that don‚Äôt make sense ‚Äì you can‚Äôt use > with boolean for example. If you don‚Äôt see the comparison operator you need, let us know, many are easy to add. Equals is the Default Comparison Operator Because comparing by = is so common, you can leave it out and where defaults to =. zql.issue.where('priority', 'high') Comparing to null As in SQL, ZQL‚Äôs null cannot be compared with =, !=, <, or any other normal comparison operator. Comparing any value to null with such operators is always false: These semantics feel a bit weird, but they are consistent with SQL. The reason SQL does it this way is to make join semantics work: if you‚Äôre joining employee.orgID on org.id you do not want an employee in no organization to match an org that hasn‚Äôt yet been assigned an ID. For when you purposely do want to compare to null ZQL supports IS and IS NOT operators that also work just like in SQL: // Find employees not in any org. zql.employee.where('orgID', 'IS', null) // Find employees in an org other than 42 OR employees in NO org zql.employee.where('orgID', 'IS NOT', 42) TypeScript will prevent you from comparing to null with other operators. Compound Filters The argument to where can also be a callback that returns a complex expression: // Get all issues that have priority 'critical' or else have both // priority 'medium' and not more than 100 votes. zql.issue.where(({cmp, and, or, not}) => or( cmp('priority', 'critical'), and( cmp('priority', 'medium'), not(cmp('numVotes', '>', 100)) ) ) ) cmp is short for compare and works the same as where at the top-level except that it can‚Äôt be chained and it only accepts comparison operators (no relationship filters ‚Äì see below). Note that chaining where() is also a one-level and: // Find issues with priority 3 or higher, owned by aa zql.issue .where('priority', '>=', 3) .where('owner', 'aa') Comparing Literal Values The where clause always expects its first parameter to be a column name as a string. Same with the cmp helper: // \"foo\" is a column name, not a string: zql.issue.where('foo', 'bar') // \"foo\" is a column name, not a string: zql.issue.where(({cmp}) => cmp('foo', 'bar')) To compare to a literal value, use the cmpLit helper: zql.issue.where(cmpLit('foobar', 'foo' + 'bar')) This is particularly useful for implementing permissions, because the first parameter can be a field of your context: zql.issue.where(cmpLit(ctx.role, 'admin')) Relationship Filters Your filter can also test properties of relationships. Currently the only supported test is existence: // Find all orgs that have at least one employee zql.organization.whereExists('employees') The argument to whereExists is a relationship, so just like other relationships, it can be refined with a query: // Find all orgs that have at least one cool employee zql.organization.whereExists('employees', q => q.where('location', 'Hawaii') ) As with querying relationships, relationship filters can be arbitrarily nested: // Get all issues that have comments that have reactions zql.issue.whereExists('comments', q => q.whereExists('reactions') ) The exists helper is also provided which can be used with and, or, cmp, and not to build compound filters that check relationship existence: // Find issues that have at least one comment or are high priority zql.issue.where({cmp, or, exists} => or( cmp('priority', 'high'), exists('comments'), ), )",
    "kind": "section"
  },
  {
    "id": "515-zql#comparison-operators",
    "title": "ZQL",
    "searchTitle": "Comparison Operators",
    "sectionTitle": "Comparison Operators",
    "sectionId": "comparison-operators",
    "url": "/docs/zql",
    "content": "Where supports the following comparison operators: TypeScript will restrict you from using operators with types that don‚Äôt make sense ‚Äì you can‚Äôt use > with boolean for example. If you don‚Äôt see the comparison operator you need, let us know, many are easy to add.",
    "kind": "section"
  },
  {
    "id": "516-zql#equals-is-the-default-comparison-operator",
    "title": "ZQL",
    "searchTitle": "Equals is the Default Comparison Operator",
    "sectionTitle": "Equals is the Default Comparison Operator",
    "sectionId": "equals-is-the-default-comparison-operator",
    "url": "/docs/zql",
    "content": "Because comparing by = is so common, you can leave it out and where defaults to =. zql.issue.where('priority', 'high')",
    "kind": "section"
  },
  {
    "id": "517-zql#comparing-to-null",
    "title": "ZQL",
    "searchTitle": "Comparing to null",
    "sectionTitle": "Comparing to null",
    "sectionId": "comparing-to-null",
    "url": "/docs/zql",
    "content": "As in SQL, ZQL‚Äôs null cannot be compared with =, !=, <, or any other normal comparison operator. Comparing any value to null with such operators is always false: These semantics feel a bit weird, but they are consistent with SQL. The reason SQL does it this way is to make join semantics work: if you‚Äôre joining employee.orgID on org.id you do not want an employee in no organization to match an org that hasn‚Äôt yet been assigned an ID. For when you purposely do want to compare to null ZQL supports IS and IS NOT operators that also work just like in SQL: // Find employees not in any org. zql.employee.where('orgID', 'IS', null) // Find employees in an org other than 42 OR employees in NO org zql.employee.where('orgID', 'IS NOT', 42) TypeScript will prevent you from comparing to null with other operators.",
    "kind": "section"
  },
  {
    "id": "518-zql#compound-filters",
    "title": "ZQL",
    "searchTitle": "Compound Filters",
    "sectionTitle": "Compound Filters",
    "sectionId": "compound-filters",
    "url": "/docs/zql",
    "content": "The argument to where can also be a callback that returns a complex expression: // Get all issues that have priority 'critical' or else have both // priority 'medium' and not more than 100 votes. zql.issue.where(({cmp, and, or, not}) => or( cmp('priority', 'critical'), and( cmp('priority', 'medium'), not(cmp('numVotes', '>', 100)) ) ) ) cmp is short for compare and works the same as where at the top-level except that it can‚Äôt be chained and it only accepts comparison operators (no relationship filters ‚Äì see below). Note that chaining where() is also a one-level and: // Find issues with priority 3 or higher, owned by aa zql.issue .where('priority', '>=', 3) .where('owner', 'aa')",
    "kind": "section"
  },
  {
    "id": "519-zql#comparing-literal-values",
    "title": "ZQL",
    "searchTitle": "Comparing Literal Values",
    "sectionTitle": "Comparing Literal Values",
    "sectionId": "comparing-literal-values",
    "url": "/docs/zql",
    "content": "The where clause always expects its first parameter to be a column name as a string. Same with the cmp helper: // \"foo\" is a column name, not a string: zql.issue.where('foo', 'bar') // \"foo\" is a column name, not a string: zql.issue.where(({cmp}) => cmp('foo', 'bar')) To compare to a literal value, use the cmpLit helper: zql.issue.where(cmpLit('foobar', 'foo' + 'bar')) This is particularly useful for implementing permissions, because the first parameter can be a field of your context: zql.issue.where(cmpLit(ctx.role, 'admin'))",
    "kind": "section"
  },
  {
    "id": "520-zql#relationship-filters",
    "title": "ZQL",
    "searchTitle": "Relationship Filters",
    "sectionTitle": "Relationship Filters",
    "sectionId": "relationship-filters",
    "url": "/docs/zql",
    "content": "Your filter can also test properties of relationships. Currently the only supported test is existence: // Find all orgs that have at least one employee zql.organization.whereExists('employees') The argument to whereExists is a relationship, so just like other relationships, it can be refined with a query: // Find all orgs that have at least one cool employee zql.organization.whereExists('employees', q => q.where('location', 'Hawaii') ) As with querying relationships, relationship filters can be arbitrarily nested: // Get all issues that have comments that have reactions zql.issue.whereExists('comments', q => q.whereExists('reactions') ) The exists helper is also provided which can be used with and, or, cmp, and not to build compound filters that check relationship existence: // Find issues that have at least one comment or are high priority zql.issue.where({cmp, or, exists} => or( cmp('priority', 'high'), exists('comments'), ), )",
    "kind": "section"
  },
  {
    "id": "521-zql#type-helpers",
    "title": "ZQL",
    "searchTitle": "Type Helpers",
    "sectionTitle": "Type Helpers",
    "sectionId": "type-helpers",
    "url": "/docs/zql",
    "content": "You can get the TypeScript type of the result of a query using the QueryResultType helper: import type {QueryResultType} from '@rocicorp/zero' const complexQuery = zql.issue.related( 'comments', q => q.related('author') ) type MyComplexResult = QueryResultType<typeof complexQuery> // MyComplexResult is: readonly IssueRow & { // readonly comments: readonly (CommentRow & { // readonly author: readonly AuthorRow|undefined; // })[]; // }[] You can get the type of a single row with QueryRowType: import type {QueryRowType} from '@rocicorp/zero' type MySingleRow = QueryRowType<typeof complexQuery> // MySingleRow is: readonly IssueRow & { // readonly comments: readonly (CommentRow & { // readonly author: readonly AuthorRow|undefined; // })[]; // }",
    "kind": "section"
  },
  {
    "id": "522-zql#planning",
    "title": "ZQL",
    "searchTitle": "Planning",
    "sectionTitle": "Planning",
    "sectionId": "planning",
    "url": "/docs/zql",
    "content": "Zero automatically plans queries, selecting the best indexes and join orders in most cases. Inspecting Query Plans You can inspect the plan that Zero generates for any ZQL query using the inspector. Manually Flipping Joins The process Zero uses to optimize joins is called \"join flipping\", because it involves \"flipping\" the order of joins to minimize the number of rows processed. Typically the Zero planner will pick the joins to flip automatically. But in some rare cases, you may want to manually specify the join order. This can be done by passing the flip:true option to whereExists: // Find the first 100 documents that user 42 can edit, // ordered by created desc. Because each user is an editor // of only a few documents, flip:true is much faster than // flip:false. zql.documents.whereExists('editors', e => e.where('userID', 42), {flip: true} ), .orderBy('created', 'desc') .limit(100) Or with exists: // Find issues created by user 42 or that have a comment // by user 42. Because user 42 has commented on only a // few issues, flip:true is much faster than flip:false. zql.issue.where({cmp, or, exists} => or( cmp('creatorID', 42), exists('comments', c => c.where('creatorID', 42), {flip: true}), ), ) You can manually flip just one or a few of the whereExists clauses in a query, leaving the rest to be planned automatically.",
    "kind": "section"
  },
  {
    "id": "523-zql#inspecting-query-plans",
    "title": "ZQL",
    "searchTitle": "Inspecting Query Plans",
    "sectionTitle": "Inspecting Query Plans",
    "sectionId": "inspecting-query-plans",
    "url": "/docs/zql",
    "content": "You can inspect the plan that Zero generates for any ZQL query using the inspector.",
    "kind": "section"
  },
  {
    "id": "524-zql#manually-flipping-joins",
    "title": "ZQL",
    "searchTitle": "Manually Flipping Joins",
    "sectionTitle": "Manually Flipping Joins",
    "sectionId": "manually-flipping-joins",
    "url": "/docs/zql",
    "content": "The process Zero uses to optimize joins is called \"join flipping\", because it involves \"flipping\" the order of joins to minimize the number of rows processed. Typically the Zero planner will pick the joins to flip automatically. But in some rare cases, you may want to manually specify the join order. This can be done by passing the flip:true option to whereExists: // Find the first 100 documents that user 42 can edit, // ordered by created desc. Because each user is an editor // of only a few documents, flip:true is much faster than // flip:false. zql.documents.whereExists('editors', e => e.where('userID', 42), {flip: true} ), .orderBy('created', 'desc') .limit(100) Or with exists: // Find issues created by user 42 or that have a comment // by user 42. Because user 42 has commented on only a // few issues, flip:true is much faster than flip:false. zql.issue.where({cmp, or, exists} => or( cmp('creatorID', 42), exists('comments', c => c.where('creatorID', 42), {flip: true}), ), ) You can manually flip just one or a few of the whereExists clauses in a query, leaving the rest to be planned automatically.",
    "kind": "section"
  }
]